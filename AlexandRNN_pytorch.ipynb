{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "AlexandRNN_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5LqrlD34g3m",
        "outputId": "4e55b5ea-848b-4796-df64-a6d994473dea"
      },
      "source": [
        "!pip install fast_bleu\n",
        "!pip3 install haspirater frhyme plint\n",
        "!pip install kora\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fast_bleu\n",
            "  Downloading https://files.pythonhosted.org/packages/50/9d/82d4dec5947242dad485af5d95c62d2735f19f4dde828905d62d20ccc2f5/fast-bleu-0.0.86.tar.gz\n",
            "Building wheels for collected packages: fast-bleu\n",
            "  Building wheel for fast-bleu (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fast-bleu: filename=fast_bleu-0.0.86-cp36-cp36m-linux_x86_64.whl size=581605 sha256=5184106897c2119feb1eb10d8e125addee13f53ad71528ed42d07e969fe88d0a\n",
            "  Stored in directory: /root/.cache/pip/wheels/e9/69/bb/3ca889cdb275ea238498844f2a65e839105db499c404f544d7\n",
            "Successfully built fast-bleu\n",
            "Installing collected packages: fast-bleu\n",
            "Successfully installed fast-bleu-0.0.86\n",
            "Collecting haspirater\n",
            "  Downloading https://files.pythonhosted.org/packages/4f/18/f451ccd267b3d0a97bd46e8854b3226164219b24afb6c669d89626df5653/haspirater-0.2-py3-none-any.whl\n",
            "Collecting frhyme\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/ff/85a73b2672fe8d40b870c92aa0908419c03025b27c4b30ee44eaf4218fdd/frhyme-0.3-py3-none-any.whl (308kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 19.3MB/s \n",
            "\u001b[?25hCollecting plint\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/84/3f/67d8fad46f27d0484429ff2a0fb0b89e8f0bb00020e26620ee7921b89b8e/plint-0.1-py3-none-any.whl (431kB)\n",
            "\u001b[K     |████████████████████████████████| 440kB 56.7MB/s \n",
            "\u001b[?25hInstalling collected packages: haspirater, frhyme, plint\n",
            "Successfully installed frhyme-0.3 haspirater-0.2 plint-0.1\n",
            "Collecting kora\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5b/0d/3d9ab9ee747f0925b038e8350ce137276a7a4730a96a3516485dc1b87ba3/kora-0.9.19-py3-none-any.whl (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from kora) (5.5.0)\n",
            "Collecting fastcore\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0c/98/60404e2817cff113a6ae4023bc1772e23179408fdf7857fa410551758dfe/fastcore-1.3.19-py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython->kora) (2.6.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->kora) (4.8.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython->kora) (4.3.3)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython->kora) (53.0.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->kora) (0.8.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython->kora) (1.0.18)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->kora) (0.7.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython->kora) (4.4.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from fastcore->kora) (20.9)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.6/dist-packages (from fastcore->kora) (19.3.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->kora) (0.7.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython->kora) (0.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython->kora) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->kora) (0.2.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->fastcore->kora) (2.4.7)\n",
            "Installing collected packages: fastcore, kora\n",
            "Successfully installed fastcore-1.3.19 kora-0.9.19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1ACLXSw4g3s"
      },
      "source": [
        "### Tools for data processing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6D83zYN44g3s"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import math\n",
        "from collections import Counter\n",
        "import pprint\n",
        "pp = pprint.PrettyPrinter(indent=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6HdDjADEQxS",
        "outputId": "4156b0f9-e47e-4469-bba6-b6457484da12"
      },
      "source": [
        "with open('mots_rimes.txt', encoding='utf-8') as f:\r\n",
        "  categories, eos_tokens = [], []\r\n",
        "  incr = 0\r\n",
        "  for line in f:\r\n",
        "    if incr % 2 == 1:\r\n",
        "      categories.append(line.split())\r\n",
        "      \r\n",
        "    else :\r\n",
        "      eos_tokens.append(line.split()[0])\r\n",
        "    incr +=1\r\n",
        "print(categories[1])\r\n",
        "print(eos_tokens[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Silence', 'Balance', 'convalescence', 'instance', 'Ordonnance', 'surséance', 'Vengeance', 'Clémence', 'prééminence', 'condescendance', 'Innocence', 'Puissance', 'Danse', 'Dépense', 'Innocence', 'indécence', 'Térence', 'Maxence', 'Byzance', 'Constance', 'Florence', 'France', 'abondance', 'absence', 'alliance', 'allégeance', 'apparence', 'arrogance', 'assistance', 'assurance', 'audience', 'avance', 'balance', 'bienséance', 'bienveillance', 'circonstance', 'clémence', 'commence', 'complaisance', 'concurrence', 'confiance', 'confidence', 'conférence', 'connaissance', 'conscience', 'constance', 'conséquence', 'croyance', 'créance', 'danse', 'devance', 'différence', 'diligence', 'dispense', 'distance', 'défense', 'défiance', 'déférence', 'délivrance', 'dépendance', 'désobéissance', 'enfance', 'espérance', 'excellence', 'expérience', 'extravagance', 'ignorance', 'immense', 'impatience', 'importance', 'imprudence', 'impudence', 'impuissance', 'inclémence', 'inconstance', 'indifférence', 'indulgence', 'indépendance', 'influence', 'innocence', 'insolence', 'insuffisance', 'intelligence', 'irrévérence', 'jouissance', 'licence', 'magnificence', 'méconnaissance', 'médisance', 'naissance', 'nonchalance', 'négligence', 'obéissance', 'occurrence', 'offense', 'opulence', 'panse', 'patience', 'pense', 'persévérance', 'providence', 'prudence', 'préférence', 'présence', 'prévoyance', 'puissance', 'reconnaissance', 'ressemblance', 'récompense', 'répugnance', 'résistance', 'science', 'sentence', 'silence', 'souffrance', 'vaillance', 'vengeance', 'vigilance', 'violence', 'véhémence', 'éloquence', 'éminence', 'évidence', 'décadence', 'Hortense', 'aisance', 'révérence', 'décence', 'finance', 'Puissance', 'Innocence', 'Enfance', 'chance', 'impertinence', 'Science', 'dépense', 'ordonnance', 'indigence', 'semence', 'lance', 'bienfaisance', 'encense', 'existence', 'Vengeance', 'Ordonnance', 'Conscience', 'convalescence', 'Alliance', 'Ignorance', 'instance', 'suffisance', 'outrance', 'manigance', 'Terence', 'Patience', 'réminiscence', 'compétence', 'potence', 'Influence', 'contenance', 'quittance', 'abstinence', 'pénitence', 'substance', 'incontinence', 'continence', 'excressanse', 'quintessence', 'transparence', 'intempérance', 'démence', 'cadence', 'Préférence', 'Assurance', 'Dépendance', 'Offense', 'résidence', 'Médisance', 'indécence', 'Bienséance', 'Complaisance', 'Maxence', 'défaillance', 'Clémence', 'surséance', 'Térence']\n",
            "-air[e]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urQQ5WEI4g3s"
      },
      "source": [
        "class Dictionary(object):\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = []\n",
        "        self.counter = {}\n",
        "        self.total = 0\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.idx2word.append(word)\n",
        "            self.word2idx[word] = len(self.idx2word) - 1\n",
        "            self.counter.setdefault(word, 0)\n",
        "        self.counter[word] += 1\n",
        "        self.total += 1\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RG6rsEQB4g3t"
      },
      "source": [
        "class Corpus(object):\n",
        "    def __init__(self, path):\n",
        "        # We create an object Dictionary associated to Corpus\n",
        "        self.dictionary = Dictionary()\n",
        "        # We go through all files, adding all words to the dictionary\n",
        "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
        "        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n",
        "        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n",
        "        \n",
        "    def tokenize(self, path):\n",
        "        \"\"\"Tokenizes a text file, knowing the dictionary, in order to tranform it into a list of indexes\"\"\"\n",
        "        assert os.path.exists(path)\n",
        "        # Add words to the dictionary\n",
        "        with open(path, 'r') as f:\n",
        "            tokens = 0\n",
        "            eos_seq = []\n",
        "            for line in f:\n",
        "                  words = ['<sos>'] + line.split() + ['<eos>']\n",
        "                  rime, inc = words[-1], len(words)-1\n",
        "                  while(rime in ['\"', '.', ',', ';', ':', '?', '!', ' ', ')', '»', '-', '\\xa0', '\\n', '<eos>']):\n",
        "                    inc -= 1\n",
        "                    rime = words[inc]\n",
        "                  isInNoCat = True\n",
        "                  for i in range(100):\n",
        "                    if rime in categories[i]:\n",
        "                      isInNoCat = False\n",
        "                      words.insert(inc, eos_tokens[i])\n",
        "                      eos_seq.append([eos_tokens[i], inc])\n",
        "                      break\n",
        "                  if isInNoCat :\n",
        "                    eos_seq.append(['UNK', inc])\n",
        "                  else:\n",
        "                      for word in words:\n",
        "                        self.dictionary.add_word(word)\n",
        "                      tokens += len(words)\n",
        "        \n",
        "        # Once done, go through the file a second time and fill a Torch Tensor with the associated indexes \n",
        "        with open(path, 'r') as f:\n",
        "            ids = torch.LongTensor(tokens)\n",
        "            token,incr = 0, 0\n",
        "            for line in f:\n",
        "              cur_eos = eos_seq[incr]\n",
        "              if cur_eos[0] != 'UNK':\n",
        "                  words = ['<sos>'] + line.split() + ['<eos>']\n",
        "                  words.insert(cur_eos[1], cur_eos[0])\n",
        "                  for word in words:\n",
        "                    ids[token] = self.dictionary.word2idx[word]\n",
        "                    token += 1\n",
        "              incr +=1\n",
        "        return ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytRgixof4g3t"
      },
      "source": [
        "###############################################################################\n",
        "# Load data\n",
        "###############################################################################\n",
        "\n",
        "data = './corpus/'\n",
        "corpus = Corpus(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHf1bA7C4g3t",
        "outputId": "09595567-9e17-4333-f0b9-6b33926d450b"
      },
      "source": [
        "print(corpus.dictionary.total)\n",
        "print(len(corpus.dictionary.idx2word))\n",
        "print(len(corpus.dictionary.word2idx))\n",
        "\n",
        "print(corpus.train.shape)\n",
        "print(corpus.train[0:7])\n",
        "print([corpus.dictionary.idx2word[corpus.train[i]] for i in range(40)])\n",
        "\n",
        "print(corpus.valid.shape)\n",
        "print(corpus.valid[0:7])\n",
        "print([corpus.dictionary.idx2word[corpus.valid[i]] for i in range(7)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2000441\n",
            "28918\n",
            "28918\n",
            "torch.Size([1502851])\n",
            "tensor([0, 1, 2, 3, 4, 5, 6])\n",
            "['<sos>', 'Impatients', 'désirs', 'd’', 'une', 'illustre', 'vengeance', '<eos>', '<sos>', 'Dont', 'la', 'mort', 'de', 'mon', 'père', 'a', 'formé', 'la', 'naissance', ',', '<eos>', '<sos>', 'Enfants', 'impétueux', 'de', 'mon', 'ressentiment', '<eos>', '<sos>', 'Que', 'ma', 'douleur', 'séduite', 'embrasse', 'aveuglément', ',', '<eos>', '<sos>', 'Vous', 'régnez']\n",
            "torch.Size([252869])\n",
            "tensor([    0,    87,    96, 14596,     9,  1267,    40])\n",
            "['<sos>', 'Je', 'lui', 'prescris', 'la', 'loi', 'que']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIWjBwFj4g3t"
      },
      "source": [
        "# We now have data under a very long list of indexes: the text is as one sequence.\n",
        "# The idea now is to create batches from this. Note that this is absolutely not the best\n",
        "# way to proceed with large quantities of data (where we'll try not to store huge tensors\n",
        "# in memory but read them from file as we go) !\n",
        "# Here, we are looking for simplicity and efficiency with regards to computation time.\n",
        "# That is why we will ignore sentence separations and treat the data as one long stream that\n",
        "# we will cut arbitrarily as we need.\n",
        "# With the alphabet being our data, we currently have the sequence:\n",
        "# [a b c d e f g h i j k l m n o p q r s t u v w x y z]\n",
        "# We want to reorganize it as independant batches that will be processed independantly by the model !\n",
        "# For instance, with the alphabet as the sequence and batch size 4, we'd get the 4 following sequences:\n",
        "# ┌ a g m s ┐\n",
        "# │ b h n t │\n",
        "# │ c i o u │\n",
        "# │ d j p v │\n",
        "# │ e k q w │\n",
        "# └ f l r x ┘\n",
        "# with the last two elements being lost.\n",
        "# Again, these columns are treated as independent by the model, which means that the\n",
        "# dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient processing.\n",
        "\n",
        "def batchify(data, batch_size, cuda = False):\n",
        "    # Cut the elements that are unnecessary\n",
        "    nbatch = data.size(0) // batch_size\n",
        "    data = data.narrow(0, 0, nbatch * batch_size)\n",
        "    # Reorganize the data\n",
        "    data = data.view(batch_size, -1).t().contiguous()\n",
        "    # If we can use a GPU, let's transfer the tensor to it\n",
        "    return data.to(device)\n",
        "\n",
        "# get_batch subdivides the source data into chunks of the appropriate length.\n",
        "# If source is equal to the example output of the batchify function, with\n",
        "# a sequence length (seq_len) of 3, we'd get the following two variables:\n",
        "# ┌ a g m s ┐ ┌ b h n t ┐\n",
        "# | b h n t | | c i o u │\n",
        "# └ c i o u ┘ └ d j p v ┘\n",
        "# The first variable contains the letters input to the network, while the second\n",
        "# contains the one we want the network to predict (b for a, h for g, v for u, etc..)\n",
        "# Note that despite the name of the function, we are cutting the data in the\n",
        "# temporal dimension, since we already divided data into batches in the previous\n",
        "# function. \n",
        "\n",
        "def get_batch(source, i, seq_len, evaluation=False):\n",
        "    # Deal with the possibility that there's not enough data left for a full sequence\n",
        "    seq_len = min(seq_len, len(source) - 1 - i)\n",
        "    # Take the input data\n",
        "    data = source[i:i+seq_len]\n",
        "    # Shift by one for the target data\n",
        "    target = source[i+1:i+1+seq_len]\n",
        "    return data, target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMeQ0gjzUOLd"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwfqKiMJ4g3t",
        "outputId": "71288728-bbd5-4d16-bb42-d904885f2715"
      },
      "source": [
        "batch_size = 100\n",
        "eval_batch_size = 4\n",
        "train_data = batchify(corpus.train, batch_size)\n",
        "val_data = batchify(corpus.valid, eval_batch_size)\n",
        "test_data = batchify(corpus.test, eval_batch_size)\n",
        "\n",
        "print(train_data.shape)\n",
        "print(val_data.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([15028, 100])\n",
            "torch.Size([63217, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9OVIoVJ4g3u"
      },
      "source": [
        "### LSTM Cells in pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztKrssh84g3u"
      },
      "source": [
        "### Creating our own LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zD0K9CQ14g3u"
      },
      "source": [
        "# Models are usually implemented as custom nn.Module subclass\n",
        "# We need to redefine the __init__ method, which creates the object\n",
        "# We also need to redefine the forward method, which transform the input into outputs\n",
        "# We can also add any method that we need: here, in order to initiate weights in the model\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        # Create a dropout object to use on layers for regularization\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        # Create an encoder - which is an embedding layer\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        # Create the LSTM layers - find out how to stack them !\n",
        "        self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
        "        # Create what we call the decoder: a linear transformation to map the hidden state into scores for all words in the vocabulary\n",
        "        # (Note that the softmax application function will be applied out of the model)\n",
        "        self.decoder = nn.Linear(nhid, ntoken)\n",
        "        \n",
        "        # Initialize non-reccurent weights \n",
        "        self.init_weights()\n",
        "\n",
        "        self.ninp = ninp\n",
        "        self.nhid = nhid\n",
        "        self.nlayers = nlayers\n",
        "        \n",
        "    def init_weights(self):\n",
        "        # Initialize the encoder and decoder weights with the uniform distribution\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.fill_(0)\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "        \n",
        "    def init_hidden(self, batch_size):\n",
        "        # Initialize the hidden state and cell state to zero, with the right sizes\n",
        "        weight = next(self.parameters())\n",
        "        return (weight.new_zeros(self.nlayers, batch_size, self.nhid),\n",
        "                weight.new_zeros(self.nlayers, batch_size, self.nhid))    \n",
        "\n",
        "    def forward(self, input, hidden, return_h=False):\n",
        "        # Process the input\n",
        "        emb = self.drop(self.encoder(input))   \n",
        "        \n",
        "        # Apply the LSTMs\n",
        "        output, hidden = self.rnn(emb, hidden)\n",
        "        \n",
        "        # Decode into scores\n",
        "        output = self.drop(output)      \n",
        "        decoded = self.decoder(output)\n",
        "        return decoded, hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucXMYksR4g3u"
      },
      "source": [
        "### Building the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HL-vvLGm4g3u",
        "outputId": "a1855e14-c0c7-441e-b332-7397635bcd24"
      },
      "source": [
        "# Set the random seed manually for reproducibility.\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f20af16fba0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S147j0Dx4g3u"
      },
      "source": [
        "embedding_size = 500\n",
        "hidden_size = 1024\n",
        "layers = 2\n",
        "dropout = 0.2\n",
        "\n",
        "###############################################################################\n",
        "# Build the model\n",
        "###############################################################################\n",
        "\n",
        "vocab_size = len(corpus.dictionary)\n",
        "model = LSTMModel(vocab_size, embedding_size, hidden_size, layers, dropout).to(device)\n",
        "params = list(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RU2X8VHG4g3v"
      },
      "source": [
        "lr = 10.0\n",
        "optimizer = 'sgd'\n",
        "wdecay = 1.2e-6\n",
        "# For gradient clipping\n",
        "clip = 0.25\n",
        "\n",
        "if optimizer == 'sgd':\n",
        "    optim = torch.optim.SGD(params, lr=lr, weight_decay=wdecay)\n",
        "if optimizer == 'adam':\n",
        "    optim = torch.optim.Adam(params, lr=lr, weight_decay=wdecay)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fGlrwME4g3v"
      },
      "source": [
        "# Let's think about gradient propagation:\n",
        "# We plan to keep the second ouput of the LSTM layer (the hidden/cell states) to initialize\n",
        "# the next call to LSTM. In this way, we can back-propagate the gradient for as long as we want.\n",
        "# However, this put a huge strain on the memory used by the model, since it implies retaining\n",
        "# a always-growing number of tensors of gradients in the cache.\n",
        "# We decide to not backpropagate through time beyond the current sequence ! \n",
        "# We use a specific function to cut the 'hidden/state cell' states from their previous dependencies\n",
        "# before using them to initialize the next call to the LSTM.\n",
        "# This is done with the .detach() function.\n",
        "\n",
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4CGuudp4g3v"
      },
      "source": [
        "# Other global parameters\n",
        "epochs = 30\n",
        "seq_len = 40\n",
        "log_interval = 10\n",
        "save = 'model.pt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xx6UKwTO4g3v"
      },
      "source": [
        "def evaluate(data_source):\n",
        "    # Turn on evaluation mode which disables dropout.\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    hidden = model.init_hidden(eval_batch_size)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, seq_len):\n",
        "            data, targets = get_batch(data_source, i, seq_len)\n",
        "            output, hidden = model(data, hidden)\n",
        "            hidden = repackage_hidden(hidden)\n",
        "            total_loss += len(data) * criterion(output.view(-1, vocab_size), targets.view(-1)).item()\n",
        "    return total_loss / (len(data_source) - 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EhuQglX4g3v"
      },
      "source": [
        "def train():\n",
        "    # Turn on training mode which enables dropout.\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    hidden = model.init_hidden(batch_size)\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, seq_len)):\n",
        "        data, targets = get_batch(train_data, i, seq_len)\n",
        "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "        hidden = repackage_hidden(hidden)\n",
        "        optim.zero_grad()\n",
        "        \n",
        "        output, hidden = model(data, hidden)\n",
        "        loss = criterion(output.view(-1, vocab_size), targets.view(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        torch.nn.utils.clip_grad_norm_(params, clip)\n",
        "        optim.step()\n",
        "        \n",
        "        total_loss += loss.data\n",
        "\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
        "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                epoch, batch, len(train_data) // seq_len, lr,\n",
        "                elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bN0gBRgf4g3v",
        "outputId": "e76bfb3a-228f-4c07-ff84-28ad67f7dca2"
      },
      "source": [
        "# Loop over epochs.\n",
        "best_val_loss = None\n",
        "\n",
        "# At any point you can hit Ctrl + C to break out of training early.\n",
        "try:\n",
        "    for epoch in range(1, epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        train()\n",
        "        val_loss = evaluate(val_data)\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                           val_loss, math.exp(val_loss)))\n",
        "        print('-' * 89)\n",
        "        # Save the model if the validation loss is the best we've seen so far.\n",
        "        if not best_val_loss or val_loss < best_val_loss:\n",
        "            with open(save, 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_val_loss = val_loss\n",
        "        else:\n",
        "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "            lr /= 4.0\n",
        "except KeyboardInterrupt:\n",
        "    print('-' * 89)\n",
        "    print('Exiting from training early')\n",
        "\n",
        "# Load the best saved model.\n",
        "with open(save, 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "    # after load the rnn params are not a continuous chunk of memory\n",
        "    # this makes them a continuous chunk, and will speed up forward pass\n",
        "    model.rnn.flatten_parameters()\n",
        "\n",
        "# Run on test data.\n",
        "test_loss = evaluate(test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, math.exp(test_loss)))\n",
        "print('=' * 89)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| epoch   1 |    10/  375 batches | lr 10.00 | ms/batch 337.83 | loss 10.38 | ppl 32151.94\n",
            "| epoch   1 |    20/  375 batches | lr 10.00 | ms/batch 298.88 | loss  8.27 | ppl  3906.01\n",
            "| epoch   1 |    30/  375 batches | lr 10.00 | ms/batch 300.37 | loss  7.74 | ppl  2302.01\n",
            "| epoch   1 |    40/  375 batches | lr 10.00 | ms/batch 303.33 | loss  7.61 | ppl  2020.72\n",
            "| epoch   1 |    50/  375 batches | lr 10.00 | ms/batch 304.50 | loss  7.25 | ppl  1407.82\n",
            "| epoch   1 |    60/  375 batches | lr 10.00 | ms/batch 304.01 | loss  7.05 | ppl  1152.44\n",
            "| epoch   1 |    70/  375 batches | lr 10.00 | ms/batch 305.77 | loss  7.03 | ppl  1128.31\n",
            "| epoch   1 |    80/  375 batches | lr 10.00 | ms/batch 306.60 | loss  6.93 | ppl  1019.55\n",
            "| epoch   1 |    90/  375 batches | lr 10.00 | ms/batch 310.56 | loss  6.84 | ppl   938.45\n",
            "| epoch   1 |   100/  375 batches | lr 10.00 | ms/batch 312.45 | loss  6.74 | ppl   845.07\n",
            "| epoch   1 |   110/  375 batches | lr 10.00 | ms/batch 315.43 | loss  6.46 | ppl   640.72\n",
            "| epoch   1 |   120/  375 batches | lr 10.00 | ms/batch 319.75 | loss  6.25 | ppl   515.98\n",
            "| epoch   1 |   130/  375 batches | lr 10.00 | ms/batch 321.11 | loss  6.10 | ppl   447.53\n",
            "| epoch   1 |   140/  375 batches | lr 10.00 | ms/batch 321.61 | loss  5.95 | ppl   384.64\n",
            "| epoch   1 |   150/  375 batches | lr 10.00 | ms/batch 324.22 | loss  5.88 | ppl   356.38\n",
            "| epoch   1 |   160/  375 batches | lr 10.00 | ms/batch 326.00 | loss  5.83 | ppl   339.32\n",
            "| epoch   1 |   170/  375 batches | lr 10.00 | ms/batch 325.80 | loss  5.80 | ppl   330.50\n",
            "| epoch   1 |   180/  375 batches | lr 10.00 | ms/batch 329.52 | loss  5.80 | ppl   328.88\n",
            "| epoch   1 |   190/  375 batches | lr 10.00 | ms/batch 335.50 | loss  5.72 | ppl   304.77\n",
            "| epoch   1 |   200/  375 batches | lr 10.00 | ms/batch 339.26 | loss  5.66 | ppl   288.51\n",
            "| epoch   1 |   210/  375 batches | lr 10.00 | ms/batch 344.79 | loss  5.70 | ppl   300.24\n",
            "| epoch   1 |   220/  375 batches | lr 10.00 | ms/batch 344.40 | loss  5.59 | ppl   268.81\n",
            "| epoch   1 |   230/  375 batches | lr 10.00 | ms/batch 343.19 | loss  5.53 | ppl   252.24\n",
            "| epoch   1 |   240/  375 batches | lr 10.00 | ms/batch 346.01 | loss  5.57 | ppl   261.78\n",
            "| epoch   1 |   250/  375 batches | lr 10.00 | ms/batch 349.00 | loss  5.49 | ppl   243.27\n",
            "| epoch   1 |   260/  375 batches | lr 10.00 | ms/batch 352.71 | loss  5.40 | ppl   221.57\n",
            "| epoch   1 |   270/  375 batches | lr 10.00 | ms/batch 359.10 | loss  5.38 | ppl   217.56\n",
            "| epoch   1 |   280/  375 batches | lr 10.00 | ms/batch 362.04 | loss  5.37 | ppl   214.99\n",
            "| epoch   1 |   290/  375 batches | lr 10.00 | ms/batch 365.88 | loss  5.35 | ppl   210.11\n",
            "| epoch   1 |   300/  375 batches | lr 10.00 | ms/batch 370.28 | loss  5.32 | ppl   203.47\n",
            "| epoch   1 |   310/  375 batches | lr 10.00 | ms/batch 375.10 | loss  5.30 | ppl   199.59\n",
            "| epoch   1 |   320/  375 batches | lr 10.00 | ms/batch 377.25 | loss  5.23 | ppl   187.37\n",
            "| epoch   1 |   330/  375 batches | lr 10.00 | ms/batch 379.06 | loss  5.21 | ppl   183.32\n",
            "| epoch   1 |   340/  375 batches | lr 10.00 | ms/batch 373.99 | loss  5.14 | ppl   170.17\n",
            "| epoch   1 |   350/  375 batches | lr 10.00 | ms/batch 368.91 | loss  5.17 | ppl   176.65\n",
            "| epoch   1 |   360/  375 batches | lr 10.00 | ms/batch 363.64 | loss  5.16 | ppl   174.34\n",
            "| epoch   1 |   370/  375 batches | lr 10.00 | ms/batch 360.74 | loss  5.17 | ppl   175.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 145.33s | valid loss  5.09 | valid ppl   162.33\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |    10/  375 batches | lr 10.00 | ms/batch 383.62 | loss  5.62 | ppl   276.25\n",
            "| epoch   2 |    20/  375 batches | lr 10.00 | ms/batch 351.04 | loss  5.05 | ppl   156.79\n",
            "| epoch   2 |    30/  375 batches | lr 10.00 | ms/batch 353.72 | loss  5.04 | ppl   154.69\n",
            "| epoch   2 |    40/  375 batches | lr 10.00 | ms/batch 355.97 | loss  5.04 | ppl   154.18\n",
            "| epoch   2 |    50/  375 batches | lr 10.00 | ms/batch 359.41 | loss  5.03 | ppl   153.67\n",
            "| epoch   2 |    60/  375 batches | lr 10.00 | ms/batch 362.71 | loss  4.99 | ppl   146.45\n",
            "| epoch   2 |    70/  375 batches | lr 10.00 | ms/batch 364.69 | loss  4.97 | ppl   144.20\n",
            "| epoch   2 |    80/  375 batches | lr 10.00 | ms/batch 364.92 | loss  4.92 | ppl   137.19\n",
            "| epoch   2 |    90/  375 batches | lr 10.00 | ms/batch 364.66 | loss  4.92 | ppl   136.62\n",
            "| epoch   2 |   100/  375 batches | lr 10.00 | ms/batch 366.02 | loss  4.91 | ppl   135.07\n",
            "| epoch   2 |   110/  375 batches | lr 10.00 | ms/batch 365.99 | loss  4.91 | ppl   135.33\n",
            "| epoch   2 |   120/  375 batches | lr 10.00 | ms/batch 364.83 | loss  4.89 | ppl   132.59\n",
            "| epoch   2 |   130/  375 batches | lr 10.00 | ms/batch 364.68 | loss  4.85 | ppl   128.02\n",
            "| epoch   2 |   140/  375 batches | lr 10.00 | ms/batch 363.67 | loss  4.85 | ppl   127.98\n",
            "| epoch   2 |   150/  375 batches | lr 10.00 | ms/batch 360.96 | loss  4.81 | ppl   122.99\n",
            "| epoch   2 |   160/  375 batches | lr 10.00 | ms/batch 359.52 | loss  4.81 | ppl   122.80\n",
            "| epoch   2 |   170/  375 batches | lr 10.00 | ms/batch 358.46 | loss  4.79 | ppl   120.50\n",
            "| epoch   2 |   180/  375 batches | lr 10.00 | ms/batch 356.68 | loss  4.77 | ppl   118.15\n",
            "| epoch   2 |   190/  375 batches | lr 10.00 | ms/batch 356.36 | loss  4.74 | ppl   114.43\n",
            "| epoch   2 |   200/  375 batches | lr 10.00 | ms/batch 355.39 | loss  4.71 | ppl   110.65\n",
            "| epoch   2 |   210/  375 batches | lr 10.00 | ms/batch 355.07 | loss  4.74 | ppl   114.67\n",
            "| epoch   2 |   220/  375 batches | lr 10.00 | ms/batch 355.01 | loss  4.71 | ppl   111.37\n",
            "| epoch   2 |   230/  375 batches | lr 10.00 | ms/batch 355.69 | loss  4.68 | ppl   108.29\n",
            "| epoch   2 |   240/  375 batches | lr 10.00 | ms/batch 356.20 | loss  4.68 | ppl   107.46\n",
            "| epoch   2 |   250/  375 batches | lr 10.00 | ms/batch 357.16 | loss  4.65 | ppl   104.99\n",
            "| epoch   2 |   260/  375 batches | lr 10.00 | ms/batch 358.45 | loss  4.62 | ppl   101.18\n",
            "| epoch   2 |   270/  375 batches | lr 10.00 | ms/batch 358.81 | loss  4.62 | ppl   101.43\n",
            "| epoch   2 |   280/  375 batches | lr 10.00 | ms/batch 359.43 | loss  4.57 | ppl    97.00\n",
            "| epoch   2 |   290/  375 batches | lr 10.00 | ms/batch 359.54 | loss  4.56 | ppl    95.77\n",
            "| epoch   2 |   300/  375 batches | lr 10.00 | ms/batch 360.96 | loss  4.60 | ppl    99.78\n",
            "| epoch   2 |   310/  375 batches | lr 10.00 | ms/batch 361.67 | loss  4.55 | ppl    94.84\n",
            "| epoch   2 |   320/  375 batches | lr 10.00 | ms/batch 361.64 | loss  4.55 | ppl    94.33\n",
            "| epoch   2 |   330/  375 batches | lr 10.00 | ms/batch 362.40 | loss  4.51 | ppl    90.75\n",
            "| epoch   2 |   340/  375 batches | lr 10.00 | ms/batch 361.87 | loss  4.47 | ppl    87.76\n",
            "| epoch   2 |   350/  375 batches | lr 10.00 | ms/batch 361.55 | loss  4.52 | ppl    91.92\n",
            "| epoch   2 |   360/  375 batches | lr 10.00 | ms/batch 361.26 | loss  4.49 | ppl    89.30\n",
            "| epoch   2 |   370/  375 batches | lr 10.00 | ms/batch 360.74 | loss  4.49 | ppl    88.69\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 154.42s | valid loss  4.48 | valid ppl    88.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |    10/  375 batches | lr 10.00 | ms/batch 392.08 | loss  4.90 | ppl   134.53\n",
            "| epoch   3 |    20/  375 batches | lr 10.00 | ms/batch 360.10 | loss  4.42 | ppl    83.25\n",
            "| epoch   3 |    30/  375 batches | lr 10.00 | ms/batch 360.94 | loss  4.45 | ppl    85.35\n",
            "| epoch   3 |    40/  375 batches | lr 10.00 | ms/batch 362.64 | loss  4.43 | ppl    84.22\n",
            "| epoch   3 |    50/  375 batches | lr 10.00 | ms/batch 362.97 | loss  4.45 | ppl    85.91\n",
            "| epoch   3 |    60/  375 batches | lr 10.00 | ms/batch 363.48 | loss  4.40 | ppl    81.44\n",
            "| epoch   3 |    70/  375 batches | lr 10.00 | ms/batch 363.44 | loss  4.37 | ppl    79.44\n",
            "| epoch   3 |    80/  375 batches | lr 10.00 | ms/batch 363.51 | loss  4.40 | ppl    81.18\n",
            "| epoch   3 |    90/  375 batches | lr 10.00 | ms/batch 362.03 | loss  4.39 | ppl    80.72\n",
            "| epoch   3 |   100/  375 batches | lr 10.00 | ms/batch 360.76 | loss  4.37 | ppl    79.41\n",
            "| epoch   3 |   110/  375 batches | lr 10.00 | ms/batch 360.42 | loss  4.36 | ppl    78.06\n",
            "| epoch   3 |   120/  375 batches | lr 10.00 | ms/batch 359.10 | loss  4.34 | ppl    76.47\n",
            "| epoch   3 |   130/  375 batches | lr 10.00 | ms/batch 358.58 | loss  4.31 | ppl    74.26\n",
            "| epoch   3 |   140/  375 batches | lr 10.00 | ms/batch 358.38 | loss  4.35 | ppl    77.48\n",
            "| epoch   3 |   150/  375 batches | lr 10.00 | ms/batch 357.05 | loss  4.30 | ppl    73.74\n",
            "| epoch   3 |   160/  375 batches | lr 10.00 | ms/batch 356.19 | loss  4.34 | ppl    76.86\n",
            "| epoch   3 |   170/  375 batches | lr 10.00 | ms/batch 356.07 | loss  4.29 | ppl    73.32\n",
            "| epoch   3 |   180/  375 batches | lr 10.00 | ms/batch 356.59 | loss  4.31 | ppl    74.16\n",
            "| epoch   3 |   190/  375 batches | lr 10.00 | ms/batch 356.76 | loss  4.27 | ppl    71.87\n",
            "| epoch   3 |   200/  375 batches | lr 10.00 | ms/batch 356.99 | loss  4.25 | ppl    70.37\n",
            "| epoch   3 |   210/  375 batches | lr 10.00 | ms/batch 358.09 | loss  4.31 | ppl    74.23\n",
            "| epoch   3 |   220/  375 batches | lr 10.00 | ms/batch 358.64 | loss  4.28 | ppl    72.54\n",
            "| epoch   3 |   230/  375 batches | lr 10.00 | ms/batch 358.78 | loss  4.24 | ppl    69.56\n",
            "| epoch   3 |   240/  375 batches | lr 10.00 | ms/batch 358.94 | loss  4.24 | ppl    69.58\n",
            "| epoch   3 |   250/  375 batches | lr 10.00 | ms/batch 359.58 | loss  4.22 | ppl    68.10\n",
            "| epoch   3 |   260/  375 batches | lr 10.00 | ms/batch 359.65 | loss  4.19 | ppl    66.16\n",
            "| epoch   3 |   270/  375 batches | lr 10.00 | ms/batch 360.05 | loss  4.21 | ppl    67.15\n",
            "| epoch   3 |   280/  375 batches | lr 10.00 | ms/batch 360.02 | loss  4.18 | ppl    65.39\n",
            "| epoch   3 |   290/  375 batches | lr 10.00 | ms/batch 360.26 | loss  4.16 | ppl    64.32\n",
            "| epoch   3 |   300/  375 batches | lr 10.00 | ms/batch 360.61 | loss  4.22 | ppl    67.70\n",
            "| epoch   3 |   310/  375 batches | lr 10.00 | ms/batch 361.18 | loss  4.18 | ppl    65.64\n",
            "| epoch   3 |   320/  375 batches | lr 10.00 | ms/batch 360.93 | loss  4.17 | ppl    64.96\n",
            "| epoch   3 |   330/  375 batches | lr 10.00 | ms/batch 361.13 | loss  4.14 | ppl    62.65\n",
            "| epoch   3 |   340/  375 batches | lr 10.00 | ms/batch 361.09 | loss  4.12 | ppl    61.36\n",
            "| epoch   3 |   350/  375 batches | lr 10.00 | ms/batch 361.20 | loss  4.15 | ppl    63.71\n",
            "| epoch   3 |   360/  375 batches | lr 10.00 | ms/batch 361.19 | loss  4.15 | ppl    63.22\n",
            "| epoch   3 |   370/  375 batches | lr 10.00 | ms/batch 361.41 | loss  4.13 | ppl    62.16\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 154.59s | valid loss  4.14 | valid ppl    62.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |    10/  375 batches | lr 10.00 | ms/batch 392.87 | loss  4.53 | ppl    92.51\n",
            "| epoch   4 |    20/  375 batches | lr 10.00 | ms/batch 360.99 | loss  4.11 | ppl    60.69\n",
            "| epoch   4 |    30/  375 batches | lr 10.00 | ms/batch 361.62 | loss  4.12 | ppl    61.59\n",
            "| epoch   4 |    40/  375 batches | lr 10.00 | ms/batch 361.47 | loss  4.11 | ppl    61.00\n",
            "| epoch   4 |    50/  375 batches | lr 10.00 | ms/batch 362.51 | loss  4.13 | ppl    62.34\n",
            "| epoch   4 |    60/  375 batches | lr 10.00 | ms/batch 362.78 | loss  4.10 | ppl    60.05\n",
            "| epoch   4 |    70/  375 batches | lr 10.00 | ms/batch 363.01 | loss  4.11 | ppl    61.09\n",
            "| epoch   4 |    80/  375 batches | lr 10.00 | ms/batch 363.01 | loss  4.07 | ppl    58.65\n",
            "| epoch   4 |    90/  375 batches | lr 10.00 | ms/batch 362.10 | loss  4.09 | ppl    60.00\n",
            "| epoch   4 |   100/  375 batches | lr 10.00 | ms/batch 362.26 | loss  4.11 | ppl    60.84\n",
            "| epoch   4 |   110/  375 batches | lr 10.00 | ms/batch 361.63 | loss  4.06 | ppl    58.18\n",
            "| epoch   4 |   120/  375 batches | lr 10.00 | ms/batch 361.06 | loss  4.07 | ppl    58.73\n",
            "| epoch   4 |   130/  375 batches | lr 10.00 | ms/batch 360.58 | loss  4.02 | ppl    55.65\n",
            "| epoch   4 |   140/  375 batches | lr 10.00 | ms/batch 360.61 | loss  4.05 | ppl    57.67\n",
            "| epoch   4 |   150/  375 batches | lr 10.00 | ms/batch 359.94 | loss  4.03 | ppl    56.48\n",
            "| epoch   4 |   160/  375 batches | lr 10.00 | ms/batch 360.23 | loss  4.10 | ppl    60.11\n",
            "| epoch   4 |   170/  375 batches | lr 10.00 | ms/batch 359.32 | loss  4.03 | ppl    56.03\n",
            "| epoch   4 |   180/  375 batches | lr 10.00 | ms/batch 359.67 | loss  4.05 | ppl    57.60\n",
            "| epoch   4 |   190/  375 batches | lr 10.00 | ms/batch 359.10 | loss  4.01 | ppl    55.25\n",
            "| epoch   4 |   200/  375 batches | lr 10.00 | ms/batch 359.10 | loss  4.01 | ppl    54.93\n",
            "| epoch   4 |   210/  375 batches | lr 10.00 | ms/batch 358.56 | loss  4.04 | ppl    56.77\n",
            "| epoch   4 |   220/  375 batches | lr 10.00 | ms/batch 357.66 | loss  4.05 | ppl    57.35\n",
            "| epoch   4 |   230/  375 batches | lr 10.00 | ms/batch 358.19 | loss  4.00 | ppl    54.55\n",
            "| epoch   4 |   240/  375 batches | lr 10.00 | ms/batch 358.48 | loss  3.99 | ppl    54.08\n",
            "| epoch   4 |   250/  375 batches | lr 10.00 | ms/batch 357.89 | loss  3.99 | ppl    54.22\n",
            "| epoch   4 |   260/  375 batches | lr 10.00 | ms/batch 357.40 | loss  3.99 | ppl    53.85\n",
            "| epoch   4 |   270/  375 batches | lr 10.00 | ms/batch 358.06 | loss  3.99 | ppl    53.86\n",
            "| epoch   4 |   280/  375 batches | lr 10.00 | ms/batch 358.01 | loss  3.94 | ppl    51.50\n",
            "| epoch   4 |   290/  375 batches | lr 10.00 | ms/batch 358.57 | loss  3.95 | ppl    51.72\n",
            "| epoch   4 |   300/  375 batches | lr 10.00 | ms/batch 358.20 | loss  3.97 | ppl    53.19\n",
            "| epoch   4 |   310/  375 batches | lr 10.00 | ms/batch 357.74 | loss  3.94 | ppl    51.66\n",
            "| epoch   4 |   320/  375 batches | lr 10.00 | ms/batch 357.69 | loss  3.97 | ppl    53.17\n",
            "| epoch   4 |   330/  375 batches | lr 10.00 | ms/batch 358.85 | loss  3.93 | ppl    50.96\n",
            "| epoch   4 |   340/  375 batches | lr 10.00 | ms/batch 359.16 | loss  3.88 | ppl    48.59\n",
            "| epoch   4 |   350/  375 batches | lr 10.00 | ms/batch 359.55 | loss  3.94 | ppl    51.34\n",
            "| epoch   4 |   360/  375 batches | lr 10.00 | ms/batch 359.85 | loss  3.94 | ppl    51.60\n",
            "| epoch   4 |   370/  375 batches | lr 10.00 | ms/batch 359.64 | loss  3.92 | ppl    50.23\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 154.50s | valid loss  3.96 | valid ppl    52.70\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |    10/  375 batches | lr 10.00 | ms/batch 392.52 | loss  4.33 | ppl    75.74\n",
            "| epoch   5 |    20/  375 batches | lr 10.00 | ms/batch 362.49 | loss  3.90 | ppl    49.60\n",
            "| epoch   5 |    30/  375 batches | lr 10.00 | ms/batch 365.23 | loss  3.95 | ppl    51.94\n",
            "| epoch   5 |    40/  375 batches | lr 10.00 | ms/batch 364.75 | loss  3.93 | ppl    50.78\n",
            "| epoch   5 |    50/  375 batches | lr 10.00 | ms/batch 364.83 | loss  3.94 | ppl    51.55\n",
            "| epoch   5 |    60/  375 batches | lr 10.00 | ms/batch 364.64 | loss  3.89 | ppl    48.74\n",
            "| epoch   5 |    70/  375 batches | lr 10.00 | ms/batch 365.08 | loss  3.91 | ppl    49.70\n",
            "| epoch   5 |    80/  375 batches | lr 10.00 | ms/batch 364.85 | loss  3.89 | ppl    48.80\n",
            "| epoch   5 |    90/  375 batches | lr 10.00 | ms/batch 364.38 | loss  3.92 | ppl    50.19\n",
            "| epoch   5 |   100/  375 batches | lr 10.00 | ms/batch 363.95 | loss  3.92 | ppl    50.34\n",
            "| epoch   5 |   110/  375 batches | lr 10.00 | ms/batch 361.52 | loss  3.88 | ppl    48.25\n",
            "| epoch   5 |   120/  375 batches | lr 10.00 | ms/batch 360.61 | loss  3.89 | ppl    48.80\n",
            "| epoch   5 |   130/  375 batches | lr 10.00 | ms/batch 359.27 | loss  3.85 | ppl    46.85\n",
            "| epoch   5 |   140/  375 batches | lr 10.00 | ms/batch 358.06 | loss  3.88 | ppl    48.29\n",
            "| epoch   5 |   150/  375 batches | lr 10.00 | ms/batch 357.58 | loss  3.87 | ppl    47.97\n",
            "| epoch   5 |   160/  375 batches | lr 10.00 | ms/batch 356.93 | loss  3.89 | ppl    48.87\n",
            "| epoch   5 |   170/  375 batches | lr 10.00 | ms/batch 356.39 | loss  3.86 | ppl    47.42\n",
            "| epoch   5 |   180/  375 batches | lr 10.00 | ms/batch 357.14 | loss  3.90 | ppl    49.34\n",
            "| epoch   5 |   190/  375 batches | lr 10.00 | ms/batch 356.64 | loss  3.86 | ppl    47.26\n",
            "| epoch   5 |   200/  375 batches | lr 10.00 | ms/batch 357.04 | loss  3.84 | ppl    46.72\n",
            "| epoch   5 |   210/  375 batches | lr 10.00 | ms/batch 357.54 | loss  3.87 | ppl    47.75\n",
            "| epoch   5 |   220/  375 batches | lr 10.00 | ms/batch 357.53 | loss  3.89 | ppl    48.77\n",
            "| epoch   5 |   230/  375 batches | lr 10.00 | ms/batch 358.62 | loss  3.83 | ppl    46.10\n",
            "| epoch   5 |   240/  375 batches | lr 10.00 | ms/batch 359.15 | loss  3.83 | ppl    46.07\n",
            "| epoch   5 |   250/  375 batches | lr 10.00 | ms/batch 359.39 | loss  3.86 | ppl    47.23\n",
            "| epoch   5 |   260/  375 batches | lr 10.00 | ms/batch 359.02 | loss  3.82 | ppl    45.69\n",
            "| epoch   5 |   270/  375 batches | lr 10.00 | ms/batch 360.04 | loss  3.83 | ppl    46.08\n",
            "| epoch   5 |   280/  375 batches | lr 10.00 | ms/batch 359.70 | loss  3.79 | ppl    44.34\n",
            "| epoch   5 |   290/  375 batches | lr 10.00 | ms/batch 359.92 | loss  3.79 | ppl    44.09\n",
            "| epoch   5 |   300/  375 batches | lr 10.00 | ms/batch 360.63 | loss  3.83 | ppl    46.04\n",
            "| epoch   5 |   310/  375 batches | lr 10.00 | ms/batch 359.93 | loss  3.80 | ppl    44.65\n",
            "| epoch   5 |   320/  375 batches | lr 10.00 | ms/batch 360.79 | loss  3.81 | ppl    44.99\n",
            "| epoch   5 |   330/  375 batches | lr 10.00 | ms/batch 360.45 | loss  3.78 | ppl    43.67\n",
            "| epoch   5 |   340/  375 batches | lr 10.00 | ms/batch 360.51 | loss  3.74 | ppl    42.18\n",
            "| epoch   5 |   350/  375 batches | lr 10.00 | ms/batch 360.42 | loss  3.80 | ppl    44.53\n",
            "| epoch   5 |   360/  375 batches | lr 10.00 | ms/batch 360.58 | loss  3.80 | ppl    44.74\n",
            "| epoch   5 |   370/  375 batches | lr 10.00 | ms/batch 361.23 | loss  3.77 | ppl    43.34\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 154.82s | valid loss  3.84 | valid ppl    46.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |    10/  375 batches | lr 10.00 | ms/batch 391.80 | loss  4.15 | ppl    63.54\n",
            "| epoch   6 |    20/  375 batches | lr 10.00 | ms/batch 362.06 | loss  3.76 | ppl    43.10\n",
            "| epoch   6 |    30/  375 batches | lr 10.00 | ms/batch 363.99 | loss  3.79 | ppl    44.06\n",
            "| epoch   6 |    40/  375 batches | lr 10.00 | ms/batch 364.75 | loss  3.78 | ppl    43.88\n",
            "| epoch   6 |    50/  375 batches | lr 10.00 | ms/batch 365.16 | loss  3.82 | ppl    45.79\n",
            "| epoch   6 |    60/  375 batches | lr 10.00 | ms/batch 364.34 | loss  3.77 | ppl    43.19\n",
            "| epoch   6 |    70/  375 batches | lr 10.00 | ms/batch 365.22 | loss  3.76 | ppl    42.95\n",
            "| epoch   6 |    80/  375 batches | lr 10.00 | ms/batch 365.05 | loss  3.75 | ppl    42.48\n",
            "| epoch   6 |    90/  375 batches | lr 10.00 | ms/batch 362.52 | loss  3.77 | ppl    43.18\n",
            "| epoch   6 |   100/  375 batches | lr 10.00 | ms/batch 362.58 | loss  3.76 | ppl    43.10\n",
            "| epoch   6 |   110/  375 batches | lr 10.00 | ms/batch 362.04 | loss  3.76 | ppl    43.09\n",
            "| epoch   6 |   120/  375 batches | lr 10.00 | ms/batch 361.81 | loss  3.76 | ppl    42.90\n",
            "| epoch   6 |   130/  375 batches | lr 10.00 | ms/batch 361.21 | loss  3.72 | ppl    41.17\n",
            "| epoch   6 |   140/  375 batches | lr 10.00 | ms/batch 361.05 | loss  3.75 | ppl    42.44\n",
            "| epoch   6 |   150/  375 batches | lr 10.00 | ms/batch 359.75 | loss  3.74 | ppl    41.92\n",
            "| epoch   6 |   160/  375 batches | lr 10.00 | ms/batch 359.59 | loss  3.76 | ppl    43.02\n",
            "| epoch   6 |   170/  375 batches | lr 10.00 | ms/batch 359.33 | loss  3.73 | ppl    41.55\n",
            "| epoch   6 |   180/  375 batches | lr 10.00 | ms/batch 358.81 | loss  3.75 | ppl    42.45\n",
            "| epoch   6 |   190/  375 batches | lr 10.00 | ms/batch 359.08 | loss  3.71 | ppl    40.96\n",
            "| epoch   6 |   200/  375 batches | lr 10.00 | ms/batch 358.34 | loss  3.72 | ppl    41.15\n",
            "| epoch   6 |   210/  375 batches | lr 10.00 | ms/batch 358.73 | loss  3.75 | ppl    42.38\n",
            "| epoch   6 |   220/  375 batches | lr 10.00 | ms/batch 359.17 | loss  3.77 | ppl    43.38\n",
            "| epoch   6 |   230/  375 batches | lr 10.00 | ms/batch 359.55 | loss  3.70 | ppl    40.63\n",
            "| epoch   6 |   240/  375 batches | lr 10.00 | ms/batch 359.51 | loss  3.71 | ppl    40.69\n",
            "| epoch   6 |   250/  375 batches | lr 10.00 | ms/batch 360.29 | loss  3.69 | ppl    40.23\n",
            "| epoch   6 |   260/  375 batches | lr 10.00 | ms/batch 360.19 | loss  3.70 | ppl    40.37\n",
            "| epoch   6 |   270/  375 batches | lr 10.00 | ms/batch 360.85 | loss  3.70 | ppl    40.55\n",
            "| epoch   6 |   280/  375 batches | lr 10.00 | ms/batch 360.14 | loss  3.67 | ppl    39.14\n",
            "| epoch   6 |   290/  375 batches | lr 10.00 | ms/batch 360.65 | loss  3.65 | ppl    38.60\n",
            "| epoch   6 |   300/  375 batches | lr 10.00 | ms/batch 361.05 | loss  3.70 | ppl    40.58\n",
            "| epoch   6 |   310/  375 batches | lr 10.00 | ms/batch 361.90 | loss  3.70 | ppl    40.29\n",
            "| epoch   6 |   320/  375 batches | lr 10.00 | ms/batch 361.82 | loss  3.69 | ppl    39.92\n",
            "| epoch   6 |   330/  375 batches | lr 10.00 | ms/batch 361.48 | loss  3.64 | ppl    38.28\n",
            "| epoch   6 |   340/  375 batches | lr 10.00 | ms/batch 361.96 | loss  3.62 | ppl    37.40\n",
            "| epoch   6 |   350/  375 batches | lr 10.00 | ms/batch 361.60 | loss  3.68 | ppl    39.66\n",
            "| epoch   6 |   360/  375 batches | lr 10.00 | ms/batch 362.18 | loss  3.67 | ppl    39.41\n",
            "| epoch   6 |   370/  375 batches | lr 10.00 | ms/batch 362.48 | loss  3.67 | ppl    39.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 155.15s | valid loss  3.75 | valid ppl    42.70\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |    10/  375 batches | lr 10.00 | ms/batch 392.78 | loss  4.02 | ppl    55.56\n",
            "| epoch   7 |    20/  375 batches | lr 10.00 | ms/batch 361.20 | loss  3.64 | ppl    38.15\n",
            "| epoch   7 |    30/  375 batches | lr 10.00 | ms/batch 361.31 | loss  3.67 | ppl    39.29\n",
            "| epoch   7 |    40/  375 batches | lr 10.00 | ms/batch 361.46 | loss  3.66 | ppl    38.83\n",
            "| epoch   7 |    50/  375 batches | lr 10.00 | ms/batch 362.96 | loss  3.69 | ppl    40.20\n",
            "| epoch   7 |    60/  375 batches | lr 10.00 | ms/batch 362.91 | loss  3.65 | ppl    38.53\n",
            "| epoch   7 |    70/  375 batches | lr 10.00 | ms/batch 364.16 | loss  3.65 | ppl    38.33\n",
            "| epoch   7 |    80/  375 batches | lr 10.00 | ms/batch 363.44 | loss  3.63 | ppl    37.84\n",
            "| epoch   7 |    90/  375 batches | lr 10.00 | ms/batch 365.01 | loss  3.65 | ppl    38.56\n",
            "| epoch   7 |   100/  375 batches | lr 10.00 | ms/batch 363.08 | loss  3.65 | ppl    38.44\n",
            "| epoch   7 |   110/  375 batches | lr 10.00 | ms/batch 361.86 | loss  3.65 | ppl    38.40\n",
            "| epoch   7 |   120/  375 batches | lr 10.00 | ms/batch 362.07 | loss  3.65 | ppl    38.61\n",
            "| epoch   7 |   130/  375 batches | lr 10.00 | ms/batch 361.57 | loss  3.61 | ppl    36.94\n",
            "| epoch   7 |   140/  375 batches | lr 10.00 | ms/batch 361.18 | loss  3.64 | ppl    37.99\n",
            "| epoch   7 |   150/  375 batches | lr 10.00 | ms/batch 360.62 | loss  3.62 | ppl    37.27\n",
            "| epoch   7 |   160/  375 batches | lr 10.00 | ms/batch 360.82 | loss  3.64 | ppl    38.04\n",
            "| epoch   7 |   170/  375 batches | lr 10.00 | ms/batch 360.51 | loss  3.63 | ppl    37.63\n",
            "| epoch   7 |   180/  375 batches | lr 10.00 | ms/batch 360.21 | loss  3.63 | ppl    37.82\n",
            "| epoch   7 |   190/  375 batches | lr 10.00 | ms/batch 359.80 | loss  3.61 | ppl    36.82\n",
            "| epoch   7 |   200/  375 batches | lr 10.00 | ms/batch 359.62 | loss  3.61 | ppl    37.02\n",
            "| epoch   7 |   210/  375 batches | lr 10.00 | ms/batch 359.69 | loss  3.64 | ppl    38.25\n",
            "| epoch   7 |   220/  375 batches | lr 10.00 | ms/batch 359.54 | loss  3.64 | ppl    38.27\n",
            "| epoch   7 |   230/  375 batches | lr 10.00 | ms/batch 359.11 | loss  3.59 | ppl    36.19\n",
            "| epoch   7 |   240/  375 batches | lr 10.00 | ms/batch 359.22 | loss  3.60 | ppl    36.67\n",
            "| epoch   7 |   250/  375 batches | lr 10.00 | ms/batch 359.03 | loss  3.59 | ppl    36.39\n",
            "| epoch   7 |   260/  375 batches | lr 10.00 | ms/batch 359.19 | loss  3.59 | ppl    36.13\n",
            "| epoch   7 |   270/  375 batches | lr 10.00 | ms/batch 358.61 | loss  3.60 | ppl    36.47\n",
            "| epoch   7 |   280/  375 batches | lr 10.00 | ms/batch 358.88 | loss  3.55 | ppl    34.79\n",
            "| epoch   7 |   290/  375 batches | lr 10.00 | ms/batch 358.81 | loss  3.56 | ppl    35.19\n",
            "| epoch   7 |   300/  375 batches | lr 10.00 | ms/batch 359.57 | loss  3.60 | ppl    36.60\n",
            "| epoch   7 |   310/  375 batches | lr 10.00 | ms/batch 358.37 | loss  3.57 | ppl    35.53\n",
            "| epoch   7 |   320/  375 batches | lr 10.00 | ms/batch 358.70 | loss  3.56 | ppl    35.22\n",
            "| epoch   7 |   330/  375 batches | lr 10.00 | ms/batch 359.62 | loss  3.55 | ppl    34.93\n",
            "| epoch   7 |   340/  375 batches | lr 10.00 | ms/batch 358.22 | loss  3.51 | ppl    33.49\n",
            "| epoch   7 |   350/  375 batches | lr 10.00 | ms/batch 359.81 | loss  3.57 | ppl    35.35\n",
            "| epoch   7 |   360/  375 batches | lr 10.00 | ms/batch 359.20 | loss  3.57 | ppl    35.57\n",
            "| epoch   7 |   370/  375 batches | lr 10.00 | ms/batch 358.87 | loss  3.55 | ppl    34.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 154.73s | valid loss  3.65 | valid ppl    38.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |    10/  375 batches | lr 10.00 | ms/batch 396.39 | loss  3.92 | ppl    50.41\n",
            "| epoch   8 |    20/  375 batches | lr 10.00 | ms/batch 364.13 | loss  3.54 | ppl    34.36\n",
            "| epoch   8 |    30/  375 batches | lr 10.00 | ms/batch 364.88 | loss  3.56 | ppl    35.30\n",
            "| epoch   8 |    40/  375 batches | lr 10.00 | ms/batch 367.27 | loss  3.55 | ppl    34.94\n",
            "| epoch   8 |    50/  375 batches | lr 10.00 | ms/batch 368.21 | loss  3.59 | ppl    36.21\n",
            "| epoch   8 |    60/  375 batches | lr 10.00 | ms/batch 368.14 | loss  3.53 | ppl    34.21\n",
            "| epoch   8 |    70/  375 batches | lr 10.00 | ms/batch 367.96 | loss  3.54 | ppl    34.60\n",
            "| epoch   8 |    80/  375 batches | lr 10.00 | ms/batch 367.20 | loss  3.54 | ppl    34.34\n",
            "| epoch   8 |    90/  375 batches | lr 10.00 | ms/batch 366.78 | loss  3.55 | ppl    34.97\n",
            "| epoch   8 |   100/  375 batches | lr 10.00 | ms/batch 366.43 | loss  3.56 | ppl    35.32\n",
            "| epoch   8 |   110/  375 batches | lr 10.00 | ms/batch 364.45 | loss  3.55 | ppl    34.69\n",
            "| epoch   8 |   120/  375 batches | lr 10.00 | ms/batch 364.15 | loss  3.55 | ppl    34.66\n",
            "| epoch   8 |   130/  375 batches | lr 10.00 | ms/batch 361.88 | loss  3.51 | ppl    33.46\n",
            "| epoch   8 |   140/  375 batches | lr 10.00 | ms/batch 360.62 | loss  3.53 | ppl    34.17\n",
            "| epoch   8 |   150/  375 batches | lr 10.00 | ms/batch 359.70 | loss  3.52 | ppl    33.63\n",
            "| epoch   8 |   160/  375 batches | lr 10.00 | ms/batch 359.18 | loss  3.55 | ppl    34.82\n",
            "| epoch   8 |   170/  375 batches | lr 10.00 | ms/batch 357.87 | loss  3.54 | ppl    34.36\n",
            "| epoch   8 |   180/  375 batches | lr 10.00 | ms/batch 358.47 | loss  3.53 | ppl    33.97\n",
            "| epoch   8 |   190/  375 batches | lr 10.00 | ms/batch 358.83 | loss  3.51 | ppl    33.57\n",
            "| epoch   8 |   200/  375 batches | lr 10.00 | ms/batch 359.79 | loss  3.52 | ppl    33.64\n",
            "| epoch   8 |   210/  375 batches | lr 10.00 | ms/batch 360.13 | loss  3.53 | ppl    34.06\n",
            "| epoch   8 |   220/  375 batches | lr 10.00 | ms/batch 361.16 | loss  3.54 | ppl    34.54\n",
            "| epoch   8 |   230/  375 batches | lr 10.00 | ms/batch 361.55 | loss  3.50 | ppl    33.03\n",
            "| epoch   8 |   240/  375 batches | lr 10.00 | ms/batch 361.55 | loss  3.51 | ppl    33.47\n",
            "| epoch   8 |   250/  375 batches | lr 10.00 | ms/batch 361.82 | loss  3.50 | ppl    33.13\n",
            "| epoch   8 |   260/  375 batches | lr 10.00 | ms/batch 362.05 | loss  3.49 | ppl    32.68\n",
            "| epoch   8 |   270/  375 batches | lr 10.00 | ms/batch 362.45 | loss  3.50 | ppl    33.07\n",
            "| epoch   8 |   280/  375 batches | lr 10.00 | ms/batch 363.04 | loss  3.46 | ppl    31.81\n",
            "| epoch   8 |   290/  375 batches | lr 10.00 | ms/batch 364.43 | loss  3.49 | ppl    32.82\n",
            "| epoch   8 |   300/  375 batches | lr 10.00 | ms/batch 364.65 | loss  3.50 | ppl    33.17\n",
            "| epoch   8 |   310/  375 batches | lr 10.00 | ms/batch 365.05 | loss  3.48 | ppl    32.59\n",
            "| epoch   8 |   320/  375 batches | lr 10.00 | ms/batch 363.12 | loss  3.47 | ppl    32.20\n",
            "| epoch   8 |   330/  375 batches | lr 10.00 | ms/batch 363.37 | loss  3.46 | ppl    31.68\n",
            "| epoch   8 |   340/  375 batches | lr 10.00 | ms/batch 363.44 | loss  3.43 | ppl    30.86\n",
            "| epoch   8 |   350/  375 batches | lr 10.00 | ms/batch 363.19 | loss  3.48 | ppl    32.37\n",
            "| epoch   8 |   360/  375 batches | lr 10.00 | ms/batch 362.21 | loss  3.48 | ppl    32.32\n",
            "| epoch   8 |   370/  375 batches | lr 10.00 | ms/batch 364.25 | loss  3.45 | ppl    31.64\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 155.86s | valid loss  3.61 | valid ppl    36.87\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |    10/  375 batches | lr 10.00 | ms/batch 394.23 | loss  3.83 | ppl    46.09\n",
            "| epoch   9 |    20/  375 batches | lr 10.00 | ms/batch 362.23 | loss  3.46 | ppl    31.72\n",
            "| epoch   9 |    30/  375 batches | lr 10.00 | ms/batch 364.26 | loss  3.47 | ppl    32.18\n",
            "| epoch   9 |    40/  375 batches | lr 10.00 | ms/batch 365.09 | loss  3.46 | ppl    31.71\n",
            "| epoch   9 |    50/  375 batches | lr 10.00 | ms/batch 364.95 | loss  3.51 | ppl    33.38\n",
            "| epoch   9 |    60/  375 batches | lr 10.00 | ms/batch 365.46 | loss  3.46 | ppl    31.75\n",
            "| epoch   9 |    70/  375 batches | lr 10.00 | ms/batch 365.76 | loss  3.46 | ppl    31.68\n",
            "| epoch   9 |    80/  375 batches | lr 10.00 | ms/batch 364.99 | loss  3.44 | ppl    31.14\n",
            "| epoch   9 |    90/  375 batches | lr 10.00 | ms/batch 365.60 | loss  3.46 | ppl    31.78\n",
            "| epoch   9 |   100/  375 batches | lr 10.00 | ms/batch 365.10 | loss  3.46 | ppl    31.97\n",
            "| epoch   9 |   110/  375 batches | lr 10.00 | ms/batch 364.43 | loss  3.46 | ppl    31.94\n",
            "| epoch   9 |   120/  375 batches | lr 10.00 | ms/batch 362.03 | loss  3.45 | ppl    31.54\n",
            "| epoch   9 |   130/  375 batches | lr 10.00 | ms/batch 361.55 | loss  3.42 | ppl    30.47\n",
            "| epoch   9 |   140/  375 batches | lr 10.00 | ms/batch 360.73 | loss  3.44 | ppl    31.30\n",
            "| epoch   9 |   150/  375 batches | lr 10.00 | ms/batch 360.74 | loss  3.43 | ppl    30.87\n",
            "| epoch   9 |   160/  375 batches | lr 10.00 | ms/batch 360.42 | loss  3.45 | ppl    31.49\n",
            "| epoch   9 |   170/  375 batches | lr 10.00 | ms/batch 358.70 | loss  3.44 | ppl    31.05\n",
            "| epoch   9 |   180/  375 batches | lr 10.00 | ms/batch 359.81 | loss  3.45 | ppl    31.64\n",
            "| epoch   9 |   190/  375 batches | lr 10.00 | ms/batch 359.30 | loss  3.42 | ppl    30.69\n",
            "| epoch   9 |   200/  375 batches | lr 10.00 | ms/batch 359.52 | loss  3.43 | ppl    31.02\n",
            "| epoch   9 |   210/  375 batches | lr 10.00 | ms/batch 360.35 | loss  3.43 | ppl    30.85\n",
            "| epoch   9 |   220/  375 batches | lr 10.00 | ms/batch 359.70 | loss  3.46 | ppl    31.97\n",
            "| epoch   9 |   230/  375 batches | lr 10.00 | ms/batch 361.01 | loss  3.43 | ppl    30.87\n",
            "| epoch   9 |   240/  375 batches | lr 10.00 | ms/batch 361.15 | loss  3.42 | ppl    30.49\n",
            "| epoch   9 |   250/  375 batches | lr 10.00 | ms/batch 360.96 | loss  3.41 | ppl    30.39\n",
            "| epoch   9 |   260/  375 batches | lr 10.00 | ms/batch 361.38 | loss  3.40 | ppl    29.94\n",
            "| epoch   9 |   270/  375 batches | lr 10.00 | ms/batch 360.37 | loss  3.41 | ppl    30.20\n",
            "| epoch   9 |   280/  375 batches | lr 10.00 | ms/batch 361.15 | loss  3.37 | ppl    29.17\n",
            "| epoch   9 |   290/  375 batches | lr 10.00 | ms/batch 362.60 | loss  3.40 | ppl    30.10\n",
            "| epoch   9 |   300/  375 batches | lr 10.00 | ms/batch 361.54 | loss  3.41 | ppl    30.37\n",
            "| epoch   9 |   310/  375 batches | lr 10.00 | ms/batch 361.41 | loss  3.39 | ppl    29.56\n",
            "| epoch   9 |   320/  375 batches | lr 10.00 | ms/batch 361.60 | loss  3.39 | ppl    29.74\n",
            "| epoch   9 |   330/  375 batches | lr 10.00 | ms/batch 361.62 | loss  3.37 | ppl    28.96\n",
            "| epoch   9 |   340/  375 batches | lr 10.00 | ms/batch 361.57 | loss  3.34 | ppl    28.28\n",
            "| epoch   9 |   350/  375 batches | lr 10.00 | ms/batch 362.63 | loss  3.39 | ppl    29.69\n",
            "| epoch   9 |   360/  375 batches | lr 10.00 | ms/batch 362.11 | loss  3.39 | ppl    29.70\n",
            "| epoch   9 |   370/  375 batches | lr 10.00 | ms/batch 361.88 | loss  3.37 | ppl    29.13\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 155.43s | valid loss  3.58 | valid ppl    35.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |    10/  375 batches | lr 10.00 | ms/batch 394.34 | loss  3.72 | ppl    41.37\n",
            "| epoch  10 |    20/  375 batches | lr 10.00 | ms/batch 363.08 | loss  3.37 | ppl    29.00\n",
            "| epoch  10 |    30/  375 batches | lr 10.00 | ms/batch 363.66 | loss  3.39 | ppl    29.53\n",
            "| epoch  10 |    40/  375 batches | lr 10.00 | ms/batch 365.61 | loss  3.38 | ppl    29.31\n",
            "| epoch  10 |    50/  375 batches | lr 10.00 | ms/batch 365.79 | loss  3.42 | ppl    30.68\n",
            "| epoch  10 |    60/  375 batches | lr 10.00 | ms/batch 367.19 | loss  3.38 | ppl    29.36\n",
            "| epoch  10 |    70/  375 batches | lr 10.00 | ms/batch 365.46 | loss  3.37 | ppl    29.20\n",
            "| epoch  10 |    80/  375 batches | lr 10.00 | ms/batch 364.91 | loss  3.36 | ppl    28.73\n",
            "| epoch  10 |    90/  375 batches | lr 10.00 | ms/batch 364.57 | loss  3.38 | ppl    29.48\n",
            "| epoch  10 |   100/  375 batches | lr 10.00 | ms/batch 364.39 | loss  3.39 | ppl    29.70\n",
            "| epoch  10 |   110/  375 batches | lr 10.00 | ms/batch 363.39 | loss  3.38 | ppl    29.28\n",
            "| epoch  10 |   120/  375 batches | lr 10.00 | ms/batch 362.08 | loss  3.36 | ppl    28.86\n",
            "| epoch  10 |   130/  375 batches | lr 10.00 | ms/batch 360.69 | loss  3.34 | ppl    28.09\n",
            "| epoch  10 |   140/  375 batches | lr 10.00 | ms/batch 359.96 | loss  3.36 | ppl    28.66\n",
            "| epoch  10 |   150/  375 batches | lr 10.00 | ms/batch 361.16 | loss  3.36 | ppl    28.72\n",
            "| epoch  10 |   160/  375 batches | lr 10.00 | ms/batch 358.75 | loss  3.37 | ppl    29.10\n",
            "| epoch  10 |   170/  375 batches | lr 10.00 | ms/batch 358.69 | loss  3.35 | ppl    28.52\n",
            "| epoch  10 |   180/  375 batches | lr 10.00 | ms/batch 359.47 | loss  3.37 | ppl    29.14\n",
            "| epoch  10 |   190/  375 batches | lr 10.00 | ms/batch 358.60 | loss  3.34 | ppl    28.35\n",
            "| epoch  10 |   200/  375 batches | lr 10.00 | ms/batch 359.83 | loss  3.35 | ppl    28.40\n",
            "| epoch  10 |   210/  375 batches | lr 10.00 | ms/batch 360.78 | loss  3.36 | ppl    28.75\n",
            "| epoch  10 |   220/  375 batches | lr 10.00 | ms/batch 360.70 | loss  3.37 | ppl    29.13\n",
            "| epoch  10 |   230/  375 batches | lr 10.00 | ms/batch 360.77 | loss  3.34 | ppl    28.17\n",
            "| epoch  10 |   240/  375 batches | lr 10.00 | ms/batch 362.09 | loss  3.34 | ppl    28.24\n",
            "| epoch  10 |   250/  375 batches | lr 10.00 | ms/batch 362.46 | loss  3.34 | ppl    28.14\n",
            "| epoch  10 |   260/  375 batches | lr 10.00 | ms/batch 362.96 | loss  3.31 | ppl    27.50\n",
            "| epoch  10 |   270/  375 batches | lr 10.00 | ms/batch 362.85 | loss  3.33 | ppl    28.04\n",
            "| epoch  10 |   280/  375 batches | lr 10.00 | ms/batch 362.93 | loss  3.30 | ppl    27.15\n",
            "| epoch  10 |   290/  375 batches | lr 10.00 | ms/batch 362.90 | loss  3.30 | ppl    27.24\n",
            "| epoch  10 |   300/  375 batches | lr 10.00 | ms/batch 364.77 | loss  3.33 | ppl    27.84\n",
            "| epoch  10 |   310/  375 batches | lr 10.00 | ms/batch 364.30 | loss  3.32 | ppl    27.67\n",
            "| epoch  10 |   320/  375 batches | lr 10.00 | ms/batch 365.06 | loss  3.30 | ppl    27.25\n",
            "| epoch  10 |   330/  375 batches | lr 10.00 | ms/batch 364.87 | loss  3.29 | ppl    26.81\n",
            "| epoch  10 |   340/  375 batches | lr 10.00 | ms/batch 364.43 | loss  3.26 | ppl    25.92\n",
            "| epoch  10 |   350/  375 batches | lr 10.00 | ms/batch 362.76 | loss  3.30 | ppl    27.22\n",
            "| epoch  10 |   360/  375 batches | lr 10.00 | ms/batch 363.51 | loss  3.32 | ppl    27.65\n",
            "| epoch  10 |   370/  375 batches | lr 10.00 | ms/batch 363.81 | loss  3.29 | ppl    26.91\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 155.65s | valid loss  3.52 | valid ppl    33.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  11 |    10/  375 batches | lr 10.00 | ms/batch 396.40 | loss  3.63 | ppl    37.79\n",
            "| epoch  11 |    20/  375 batches | lr 10.00 | ms/batch 363.73 | loss  3.29 | ppl    26.74\n",
            "| epoch  11 |    30/  375 batches | lr 10.00 | ms/batch 364.66 | loss  3.30 | ppl    27.19\n",
            "| epoch  11 |    40/  375 batches | lr 10.00 | ms/batch 365.20 | loss  3.30 | ppl    27.25\n",
            "| epoch  11 |    50/  375 batches | lr 10.00 | ms/batch 366.23 | loss  3.34 | ppl    28.26\n",
            "| epoch  11 |    60/  375 batches | lr 10.00 | ms/batch 365.52 | loss  3.30 | ppl    27.04\n",
            "| epoch  11 |    70/  375 batches | lr 10.00 | ms/batch 365.65 | loss  3.29 | ppl    26.88\n",
            "| epoch  11 |    80/  375 batches | lr 10.00 | ms/batch 364.46 | loss  3.28 | ppl    26.45\n",
            "| epoch  11 |    90/  375 batches | lr 10.00 | ms/batch 364.52 | loss  3.30 | ppl    27.12\n",
            "| epoch  11 |   100/  375 batches | lr 10.00 | ms/batch 364.30 | loss  3.31 | ppl    27.31\n",
            "| epoch  11 |   110/  375 batches | lr 10.00 | ms/batch 363.18 | loss  3.29 | ppl    26.78\n",
            "| epoch  11 |   120/  375 batches | lr 10.00 | ms/batch 362.70 | loss  3.28 | ppl    26.62\n",
            "| epoch  11 |   130/  375 batches | lr 10.00 | ms/batch 362.55 | loss  3.26 | ppl    26.14\n",
            "| epoch  11 |   140/  375 batches | lr 10.00 | ms/batch 362.54 | loss  3.28 | ppl    26.50\n",
            "| epoch  11 |   150/  375 batches | lr 10.00 | ms/batch 361.22 | loss  3.28 | ppl    26.45\n",
            "| epoch  11 |   160/  375 batches | lr 10.00 | ms/batch 361.16 | loss  3.29 | ppl    26.73\n",
            "| epoch  11 |   170/  375 batches | lr 10.00 | ms/batch 360.65 | loss  3.28 | ppl    26.48\n",
            "| epoch  11 |   180/  375 batches | lr 10.00 | ms/batch 360.35 | loss  3.28 | ppl    26.63\n",
            "| epoch  11 |   190/  375 batches | lr 10.00 | ms/batch 360.70 | loss  3.28 | ppl    26.47\n",
            "| epoch  11 |   200/  375 batches | lr 10.00 | ms/batch 359.94 | loss  3.27 | ppl    26.40\n",
            "| epoch  11 |   210/  375 batches | lr 10.00 | ms/batch 360.14 | loss  3.28 | ppl    26.65\n",
            "| epoch  11 |   220/  375 batches | lr 10.00 | ms/batch 360.69 | loss  3.29 | ppl    26.79\n",
            "| epoch  11 |   230/  375 batches | lr 10.00 | ms/batch 360.49 | loss  3.25 | ppl    25.90\n",
            "| epoch  11 |   240/  375 batches | lr 10.00 | ms/batch 359.70 | loss  3.26 | ppl    25.97\n",
            "| epoch  11 |   250/  375 batches | lr 10.00 | ms/batch 360.82 | loss  3.26 | ppl    26.04\n",
            "| epoch  11 |   260/  375 batches | lr 10.00 | ms/batch 361.20 | loss  3.23 | ppl    25.34\n",
            "| epoch  11 |   270/  375 batches | lr 10.00 | ms/batch 361.39 | loss  3.26 | ppl    25.94\n",
            "| epoch  11 |   280/  375 batches | lr 10.00 | ms/batch 361.23 | loss  3.23 | ppl    25.27\n",
            "| epoch  11 |   290/  375 batches | lr 10.00 | ms/batch 361.65 | loss  3.22 | ppl    25.12\n",
            "| epoch  11 |   300/  375 batches | lr 10.00 | ms/batch 361.87 | loss  3.25 | ppl    25.86\n",
            "| epoch  11 |   310/  375 batches | lr 10.00 | ms/batch 361.83 | loss  3.25 | ppl    25.79\n",
            "| epoch  11 |   320/  375 batches | lr 10.00 | ms/batch 362.52 | loss  3.23 | ppl    25.25\n",
            "| epoch  11 |   330/  375 batches | lr 10.00 | ms/batch 362.19 | loss  3.21 | ppl    24.74\n",
            "| epoch  11 |   340/  375 batches | lr 10.00 | ms/batch 362.47 | loss  3.19 | ppl    24.20\n",
            "| epoch  11 |   350/  375 batches | lr 10.00 | ms/batch 362.40 | loss  3.24 | ppl    25.41\n",
            "| epoch  11 |   360/  375 batches | lr 10.00 | ms/batch 362.13 | loss  3.24 | ppl    25.57\n",
            "| epoch  11 |   370/  375 batches | lr 10.00 | ms/batch 362.56 | loss  3.22 | ppl    24.91\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time: 155.53s | valid loss  3.50 | valid ppl    33.25\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  12 |    10/  375 batches | lr 10.00 | ms/batch 397.00 | loss  3.55 | ppl    34.84\n",
            "| epoch  12 |    20/  375 batches | lr 10.00 | ms/batch 363.41 | loss  3.21 | ppl    24.81\n",
            "| epoch  12 |    30/  375 batches | lr 10.00 | ms/batch 364.34 | loss  3.22 | ppl    25.05\n",
            "| epoch  12 |    40/  375 batches | lr 10.00 | ms/batch 365.51 | loss  3.22 | ppl    25.04\n",
            "| epoch  12 |    50/  375 batches | lr 10.00 | ms/batch 366.70 | loss  3.26 | ppl    26.02\n",
            "| epoch  12 |    60/  375 batches | lr 10.00 | ms/batch 367.08 | loss  3.21 | ppl    24.87\n",
            "| epoch  12 |    70/  375 batches | lr 10.00 | ms/batch 366.59 | loss  3.22 | ppl    24.99\n",
            "| epoch  12 |    80/  375 batches | lr 10.00 | ms/batch 366.54 | loss  3.20 | ppl    24.49\n",
            "| epoch  12 |    90/  375 batches | lr 10.00 | ms/batch 364.56 | loss  3.22 | ppl    24.96\n",
            "| epoch  12 |   100/  375 batches | lr 10.00 | ms/batch 364.50 | loss  3.22 | ppl    25.11\n",
            "| epoch  12 |   110/  375 batches | lr 10.00 | ms/batch 363.51 | loss  3.21 | ppl    24.71\n",
            "| epoch  12 |   120/  375 batches | lr 10.00 | ms/batch 363.06 | loss  3.20 | ppl    24.65\n",
            "| epoch  12 |   130/  375 batches | lr 10.00 | ms/batch 361.80 | loss  3.19 | ppl    24.22\n",
            "| epoch  12 |   140/  375 batches | lr 10.00 | ms/batch 361.62 | loss  3.19 | ppl    24.31\n",
            "| epoch  12 |   150/  375 batches | lr 10.00 | ms/batch 360.82 | loss  3.19 | ppl    24.36\n",
            "| epoch  12 |   160/  375 batches | lr 10.00 | ms/batch 360.18 | loss  3.21 | ppl    24.78\n",
            "| epoch  12 |   170/  375 batches | lr 10.00 | ms/batch 360.02 | loss  3.20 | ppl    24.58\n",
            "| epoch  12 |   180/  375 batches | lr 10.00 | ms/batch 360.43 | loss  3.20 | ppl    24.60\n",
            "| epoch  12 |   190/  375 batches | lr 10.00 | ms/batch 360.06 | loss  3.20 | ppl    24.45\n",
            "| epoch  12 |   200/  375 batches | lr 10.00 | ms/batch 360.28 | loss  3.19 | ppl    24.27\n",
            "| epoch  12 |   210/  375 batches | lr 10.00 | ms/batch 360.88 | loss  3.20 | ppl    24.51\n",
            "| epoch  12 |   220/  375 batches | lr 10.00 | ms/batch 360.45 | loss  3.21 | ppl    24.86\n",
            "| epoch  12 |   230/  375 batches | lr 10.00 | ms/batch 361.05 | loss  3.17 | ppl    23.91\n",
            "| epoch  12 |   240/  375 batches | lr 10.00 | ms/batch 361.04 | loss  3.18 | ppl    24.04\n",
            "| epoch  12 |   250/  375 batches | lr 10.00 | ms/batch 361.55 | loss  3.17 | ppl    23.88\n",
            "| epoch  12 |   260/  375 batches | lr 10.00 | ms/batch 361.04 | loss  3.16 | ppl    23.63\n",
            "| epoch  12 |   270/  375 batches | lr 10.00 | ms/batch 361.66 | loss  3.18 | ppl    24.10\n",
            "| epoch  12 |   280/  375 batches | lr 10.00 | ms/batch 361.70 | loss  3.15 | ppl    23.32\n",
            "| epoch  12 |   290/  375 batches | lr 10.00 | ms/batch 362.06 | loss  3.15 | ppl    23.35\n",
            "| epoch  12 |   300/  375 batches | lr 10.00 | ms/batch 362.13 | loss  3.17 | ppl    23.78\n",
            "| epoch  12 |   310/  375 batches | lr 10.00 | ms/batch 362.00 | loss  3.17 | ppl    23.74\n",
            "| epoch  12 |   320/  375 batches | lr 10.00 | ms/batch 362.06 | loss  3.15 | ppl    23.34\n",
            "| epoch  12 |   330/  375 batches | lr 10.00 | ms/batch 363.12 | loss  3.13 | ppl    22.91\n",
            "| epoch  12 |   340/  375 batches | lr 10.00 | ms/batch 363.13 | loss  3.11 | ppl    22.37\n",
            "| epoch  12 |   350/  375 batches | lr 10.00 | ms/batch 362.64 | loss  3.16 | ppl    23.53\n",
            "| epoch  12 |   360/  375 batches | lr 10.00 | ms/batch 363.36 | loss  3.17 | ppl    23.79\n",
            "| epoch  12 |   370/  375 batches | lr 10.00 | ms/batch 363.16 | loss  3.14 | ppl    23.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time: 155.70s | valid loss  3.48 | valid ppl    32.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  13 |    10/  375 batches | lr 10.00 | ms/batch 396.25 | loss  3.47 | ppl    31.98\n",
            "| epoch  13 |    20/  375 batches | lr 10.00 | ms/batch 365.06 | loss  3.13 | ppl    22.94\n",
            "| epoch  13 |    30/  375 batches | lr 10.00 | ms/batch 365.82 | loss  3.15 | ppl    23.25\n",
            "| epoch  13 |    40/  375 batches | lr 10.00 | ms/batch 366.99 | loss  3.15 | ppl    23.29\n",
            "| epoch  13 |    50/  375 batches | lr 10.00 | ms/batch 368.00 | loss  3.17 | ppl    23.92\n",
            "| epoch  13 |    60/  375 batches | lr 10.00 | ms/batch 367.70 | loss  3.13 | ppl    22.95\n",
            "| epoch  13 |    70/  375 batches | lr 10.00 | ms/batch 367.34 | loss  3.14 | ppl    23.10\n",
            "| epoch  13 |    80/  375 batches | lr 10.00 | ms/batch 367.01 | loss  3.12 | ppl    22.76\n",
            "| epoch  13 |    90/  375 batches | lr 10.00 | ms/batch 366.63 | loss  3.14 | ppl    23.12\n",
            "| epoch  13 |   100/  375 batches | lr 10.00 | ms/batch 365.66 | loss  3.15 | ppl    23.40\n",
            "| epoch  13 |   110/  375 batches | lr 10.00 | ms/batch 364.44 | loss  3.14 | ppl    23.08\n",
            "| epoch  13 |   120/  375 batches | lr 10.00 | ms/batch 364.73 | loss  3.12 | ppl    22.74\n",
            "| epoch  13 |   130/  375 batches | lr 10.00 | ms/batch 363.93 | loss  3.10 | ppl    22.28\n",
            "| epoch  13 |   140/  375 batches | lr 10.00 | ms/batch 362.78 | loss  3.12 | ppl    22.70\n",
            "| epoch  13 |   150/  375 batches | lr 10.00 | ms/batch 362.10 | loss  3.12 | ppl    22.64\n",
            "| epoch  13 |   160/  375 batches | lr 10.00 | ms/batch 362.17 | loss  3.14 | ppl    23.04\n",
            "| epoch  13 |   170/  375 batches | lr 10.00 | ms/batch 360.47 | loss  3.13 | ppl    22.77\n",
            "| epoch  13 |   180/  375 batches | lr 10.00 | ms/batch 361.36 | loss  3.13 | ppl    22.98\n",
            "| epoch  13 |   190/  375 batches | lr 10.00 | ms/batch 361.72 | loss  3.12 | ppl    22.54\n",
            "| epoch  13 |   200/  375 batches | lr 10.00 | ms/batch 362.38 | loss  3.12 | ppl    22.56\n",
            "| epoch  13 |   210/  375 batches | lr 10.00 | ms/batch 362.15 | loss  3.12 | ppl    22.75\n",
            "| epoch  13 |   220/  375 batches | lr 10.00 | ms/batch 362.11 | loss  3.14 | ppl    23.10\n",
            "| epoch  13 |   230/  375 batches | lr 10.00 | ms/batch 362.19 | loss  3.10 | ppl    22.13\n",
            "| epoch  13 |   240/  375 batches | lr 10.00 | ms/batch 362.27 | loss  3.10 | ppl    22.27\n",
            "| epoch  13 |   250/  375 batches | lr 10.00 | ms/batch 362.65 | loss  3.09 | ppl    22.08\n",
            "| epoch  13 |   260/  375 batches | lr 10.00 | ms/batch 363.38 | loss  3.08 | ppl    21.79\n",
            "| epoch  13 |   270/  375 batches | lr 10.00 | ms/batch 363.76 | loss  3.10 | ppl    22.28\n",
            "| epoch  13 |   280/  375 batches | lr 10.00 | ms/batch 364.23 | loss  3.08 | ppl    21.66\n",
            "| epoch  13 |   290/  375 batches | lr 10.00 | ms/batch 364.62 | loss  3.07 | ppl    21.62\n",
            "| epoch  13 |   300/  375 batches | lr 10.00 | ms/batch 364.45 | loss  3.10 | ppl    22.29\n",
            "| epoch  13 |   310/  375 batches | lr 10.00 | ms/batch 364.63 | loss  3.09 | ppl    22.06\n",
            "| epoch  13 |   320/  375 batches | lr 10.00 | ms/batch 364.87 | loss  3.08 | ppl    21.82\n",
            "| epoch  13 |   330/  375 batches | lr 10.00 | ms/batch 364.26 | loss  3.07 | ppl    21.43\n",
            "| epoch  13 |   340/  375 batches | lr 10.00 | ms/batch 364.51 | loss  3.03 | ppl    20.70\n",
            "| epoch  13 |   350/  375 batches | lr 10.00 | ms/batch 364.55 | loss  3.08 | ppl    21.78\n",
            "| epoch  13 |   360/  375 batches | lr 10.00 | ms/batch 364.52 | loss  3.09 | ppl    21.98\n",
            "| epoch  13 |   370/  375 batches | lr 10.00 | ms/batch 364.77 | loss  3.06 | ppl    21.36\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time: 156.26s | valid loss  3.47 | valid ppl    32.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  14 |    10/  375 batches | lr 10.00 | ms/batch 395.50 | loss  3.39 | ppl    29.59\n",
            "| epoch  14 |    20/  375 batches | lr 10.00 | ms/batch 364.39 | loss  3.06 | ppl    21.36\n",
            "| epoch  14 |    30/  375 batches | lr 10.00 | ms/batch 364.81 | loss  3.07 | ppl    21.65\n",
            "| epoch  14 |    40/  375 batches | lr 10.00 | ms/batch 364.98 | loss  3.07 | ppl    21.57\n",
            "| epoch  14 |    50/  375 batches | lr 10.00 | ms/batch 366.55 | loss  3.10 | ppl    22.28\n",
            "| epoch  14 |    60/  375 batches | lr 10.00 | ms/batch 366.36 | loss  3.07 | ppl    21.59\n",
            "| epoch  14 |    70/  375 batches | lr 10.00 | ms/batch 365.85 | loss  3.07 | ppl    21.50\n",
            "| epoch  14 |    80/  375 batches | lr 10.00 | ms/batch 366.17 | loss  3.05 | ppl    21.18\n",
            "| epoch  14 |    90/  375 batches | lr 10.00 | ms/batch 364.66 | loss  3.07 | ppl    21.51\n",
            "| epoch  14 |   100/  375 batches | lr 10.00 | ms/batch 364.57 | loss  3.08 | ppl    21.80\n",
            "| epoch  14 |   110/  375 batches | lr 10.00 | ms/batch 364.61 | loss  3.06 | ppl    21.32\n",
            "| epoch  14 |   120/  375 batches | lr 10.00 | ms/batch 363.05 | loss  3.06 | ppl    21.22\n",
            "| epoch  14 |   130/  375 batches | lr 10.00 | ms/batch 361.94 | loss  3.03 | ppl    20.78\n",
            "| epoch  14 |   140/  375 batches | lr 10.00 | ms/batch 361.78 | loss  3.05 | ppl    21.09\n",
            "| epoch  14 |   150/  375 batches | lr 10.00 | ms/batch 361.48 | loss  3.05 | ppl    21.09\n",
            "| epoch  14 |   160/  375 batches | lr 10.00 | ms/batch 360.50 | loss  3.06 | ppl    21.26\n",
            "| epoch  14 |   170/  375 batches | lr 10.00 | ms/batch 360.24 | loss  3.06 | ppl    21.30\n",
            "| epoch  14 |   180/  375 batches | lr 10.00 | ms/batch 360.49 | loss  3.07 | ppl    21.44\n",
            "| epoch  14 |   190/  375 batches | lr 10.00 | ms/batch 360.16 | loss  3.04 | ppl    20.91\n",
            "| epoch  14 |   200/  375 batches | lr 10.00 | ms/batch 361.05 | loss  3.05 | ppl    21.03\n",
            "| epoch  14 |   210/  375 batches | lr 10.00 | ms/batch 360.51 | loss  3.04 | ppl    21.00\n",
            "| epoch  14 |   220/  375 batches | lr 10.00 | ms/batch 361.06 | loss  3.07 | ppl    21.49\n",
            "| epoch  14 |   230/  375 batches | lr 10.00 | ms/batch 361.88 | loss  3.03 | ppl    20.64\n",
            "| epoch  14 |   240/  375 batches | lr 10.00 | ms/batch 361.31 | loss  3.03 | ppl    20.65\n",
            "| epoch  14 |   250/  375 batches | lr 10.00 | ms/batch 362.39 | loss  3.02 | ppl    20.53\n",
            "| epoch  14 |   260/  375 batches | lr 10.00 | ms/batch 361.98 | loss  3.02 | ppl    20.43\n",
            "| epoch  14 |   270/  375 batches | lr 10.00 | ms/batch 361.36 | loss  3.03 | ppl    20.63\n",
            "| epoch  14 |   280/  375 batches | lr 10.00 | ms/batch 362.14 | loss  3.00 | ppl    20.07\n",
            "| epoch  14 |   290/  375 batches | lr 10.00 | ms/batch 361.68 | loss  3.00 | ppl    20.06\n",
            "| epoch  14 |   300/  375 batches | lr 10.00 | ms/batch 361.69 | loss  3.02 | ppl    20.54\n",
            "| epoch  14 |   310/  375 batches | lr 10.00 | ms/batch 362.21 | loss  3.02 | ppl    20.51\n",
            "| epoch  14 |   320/  375 batches | lr 10.00 | ms/batch 361.74 | loss  3.01 | ppl    20.27\n",
            "| epoch  14 |   330/  375 batches | lr 10.00 | ms/batch 362.39 | loss  2.99 | ppl    19.93\n",
            "| epoch  14 |   340/  375 batches | lr 10.00 | ms/batch 362.52 | loss  2.97 | ppl    19.40\n",
            "| epoch  14 |   350/  375 batches | lr 10.00 | ms/batch 362.61 | loss  3.02 | ppl    20.46\n",
            "| epoch  14 |   360/  375 batches | lr 10.00 | ms/batch 362.26 | loss  3.02 | ppl    20.46\n",
            "| epoch  14 |   370/  375 batches | lr 10.00 | ms/batch 362.68 | loss  2.99 | ppl    19.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time: 155.68s | valid loss  3.46 | valid ppl    31.94\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  15 |    10/  375 batches | lr 10.00 | ms/batch 395.99 | loss  3.31 | ppl    27.37\n",
            "| epoch  15 |    20/  375 batches | lr 10.00 | ms/batch 365.36 | loss  2.99 | ppl    19.92\n",
            "| epoch  15 |    30/  375 batches | lr 10.00 | ms/batch 365.81 | loss  3.02 | ppl    20.43\n",
            "| epoch  15 |    40/  375 batches | lr 10.00 | ms/batch 367.85 | loss  3.00 | ppl    20.17\n",
            "| epoch  15 |    50/  375 batches | lr 10.00 | ms/batch 368.03 | loss  3.03 | ppl    20.69\n",
            "| epoch  15 |    60/  375 batches | lr 10.00 | ms/batch 367.75 | loss  3.00 | ppl    20.12\n",
            "| epoch  15 |    70/  375 batches | lr 10.00 | ms/batch 367.92 | loss  3.00 | ppl    20.06\n",
            "| epoch  15 |    80/  375 batches | lr 10.00 | ms/batch 367.44 | loss  2.98 | ppl    19.73\n",
            "| epoch  15 |    90/  375 batches | lr 10.00 | ms/batch 366.28 | loss  3.00 | ppl    20.06\n",
            "| epoch  15 |   100/  375 batches | lr 10.00 | ms/batch 364.87 | loss  3.01 | ppl    20.33\n",
            "| epoch  15 |   110/  375 batches | lr 10.00 | ms/batch 365.21 | loss  2.99 | ppl    19.91\n",
            "| epoch  15 |   120/  375 batches | lr 10.00 | ms/batch 363.10 | loss  2.99 | ppl    19.87\n",
            "| epoch  15 |   130/  375 batches | lr 10.00 | ms/batch 362.75 | loss  2.97 | ppl    19.55\n",
            "| epoch  15 |   140/  375 batches | lr 10.00 | ms/batch 362.63 | loss  2.98 | ppl    19.68\n",
            "| epoch  15 |   150/  375 batches | lr 10.00 | ms/batch 361.58 | loss  2.98 | ppl    19.67\n",
            "| epoch  15 |   160/  375 batches | lr 10.00 | ms/batch 361.98 | loss  2.99 | ppl    19.97\n",
            "| epoch  15 |   170/  375 batches | lr 10.00 | ms/batch 360.54 | loss  2.98 | ppl    19.76\n",
            "| epoch  15 |   180/  375 batches | lr 10.00 | ms/batch 360.48 | loss  2.99 | ppl    19.86\n",
            "| epoch  15 |   190/  375 batches | lr 10.00 | ms/batch 361.84 | loss  2.97 | ppl    19.52\n",
            "| epoch  15 |   200/  375 batches | lr 10.00 | ms/batch 362.09 | loss  2.97 | ppl    19.43\n",
            "| epoch  15 |   210/  375 batches | lr 10.00 | ms/batch 362.31 | loss  2.99 | ppl    19.81\n",
            "| epoch  15 |   220/  375 batches | lr 10.00 | ms/batch 362.61 | loss  3.00 | ppl    20.06\n",
            "| epoch  15 |   230/  375 batches | lr 10.00 | ms/batch 363.33 | loss  2.96 | ppl    19.24\n",
            "| epoch  15 |   240/  375 batches | lr 10.00 | ms/batch 363.96 | loss  2.96 | ppl    19.35\n",
            "| epoch  15 |   250/  375 batches | lr 10.00 | ms/batch 364.68 | loss  2.96 | ppl    19.24\n",
            "| epoch  15 |   260/  375 batches | lr 10.00 | ms/batch 364.71 | loss  2.95 | ppl    19.04\n",
            "| epoch  15 |   270/  375 batches | lr 10.00 | ms/batch 364.71 | loss  2.96 | ppl    19.38\n",
            "| epoch  15 |   280/  375 batches | lr 10.00 | ms/batch 364.68 | loss  2.93 | ppl    18.79\n",
            "| epoch  15 |   290/  375 batches | lr 10.00 | ms/batch 364.84 | loss  2.93 | ppl    18.70\n",
            "| epoch  15 |   300/  375 batches | lr 10.00 | ms/batch 365.03 | loss  2.95 | ppl    19.16\n",
            "| epoch  15 |   310/  375 batches | lr 10.00 | ms/batch 365.36 | loss  2.95 | ppl    19.12\n",
            "| epoch  15 |   320/  375 batches | lr 10.00 | ms/batch 365.43 | loss  2.94 | ppl    18.86\n",
            "| epoch  15 |   330/  375 batches | lr 10.00 | ms/batch 364.93 | loss  2.93 | ppl    18.64\n",
            "| epoch  15 |   340/  375 batches | lr 10.00 | ms/batch 364.82 | loss  2.89 | ppl    18.07\n",
            "| epoch  15 |   350/  375 batches | lr 10.00 | ms/batch 364.26 | loss  2.95 | ppl    19.04\n",
            "| epoch  15 |   360/  375 batches | lr 10.00 | ms/batch 364.74 | loss  2.94 | ppl    18.99\n",
            "| epoch  15 |   370/  375 batches | lr 10.00 | ms/batch 364.82 | loss  2.92 | ppl    18.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time: 156.29s | valid loss  3.46 | valid ppl    31.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  16 |    10/  375 batches | lr 10.00 | ms/batch 395.38 | loss  3.24 | ppl    25.42\n",
            "| epoch  16 |    20/  375 batches | lr 10.00 | ms/batch 364.63 | loss  2.92 | ppl    18.59\n",
            "| epoch  16 |    30/  375 batches | lr 10.00 | ms/batch 364.83 | loss  2.95 | ppl    19.02\n",
            "| epoch  16 |    40/  375 batches | lr 10.00 | ms/batch 365.07 | loss  2.94 | ppl    18.94\n",
            "| epoch  16 |    50/  375 batches | lr 10.00 | ms/batch 365.79 | loss  2.96 | ppl    19.24\n",
            "| epoch  16 |    60/  375 batches | lr 10.00 | ms/batch 367.38 | loss  2.93 | ppl    18.68\n",
            "| epoch  16 |    70/  375 batches | lr 10.00 | ms/batch 366.98 | loss  2.93 | ppl    18.64\n",
            "| epoch  16 |    80/  375 batches | lr 10.00 | ms/batch 365.46 | loss  2.91 | ppl    18.42\n",
            "| epoch  16 |    90/  375 batches | lr 10.00 | ms/batch 365.16 | loss  2.93 | ppl    18.64\n",
            "| epoch  16 |   100/  375 batches | lr 10.00 | ms/batch 364.78 | loss  2.95 | ppl    19.01\n",
            "| epoch  16 |   110/  375 batches | lr 10.00 | ms/batch 363.99 | loss  2.93 | ppl    18.73\n",
            "| epoch  16 |   120/  375 batches | lr 10.00 | ms/batch 364.37 | loss  2.92 | ppl    18.54\n",
            "| epoch  16 |   130/  375 batches | lr 10.00 | ms/batch 363.17 | loss  2.90 | ppl    18.23\n",
            "| epoch  16 |   140/  375 batches | lr 10.00 | ms/batch 362.06 | loss  2.91 | ppl    18.30\n",
            "| epoch  16 |   150/  375 batches | lr 10.00 | ms/batch 362.07 | loss  2.91 | ppl    18.37\n",
            "| epoch  16 |   160/  375 batches | lr 10.00 | ms/batch 361.27 | loss  2.92 | ppl    18.56\n",
            "| epoch  16 |   170/  375 batches | lr 10.00 | ms/batch 360.09 | loss  2.92 | ppl    18.50\n",
            "| epoch  16 |   180/  375 batches | lr 10.00 | ms/batch 359.83 | loss  2.92 | ppl    18.58\n",
            "| epoch  16 |   190/  375 batches | lr 10.00 | ms/batch 360.00 | loss  2.91 | ppl    18.27\n",
            "| epoch  16 |   200/  375 batches | lr 10.00 | ms/batch 361.22 | loss  2.91 | ppl    18.33\n",
            "| epoch  16 |   210/  375 batches | lr 10.00 | ms/batch 360.45 | loss  2.92 | ppl    18.47\n",
            "| epoch  16 |   220/  375 batches | lr 10.00 | ms/batch 361.11 | loss  2.93 | ppl    18.71\n",
            "| epoch  16 |   230/  375 batches | lr 10.00 | ms/batch 361.50 | loss  2.89 | ppl    18.01\n",
            "| epoch  16 |   240/  375 batches | lr 10.00 | ms/batch 361.86 | loss  2.89 | ppl    18.04\n",
            "| epoch  16 |   250/  375 batches | lr 10.00 | ms/batch 361.49 | loss  2.89 | ppl    17.96\n",
            "| epoch  16 |   260/  375 batches | lr 10.00 | ms/batch 361.80 | loss  2.88 | ppl    17.77\n",
            "| epoch  16 |   270/  375 batches | lr 10.00 | ms/batch 361.33 | loss  2.90 | ppl    18.08\n",
            "| epoch  16 |   280/  375 batches | lr 10.00 | ms/batch 361.98 | loss  2.87 | ppl    17.59\n",
            "| epoch  16 |   290/  375 batches | lr 10.00 | ms/batch 362.16 | loss  2.87 | ppl    17.68\n",
            "| epoch  16 |   300/  375 batches | lr 10.00 | ms/batch 361.64 | loss  2.89 | ppl    18.00\n",
            "| epoch  16 |   310/  375 batches | lr 10.00 | ms/batch 362.30 | loss  2.88 | ppl    17.81\n",
            "| epoch  16 |   320/  375 batches | lr 10.00 | ms/batch 362.42 | loss  2.87 | ppl    17.63\n",
            "| epoch  16 |   330/  375 batches | lr 10.00 | ms/batch 362.76 | loss  2.86 | ppl    17.43\n",
            "| epoch  16 |   340/  375 batches | lr 10.00 | ms/batch 362.49 | loss  2.83 | ppl    16.99\n",
            "| epoch  16 |   350/  375 batches | lr 10.00 | ms/batch 362.45 | loss  2.88 | ppl    17.83\n",
            "| epoch  16 |   360/  375 batches | lr 10.00 | ms/batch 363.19 | loss  2.88 | ppl    17.88\n",
            "| epoch  16 |   370/  375 batches | lr 10.00 | ms/batch 362.50 | loss  2.86 | ppl    17.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time: 155.67s | valid loss  3.47 | valid ppl    32.20\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  17 |    10/  375 batches | lr 2.50 | ms/batch 400.75 | loss  3.17 | ppl    23.74\n",
            "| epoch  17 |    20/  375 batches | lr 2.50 | ms/batch 365.30 | loss  2.86 | ppl    17.45\n",
            "| epoch  17 |    30/  375 batches | lr 2.50 | ms/batch 365.66 | loss  2.87 | ppl    17.61\n",
            "| epoch  17 |    40/  375 batches | lr 2.50 | ms/batch 366.50 | loss  2.87 | ppl    17.60\n",
            "| epoch  17 |    50/  375 batches | lr 2.50 | ms/batch 367.12 | loss  2.90 | ppl    18.10\n",
            "| epoch  17 |    60/  375 batches | lr 2.50 | ms/batch 367.02 | loss  2.86 | ppl    17.46\n",
            "| epoch  17 |    70/  375 batches | lr 2.50 | ms/batch 367.00 | loss  2.86 | ppl    17.50\n",
            "| epoch  17 |    80/  375 batches | lr 2.50 | ms/batch 366.51 | loss  2.84 | ppl    17.14\n",
            "| epoch  17 |    90/  375 batches | lr 2.50 | ms/batch 365.40 | loss  2.86 | ppl    17.49\n",
            "| epoch  17 |   100/  375 batches | lr 2.50 | ms/batch 363.86 | loss  2.88 | ppl    17.87\n",
            "| epoch  17 |   110/  375 batches | lr 2.50 | ms/batch 362.78 | loss  2.86 | ppl    17.49\n",
            "| epoch  17 |   120/  375 batches | lr 2.50 | ms/batch 362.58 | loss  2.85 | ppl    17.31\n",
            "| epoch  17 |   130/  375 batches | lr 2.50 | ms/batch 361.85 | loss  2.84 | ppl    17.15\n",
            "| epoch  17 |   140/  375 batches | lr 2.50 | ms/batch 361.17 | loss  2.84 | ppl    17.16\n",
            "| epoch  17 |   150/  375 batches | lr 2.50 | ms/batch 360.12 | loss  2.85 | ppl    17.21\n",
            "| epoch  17 |   160/  375 batches | lr 2.50 | ms/batch 359.52 | loss  2.86 | ppl    17.41\n",
            "| epoch  17 |   170/  375 batches | lr 2.50 | ms/batch 359.66 | loss  2.85 | ppl    17.28\n",
            "| epoch  17 |   180/  375 batches | lr 2.50 | ms/batch 359.50 | loss  2.86 | ppl    17.41\n",
            "| epoch  17 |   190/  375 batches | lr 2.50 | ms/batch 360.23 | loss  2.84 | ppl    17.10\n",
            "| epoch  17 |   200/  375 batches | lr 2.50 | ms/batch 361.03 | loss  2.84 | ppl    17.10\n",
            "| epoch  17 |   210/  375 batches | lr 2.50 | ms/batch 360.67 | loss  2.85 | ppl    17.33\n",
            "| epoch  17 |   220/  375 batches | lr 2.50 | ms/batch 361.26 | loss  2.86 | ppl    17.51\n",
            "| epoch  17 |   230/  375 batches | lr 2.50 | ms/batch 361.20 | loss  2.82 | ppl    16.84\n",
            "| epoch  17 |   240/  375 batches | lr 2.50 | ms/batch 362.36 | loss  2.83 | ppl    16.88\n",
            "| epoch  17 |   250/  375 batches | lr 2.50 | ms/batch 362.52 | loss  2.82 | ppl    16.75\n",
            "| epoch  17 |   260/  375 batches | lr 2.50 | ms/batch 362.59 | loss  2.82 | ppl    16.80\n",
            "| epoch  17 |   270/  375 batches | lr 2.50 | ms/batch 362.75 | loss  2.83 | ppl    17.02\n",
            "| epoch  17 |   280/  375 batches | lr 2.50 | ms/batch 362.98 | loss  2.80 | ppl    16.47\n",
            "| epoch  17 |   290/  375 batches | lr 2.50 | ms/batch 363.72 | loss  2.80 | ppl    16.45\n",
            "| epoch  17 |   300/  375 batches | lr 2.50 | ms/batch 364.61 | loss  2.82 | ppl    16.81\n",
            "| epoch  17 |   310/  375 batches | lr 2.50 | ms/batch 364.56 | loss  2.82 | ppl    16.69\n",
            "| epoch  17 |   320/  375 batches | lr 2.50 | ms/batch 364.56 | loss  2.80 | ppl    16.51\n",
            "| epoch  17 |   330/  375 batches | lr 2.50 | ms/batch 364.39 | loss  2.79 | ppl    16.36\n",
            "| epoch  17 |   340/  375 batches | lr 2.50 | ms/batch 364.31 | loss  2.76 | ppl    15.83\n",
            "| epoch  17 |   350/  375 batches | lr 2.50 | ms/batch 364.94 | loss  2.81 | ppl    16.58\n",
            "| epoch  17 |   360/  375 batches | lr 2.50 | ms/batch 364.86 | loss  2.82 | ppl    16.77\n",
            "| epoch  17 |   370/  375 batches | lr 2.50 | ms/batch 363.78 | loss  2.79 | ppl    16.31\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time: 155.96s | valid loss  3.48 | valid ppl    32.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  18 |    10/  375 batches | lr 0.62 | ms/batch 400.70 | loss  3.11 | ppl    22.31\n",
            "| epoch  18 |    20/  375 batches | lr 0.62 | ms/batch 364.72 | loss  2.79 | ppl    16.36\n",
            "| epoch  18 |    30/  375 batches | lr 0.62 | ms/batch 364.64 | loss  2.81 | ppl    16.66\n",
            "| epoch  18 |    40/  375 batches | lr 0.62 | ms/batch 365.58 | loss  2.80 | ppl    16.49\n",
            "| epoch  18 |    50/  375 batches | lr 0.62 | ms/batch 366.87 | loss  2.83 | ppl    16.94\n",
            "| epoch  18 |    60/  375 batches | lr 0.62 | ms/batch 366.52 | loss  2.80 | ppl    16.39\n",
            "| epoch  18 |    70/  375 batches | lr 0.62 | ms/batch 366.37 | loss  2.80 | ppl    16.37\n",
            "| epoch  18 |    80/  375 batches | lr 0.62 | ms/batch 366.23 | loss  2.79 | ppl    16.22\n",
            "| epoch  18 |    90/  375 batches | lr 0.62 | ms/batch 364.47 | loss  2.80 | ppl    16.47\n",
            "| epoch  18 |   100/  375 batches | lr 0.62 | ms/batch 364.59 | loss  2.82 | ppl    16.70\n",
            "| epoch  18 |   110/  375 batches | lr 0.62 | ms/batch 363.99 | loss  2.80 | ppl    16.41\n",
            "| epoch  18 |   120/  375 batches | lr 0.62 | ms/batch 362.95 | loss  2.78 | ppl    16.11\n",
            "| epoch  18 |   130/  375 batches | lr 0.62 | ms/batch 361.94 | loss  2.78 | ppl    16.09\n",
            "| epoch  18 |   140/  375 batches | lr 0.62 | ms/batch 361.88 | loss  2.78 | ppl    16.14\n",
            "| epoch  18 |   150/  375 batches | lr 0.62 | ms/batch 360.93 | loss  2.78 | ppl    16.16\n",
            "| epoch  18 |   160/  375 batches | lr 0.62 | ms/batch 360.33 | loss  2.79 | ppl    16.25\n",
            "| epoch  18 |   170/  375 batches | lr 0.62 | ms/batch 360.45 | loss  2.78 | ppl    16.15\n",
            "| epoch  18 |   180/  375 batches | lr 0.62 | ms/batch 360.61 | loss  2.80 | ppl    16.37\n",
            "| epoch  18 |   190/  375 batches | lr 0.62 | ms/batch 360.84 | loss  2.78 | ppl    16.06\n",
            "| epoch  18 |   200/  375 batches | lr 0.62 | ms/batch 361.08 | loss  2.77 | ppl    15.93\n",
            "| epoch  18 |   210/  375 batches | lr 0.62 | ms/batch 361.22 | loss  2.78 | ppl    16.12\n",
            "| epoch  18 |   220/  375 batches | lr 0.62 | ms/batch 360.74 | loss  2.80 | ppl    16.37\n",
            "| epoch  18 |   230/  375 batches | lr 0.62 | ms/batch 361.24 | loss  2.76 | ppl    15.85\n",
            "| epoch  18 |   240/  375 batches | lr 0.62 | ms/batch 361.29 | loss  2.77 | ppl    15.89\n",
            "| epoch  18 |   250/  375 batches | lr 0.62 | ms/batch 361.35 | loss  2.76 | ppl    15.80\n",
            "| epoch  18 |   260/  375 batches | lr 0.62 | ms/batch 362.01 | loss  2.75 | ppl    15.60\n",
            "| epoch  18 |   270/  375 batches | lr 0.62 | ms/batch 361.59 | loss  2.77 | ppl    15.93\n",
            "| epoch  18 |   280/  375 batches | lr 0.62 | ms/batch 362.46 | loss  2.74 | ppl    15.43\n",
            "| epoch  18 |   290/  375 batches | lr 0.62 | ms/batch 362.12 | loss  2.74 | ppl    15.54\n",
            "| epoch  18 |   300/  375 batches | lr 0.62 | ms/batch 361.36 | loss  2.76 | ppl    15.73\n",
            "| epoch  18 |   310/  375 batches | lr 0.62 | ms/batch 362.37 | loss  2.76 | ppl    15.77\n",
            "| epoch  18 |   320/  375 batches | lr 0.62 | ms/batch 361.99 | loss  2.75 | ppl    15.62\n",
            "| epoch  18 |   330/  375 batches | lr 0.62 | ms/batch 361.71 | loss  2.74 | ppl    15.46\n",
            "| epoch  18 |   340/  375 batches | lr 0.62 | ms/batch 362.44 | loss  2.70 | ppl    14.93\n",
            "| epoch  18 |   350/  375 batches | lr 0.62 | ms/batch 362.28 | loss  2.75 | ppl    15.58\n",
            "| epoch  18 |   360/  375 batches | lr 0.62 | ms/batch 363.26 | loss  2.75 | ppl    15.70\n",
            "| epoch  18 |   370/  375 batches | lr 0.62 | ms/batch 362.17 | loss  2.73 | ppl    15.35\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time: 155.68s | valid loss  3.50 | valid ppl    33.02\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  19 |    10/  375 batches | lr 0.16 | ms/batch 400.85 | loss  3.03 | ppl    20.71\n",
            "| epoch  19 |    20/  375 batches | lr 0.16 | ms/batch 364.72 | loss  2.73 | ppl    15.39\n",
            "| epoch  19 |    30/  375 batches | lr 0.16 | ms/batch 365.09 | loss  2.74 | ppl    15.55\n",
            "| epoch  19 |    40/  375 batches | lr 0.16 | ms/batch 366.75 | loss  2.74 | ppl    15.50\n",
            "| epoch  19 |    50/  375 batches | lr 0.16 | ms/batch 367.77 | loss  2.77 | ppl    15.92\n",
            "| epoch  19 |    60/  375 batches | lr 0.16 | ms/batch 366.49 | loss  2.73 | ppl    15.40\n",
            "| epoch  19 |    70/  375 batches | lr 0.16 | ms/batch 366.80 | loss  2.73 | ppl    15.41\n",
            "| epoch  19 |    80/  375 batches | lr 0.16 | ms/batch 365.76 | loss  2.72 | ppl    15.18\n",
            "| epoch  19 |    90/  375 batches | lr 0.16 | ms/batch 364.81 | loss  2.74 | ppl    15.48\n",
            "| epoch  19 |   100/  375 batches | lr 0.16 | ms/batch 364.51 | loss  2.76 | ppl    15.73\n",
            "| epoch  19 |   110/  375 batches | lr 0.16 | ms/batch 363.51 | loss  2.74 | ppl    15.46\n",
            "| epoch  19 |   120/  375 batches | lr 0.16 | ms/batch 362.06 | loss  2.72 | ppl    15.20\n",
            "| epoch  19 |   130/  375 batches | lr 0.16 | ms/batch 361.55 | loss  2.71 | ppl    15.08\n",
            "| epoch  19 |   140/  375 batches | lr 0.16 | ms/batch 361.55 | loss  2.72 | ppl    15.12\n",
            "| epoch  19 |   150/  375 batches | lr 0.16 | ms/batch 360.72 | loss  2.72 | ppl    15.18\n",
            "| epoch  19 |   160/  375 batches | lr 0.16 | ms/batch 359.69 | loss  2.74 | ppl    15.48\n",
            "| epoch  19 |   170/  375 batches | lr 0.16 | ms/batch 359.61 | loss  2.72 | ppl    15.18\n",
            "| epoch  19 |   180/  375 batches | lr 0.16 | ms/batch 359.57 | loss  2.73 | ppl    15.34\n",
            "| epoch  19 |   190/  375 batches | lr 0.16 | ms/batch 359.65 | loss  2.72 | ppl    15.13\n",
            "| epoch  19 |   200/  375 batches | lr 0.16 | ms/batch 360.69 | loss  2.71 | ppl    15.02\n",
            "| epoch  19 |   210/  375 batches | lr 0.16 | ms/batch 360.93 | loss  2.72 | ppl    15.17\n",
            "| epoch  19 |   220/  375 batches | lr 0.16 | ms/batch 361.46 | loss  2.74 | ppl    15.46\n",
            "| epoch  19 |   230/  375 batches | lr 0.16 | ms/batch 361.55 | loss  2.71 | ppl    14.98\n",
            "| epoch  19 |   240/  375 batches | lr 0.16 | ms/batch 362.02 | loss  2.70 | ppl    14.93\n",
            "| epoch  19 |   250/  375 batches | lr 0.16 | ms/batch 361.72 | loss  2.71 | ppl    14.97\n",
            "| epoch  19 |   260/  375 batches | lr 0.16 | ms/batch 362.75 | loss  2.69 | ppl    14.70\n",
            "| epoch  19 |   270/  375 batches | lr 0.16 | ms/batch 363.44 | loss  2.71 | ppl    15.02\n",
            "| epoch  19 |   280/  375 batches | lr 0.16 | ms/batch 361.79 | loss  2.68 | ppl    14.59\n",
            "| epoch  19 |   290/  375 batches | lr 0.16 | ms/batch 363.07 | loss  2.68 | ppl    14.54\n",
            "| epoch  19 |   300/  375 batches | lr 0.16 | ms/batch 362.90 | loss  2.70 | ppl    14.83\n",
            "| epoch  19 |   310/  375 batches | lr 0.16 | ms/batch 364.04 | loss  2.69 | ppl    14.75\n",
            "| epoch  19 |   320/  375 batches | lr 0.16 | ms/batch 363.07 | loss  2.68 | ppl    14.64\n",
            "| epoch  19 |   330/  375 batches | lr 0.16 | ms/batch 364.41 | loss  2.68 | ppl    14.59\n",
            "| epoch  19 |   340/  375 batches | lr 0.16 | ms/batch 364.44 | loss  2.64 | ppl    14.08\n",
            "| epoch  19 |   350/  375 batches | lr 0.16 | ms/batch 365.05 | loss  2.69 | ppl    14.75\n",
            "| epoch  19 |   360/  375 batches | lr 0.16 | ms/batch 364.49 | loss  2.70 | ppl    14.87\n",
            "| epoch  19 |   370/  375 batches | lr 0.16 | ms/batch 364.71 | loss  2.67 | ppl    14.43\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time: 155.95s | valid loss  3.50 | valid ppl    33.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  20 |    10/  375 batches | lr 0.04 | ms/batch 401.33 | loss  2.97 | ppl    19.48\n",
            "| epoch  20 |    20/  375 batches | lr 0.04 | ms/batch 364.31 | loss  2.67 | ppl    14.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "Exiting from training early\n",
            "=========================================================================================\n",
            "| End of training | test loss  3.47 | test ppl    32.09\n",
            "=========================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tU8fnxYxMsWX"
      },
      "source": [
        "#from nltk.translate.bleu_score import sentence_bleu\r\n",
        "#from nltk.translate.bleu_score import SmoothingFunction\r\n",
        "#smoothie = SmoothingFunction().method4\r\n",
        "#reference = train_data.tolist()\r\n",
        "n_words = 200\r\n",
        "def pretty_print(words, beam=False, filename=''):\r\n",
        "  '''candidate = [corpus.dictionary.word2idx[word] for word in words]\r\n",
        "  print('Cumulative 1-gram: %f' % sentence_bleu(reference, candidate, weights=(1, 0, 0, 0)))\r\n",
        "  print('Cumulative 2-gram: %f' % sentence_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0)))\r\n",
        "  print('Cumulative 3-gram: %f' % sentence_bleu(reference, candidate, weights=(0.33, 0.33, 0.33, 0)))\r\n",
        "  print('Cumulative 4-gram: %f' % sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothie))'''\r\n",
        "  pretty_mat = []\r\n",
        "  i,last_eos = 0, 0\r\n",
        "  for j in range(len(words)-1,-1,-1):\r\n",
        "    if words[j] == '<eos>':\r\n",
        "      last_eos = j\r\n",
        "      break\r\n",
        "  while i<last_eos:\r\n",
        "    line = []\r\n",
        "    while (words[i] != '<eos>'):\r\n",
        "      isApost = False\r\n",
        "      if not(words[i] in eos_tokens):\r\n",
        "        if i>0:\r\n",
        "          if ('’' in words[i-1]):\r\n",
        "            line[-1] += words[i]\r\n",
        "            isApost = True\r\n",
        "        if not(isApost):\r\n",
        "          line.append(words[i])\r\n",
        "      else :\r\n",
        "        words[i] = words[i-1]\r\n",
        "      i +=1\r\n",
        "    i +=1\r\n",
        "    if (pretty_mat == []) & (beam):\r\n",
        "      pretty_mat.append(line)\r\n",
        "    else:\r\n",
        "      pretty_mat.append(line[1:])\r\n",
        "  if filename =='':\r\n",
        "    for line in pretty_mat :\r\n",
        "      print(' '.join(line))\r\n",
        "    print('\\n')\r\n",
        "  else :\r\n",
        "    with open(filename, 'a') as f:\r\n",
        "      for line in pretty_mat :\r\n",
        "        f.write(' '.join(line)+'\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ikajkc9I3Sl2"
      },
      "source": [
        "def beam_search_decode(device, net, words, vocab_to_int, int_to_vocab, top_k, temperature, pprint=True):\n",
        "  net.eval()\n",
        "  softmax = nn.Softmax(dim = -1)\n",
        "  words = words.split(' ')\n",
        "  words.append('<sos>')\n",
        "  hidden = net.init_hidden(1)\n",
        "  for v in hidden:\n",
        "    v = v.to(device)\n",
        "  for w in words:\n",
        "    ix = torch.tensor([[vocab_to_int[w]]]).to(device)\n",
        "    output, hidden = net(ix, hidden)\n",
        "  output = output / temperature\n",
        "  prob, top_ix = torch.topk(softmax(output[0]), k=top_k)\n",
        "  prob = torch.log(prob)\n",
        "  #print(top_ix)\n",
        "  list_ids = [[id] for id in top_ix[0].tolist()]\n",
        "  outputs = [output for _ in range(top_k)]\n",
        "  hiddens = [hidden for _ in range(top_k)] \n",
        "  #print(\"avant beam search, top indices : \",top_ix.tolist(), \"de proba\", prob.tolist())\n",
        "  #print(list_ids)\n",
        "  for i in range(n_words):\n",
        "    probas = torch.zeros(top_k, top_k).float().to(device)\n",
        "    indxes = torch.zeros(top_k, top_k).to(device)\n",
        "    for k in range(top_k):\n",
        "      ix = torch.tensor([[top_ix[0][k]]]).to(device)\n",
        "      output, hiddens[k] = net(ix, hiddens[k])\n",
        "      output = output / temperature\n",
        "      pro, indxes[k] = torch.topk(softmax(output[0]), k=top_k)\n",
        "      pro = torch.log(pro)\n",
        "      #print(\"probas du choix \", k+1,\" : \", pro.tolist())\n",
        "      probas[k] = torch.add(pro[0], prob[0][k])\n",
        "    #print(indxes.tolist())\n",
        "    #print(list_ids)\n",
        "    prob, indices = torch.topk(probas.flatten(), top_k)\n",
        "    prob = torch.unsqueeze(prob, 0)\n",
        "    for k in range(top_k):\n",
        "      top_ix[0][k] = indxes.flatten()[indices[k]]\n",
        "    indices = indices // top_k\n",
        "    temp1 = []\n",
        "    temp2 = []\n",
        "    for k in range(top_k):\n",
        "      temp1.append(hiddens[indices.tolist()[k]])\n",
        "      temp2.append(list_ids[indices.tolist()[k]] + [top_ix[0].tolist()[k]])\n",
        "    hiddens = temp1\n",
        "    list_ids = temp2\n",
        "    #print(\"top indices : \",top_ix.tolist(), \"de proba\", prob.tolist())\n",
        "  best_branch = list_ids[torch.argmax(prob)]\n",
        "  words = []\n",
        "  for id in best_branch:\n",
        "    words.append(int_to_vocab[id])\n",
        "  if pprint:\n",
        "    print('beam', top_k, temperature)\n",
        "    pretty_print(words, beam=True)\n",
        "  else:\n",
        "    return words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLCMetacxM6v"
      },
      "source": [
        "def predict(device, net, words, vocab_to_int, int_to_vocab, temperature, pprint=True):\n",
        "  net.eval()\n",
        "  softmax = nn.Softmax(dim=-1)\n",
        "  words = words.split(' ')\n",
        "  hidden = net.init_hidden(1)\n",
        "  for v in hidden:\n",
        "    v = v.to(device)\n",
        "  for w in words:\n",
        "    ix = torch.tensor([[vocab_to_int[w]]]).to(device)\n",
        "    output, hidden = net(ix, hidden)\n",
        "  output = output / temperature\n",
        "  idx_max = torch.argmax(softmax(output[0]))\n",
        "  words = []\n",
        "  words.append(int_to_vocab[idx_max])\n",
        "  for i in range(n_words):\n",
        "      ix = torch.tensor([[idx_max]]).to(device)\n",
        "      output, hidden = net(ix, hidden)\n",
        "      output = output / temperature\n",
        "      idx_max = torch.argmax(softmax(output[0]))\n",
        "      words.append(int_to_vocab[idx_max])\n",
        "  if pprint:\n",
        "    print('greedy')\n",
        "    pretty_print(words)\n",
        "  else:\n",
        "    return words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s22J_29pRd5r"
      },
      "source": [
        "def top_k_sampling(device, net, words, vocab_to_int, int_to_vocab, top_k, temperature, pprint=True):\r\n",
        "  net.eval()\r\n",
        "  softmax = nn.Softmax(dim=-1)\r\n",
        "  words = words.split(' ')\r\n",
        "  hidden = net.init_hidden(1)\r\n",
        "  for v in hidden:\r\n",
        "    v = v.to(device)\r\n",
        "  for w in words:\r\n",
        "    ix = torch.tensor([[vocab_to_int[w]]]).to(device)\r\n",
        "    output, hidden = net(ix, hidden)\r\n",
        "  output = output / temperature\r\n",
        "  indices_to_remove = output[0] < torch.topk(output[0], top_k)[0][..., -1, None]\r\n",
        "  output[0][indices_to_remove] = -float('Inf')\r\n",
        "  prob = softmax(output[0])\r\n",
        "  idx_max = torch.multinomial(prob, 1)\r\n",
        "  words = []\r\n",
        "  words.append(int_to_vocab[idx_max])\r\n",
        "  for i in range(n_words):\r\n",
        "      ix = torch.tensor([[idx_max]]).to(device)\r\n",
        "      output, hidden = net(ix, hidden)\r\n",
        "      output = output[0] / temperature\r\n",
        "      indices_to_remove = output < torch.topk(output, top_k)[0][..., -1, None]\r\n",
        "      output[indices_to_remove] = -float('Inf')\r\n",
        "      prob = softmax(output)\r\n",
        "      idx_max = torch.multinomial(prob, 1)\r\n",
        "      words.append(int_to_vocab[idx_max])\r\n",
        "  if pprint:\r\n",
        "    print('top-k', top_k, temperature)\r\n",
        "    pretty_print(words)\r\n",
        "  else:\r\n",
        "    return words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bP4NmbsY-vo1"
      },
      "source": [
        "def top_p_sampling(device, net, words, vocab_to_int, int_to_vocab, top_p, temperature, pprint=True):\r\n",
        "  net.eval()\r\n",
        "  softmax = nn.Softmax(dim=-1)\r\n",
        "  words = words.split(' ')\r\n",
        "  hidden = net.init_hidden(1)\r\n",
        "  for v in hidden:\r\n",
        "    v = v.to(device)\r\n",
        "  for w in words:\r\n",
        "    ix = torch.tensor([[vocab_to_int[w]]]).to(device)\r\n",
        "    output, hidden = net(ix, hidden)\r\n",
        "  output = output / temperature\r\n",
        "  cum_prob = 0.0\r\n",
        "  incr = 0\r\n",
        "  probs, indices = torch.sort(softmax(output[0][0]), descending=True)\r\n",
        "  while cum_prob < top_p:\r\n",
        "    cum_prob += probs[incr]\r\n",
        "    incr += 1\r\n",
        "  indices_to_remove = indices[incr:]\r\n",
        "  output[0][0][indices_to_remove] = -float('Inf')\r\n",
        "  prob = softmax(output[0])\r\n",
        "  idx_max = torch.multinomial(prob, 1)\r\n",
        "  words = []\r\n",
        "  words.append(int_to_vocab[idx_max])\r\n",
        "  for i in range(n_words):\r\n",
        "      ix = torch.tensor([[idx_max]]).to(device)\r\n",
        "      output, hidden = net(ix, hidden)\r\n",
        "      output = output[0][0] / temperature\r\n",
        "      cum_prob = 0.0\r\n",
        "      incr = 0\r\n",
        "      probs, indices = torch.sort(softmax(output), descending=True)\r\n",
        "      while cum_prob < top_p:\r\n",
        "        cum_prob += probs[incr]\r\n",
        "        incr += 1\r\n",
        "      indices_to_remove = indices[incr:]\r\n",
        "      #print(len(output) - len(indices_to_remove), probs[0].tolist(), probs[incr-1].tolist())\r\n",
        "      output[indices_to_remove] = -float('Inf')\r\n",
        "      prob = softmax(output)\r\n",
        "      idx_max = torch.multinomial(prob, 1)\r\n",
        "      words.append(int_to_vocab[idx_max])\r\n",
        "  if pprint:\r\n",
        "    print('top-p', top_p, temperature)\r\n",
        "    pretty_print(words)\r\n",
        "  else:\r\n",
        "    return words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9V_Q2YLG0qCB",
        "outputId": "f3a64134-97f6-458a-a7a5-b098610ee01f"
      },
      "source": [
        "words = '<sos> Je ne t’ en parle plus , va , sers la tyrannie , <eos> <sos> Abandonne ton âme à son lâche génie ; <eos>'\n",
        "#words = '<sos> Je ne t’ en parle plus , va , sers la -i-[e] tyrannie , <eos> <sos> Abandonne ton âme à son lâche -i-[e] génie ; <eos>'\n",
        "predict(device, model, words, corpus.dictionary.word2idx, corpus.dictionary.idx2word,1)\n",
        "beam_search_decode(device, model, words, corpus.dictionary.word2idx, corpus.dictionary.idx2word,20,1)\n",
        "top_k_sampling(device, model, words, corpus.dictionary.word2idx, corpus.dictionary.idx2word, 50, 1)\n",
        "top_k_sampling(device, model, words, corpus.dictionary.word2idx, corpus.dictionary.idx2word, 100, 0.7)\n",
        "top_p_sampling(device, model, words, corpus.dictionary.word2idx, corpus.dictionary.idx2word, 0.9, 1)\n",
        "top_p_sampling(device, model, words, corpus.dictionary.word2idx, corpus.dictionary.idx2word, 0.7, 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "greedy\n",
            "Et si tu m’as tantôt la vie pour le crime ,\n",
            "Je me suis fait à toi , si tu n’es pas le cœur ,\n",
            "Et ne veux pas servir de mon sang et de toi .\n",
            "Je n’en veux point douter , et je n’en puis douter ,\n",
            "Et je n’en puis douter que pour te faire aimer .\n",
            "Je n’en veux pas douter , et je n’en puis douter ,\n",
            "Et je ne puis souffrir que mon cœur s’en irrite .\n",
            "Je n’en veux point douter , et je n’en puis douter ,\n",
            "Et je n’en puis douter que pour te faire aimer .\n",
            "Je n’en veux pas douter , et je n’en puis douter ,\n",
            "Et je ne puis souffrir que mon père m’accuse .\n",
            "Je n’en veux point douter , mais je n’en puis douter .\n",
            "Je n’en veux pas douter , et je n’en puis douter .\n",
            "\n",
            "\n",
            "beam 20 1\n",
            "Je n’en veux point douter , mais je n’en puis douter .\n",
            "Je sais ce que je dois , et ce que je dois faire . . .\n",
            "C’en est fait , je l’avoue , et je n’en puis douter .\n",
            "Je sais ce que je dois , et ce que je dois faire . . .\n",
            "C’est ce que je demande , et ce que je vous dois . . .\n",
            "C’en est fait , je l’avoue , et je n’en suis pas moins .\n",
            "Je sais ce que je dois , et ce que je dois faire . . .\n",
            "C’est là ce que j’ai fait , et ce que je vous dois . . .\n",
            "C’en est fait , je l’avoue , et je n’en suis pas moins .\n",
            "Je sais ce que je dois , et ce que je dois faire . . .\n",
            "C’est là ce que j’ai fait , et ce n’est pas ma faute .\n",
            "\n",
            "\n",
            "top-k 50 1\n",
            "Tu ne l’es plus permis qu’à la vérité je fasse ,\n",
            "Puisque j’en sais ma main dessus le sacrifice .\n",
            "Elle - même une fois a vu mon changement ,\n",
            "Et me rend le succès de ne m’avoir pas dit .\n",
            "Par là le même temps , madame , à sa parole ,\n",
            "Est du moins de la mort à mon commandement .\n",
            "Je vais tout à l’Hymen , dans la haute querelle ,\n",
            "Du choix qui m’a réduit à l’indigne raison .\n",
            "C’est me faire une loi que ma triste justice\n",
            "Me puisse faire au soin de m’en faire une grâce ,\n",
            "Et que dans ce devoir , si par l’amitié ,\n",
            "L’un m’ait fait l’amour seul , je n’en veux rien à plus .\n",
            "Je n’en ai plus , pour vous , qu’en revanche - vous - sorte :\n",
            "Si pour me voir Léon j’ai voulu m’éviter ,\n",
            "\n",
            "\n",
            "top-k 100 0.7\n",
            "L’hymen en te parlant m’a fait voir mon devoir ,\n",
            "Et ton père s’en fait , si ta joie m’eût mis ,\n",
            "Quand tu n’es qu’en vain je veuille t’assurer ,\n",
            "Et de ce qu’elle a d’eux il n’a pu me surprendre ,\n",
            "S’il y faut pour mon âme une haine plus haute ,\n",
            "Et lorsqu’à mon amour son retour me trahit ,\n",
            "L’amour que je m’explique a été plus de cœur .\n",
            "Je ne la quitte point , mais pour te le supplice\n",
            "De l’affront où je suis je t’avais fait descendre ,\n",
            "Et c’est pour Martian que je me le demande ,\n",
            "Que de ma passion je me sens l’interprète .\n",
            "C’est donc ce qu’il m’a dit ? Le ciel m’a - t - il dit\n",
            "Qu’un si ferme amant avait droit de me plaire ?\n",
            "Je n’ai qu’un prétexte à ne pas m’imputer ,\n",
            "\n",
            "\n",
            "top-p 0.9 1\n",
            "Et surtout le succès te fit toucher ton père ,\n",
            "Jusqu’à ce changement la fin que je te fais ,\n",
            "Ton trop de promptitude vaut même à tes yeux .\n",
            "Mes soins ! Cher infidèle ! ô jour ont - ils la vue ?\n",
            "Pourquoi devant le choix d’une femme animée ,\n",
            "Jamais infortuné n’eût pu me garantir ,\n",
            "Et tu vis malgré toi plus que plus je ne t’aime ;\n",
            "Et connais ma pudeur , et juré de ta haine\n",
            "L’impérieux Fédéric que je fais de ta sorte .\n",
            "Je ne m’étonne point , et n’ayant pu me voir ,\n",
            "Quand je sois arrêté de régner pour l’en voir .\n",
            "Qu’à me réduire le ciel à me nuire on respire ,\n",
            "Sans que l’amitié soit capable de paraître ,\n",
            "Et que pour ce défaut des soupirs entre ses bras\n",
            "Mon père l’intéresse a fait plus que mes vœux ,\n",
            "\n",
            "\n",
            "top-p 0.7 1\n",
            "À mes tristes désirs il a paru parole ;\n",
            "Mais s’il a su laisser à mon bras offensée ,\n",
            "Je puis avec horreur le faire haïr .\n",
            "Madame , il a cru que je le dois à craindre ,\n",
            "Et m’en a fait plaisir une pure douceur ,\n",
            "Que ce que j’en dis ne vaut plus que la gloire ,\n",
            "Et que de votre amour je me suis douté .\n",
            "Je l’avais pris sans peine , et je l’avais contente ,\n",
            "Mais si je puis m’aimer . . . Je me suis fait pour elle ,\n",
            "Et la mienne . . . C’est fait tout ce que je veux savoir .\n",
            "Oui , je le puis sans voir . Je l’attends , je l’adore .\n",
            "Va , ne perds point de temps . Quoi , tu te vas penser ?\n",
            "Et ne t’en mets pas , c’est elle qui me poursuit .\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIcXwZprkI_T"
      },
      "source": [
        "from fast_bleu import SelfBLEU\r\n",
        "list_greedy, list_beam = [], []\r\n",
        "list_topk, list_topp = [], []\r\n",
        "incr,nb_line = 0, 0\r\n",
        "nb_gen = 10\r\n",
        "for k in range(nb_gen):\r\n",
        "  nb_line +=1\r\n",
        "  start_seq = []\r\n",
        "  while (nb_line % 3 != 0):\r\n",
        "    cur_word = corpus.test.tolist()[incr]\r\n",
        "    start_seq.append(cur_word)\r\n",
        "    incr +=1\r\n",
        "    if cur_word == corpus.dictionary.word2idx['<eos>']:\r\n",
        "      nb_line +=1\r\n",
        "  start_seq = ' '.join([corpus.dictionary.idx2word[k] for k in start_seq])\r\n",
        "  list_greedy.append(predict(device, model, start_seq, corpus.dictionary.word2idx, corpus.dictionary.idx2word, 1, pprint = False))\r\n",
        "  list_beam.append(beam_search_decode(device, model, start_seq, corpus.dictionary.word2idx, corpus.dictionary.idx2word, 20, 1, pprint = False))\r\n",
        "  list_topk.append(top_k_sampling(device, model, start_seq, corpus.dictionary.word2idx, corpus.dictionary.idx2word, 100, 0.7, pprint = False))\r\n",
        "  list_topp.append(top_p_sampling(device, model, start_seq, corpus.dictionary.word2idx, corpus.dictionary.idx2word, 0.7, 1, pprint = False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6zJ2WTUpQ8T",
        "outputId": "a966b76f-2569-474c-83f1-1a08a24a49f0"
      },
      "source": [
        "rimes_greedy, rimes_beam, rimes_topk, rimes_topp = [], [], [], []\r\n",
        "for i in range(nb_gen):\r\n",
        "  for j in range(len(list_greedy[i])-1):\r\n",
        "    if list_greedy[i][j] in eos_tokens : rimes_greedy.append(list_greedy[i][j])\r\n",
        "    if list_beam[i][j] in eos_tokens : rimes_beam.append(list_beam[i][j])\r\n",
        "    if list_topk[i][j] in eos_tokens : rimes_topk.append(list_topk[i][j])\r\n",
        "    if list_topp[i][j] in eos_tokens : rimes_topp.append(list_topp[i][j])\r\n",
        "for i in range(nb_gen):\r\n",
        "  pretty_print(list_greedy[i], filename='greedy.txt')\r\n",
        "  pretty_print(list_beam[i], beam= True, filename='beam.txt')\r\n",
        "  pretty_print(list_topk[i], filename='topk.txt')\r\n",
        "  pretty_print(list_topp[i], filename='topp.txt')\r\n",
        "for i in range(nb_gen):\r\n",
        "  for j in range(len(list_greedy[i])-1,-1,-1):\r\n",
        "    if (list_greedy[i][j] == '<eos>') | (list_greedy[i][j] == '<sos>') | (list_greedy[i][j] in eos_tokens):\r\n",
        "      list_greedy[i].pop(j)\r\n",
        "    if (list_beam[i][j] == '<eos>') | (list_beam[i][j] == '<sos>') | (list_beam[i][j] in eos_tokens):\r\n",
        "      list_beam[i].pop(j)\r\n",
        "    if (list_topk[i][j] == '<eos>') | (list_topk[i][j] == '<sos>') | (list_topk[i][j] in eos_tokens):\r\n",
        "      if list_topk[i][j] in eos_tokens: rimes_topk.append(list_topk[i][j])\r\n",
        "      list_topk[i].pop(j)\r\n",
        "    if (list_topp[i][j] == '<eos>') | (list_topp[i][j] == '<sos>') | (list_topp[i][j] in eos_tokens):\r\n",
        "      list_topp[i].pop(j)\r\n",
        "\r\n",
        "weights = {'quadrigram': (1/4., 1/4., 1/4., 1/4.)}\r\n",
        "self_bleu_greedy = SelfBLEU(list_greedy, weights)\r\n",
        "self_bleu_beam = SelfBLEU(list_beam, weights)\r\n",
        "self_bleu_topk = SelfBLEU(list_topk, weights)\r\n",
        "self_bleu_topp = SelfBLEU(list_topp, weights)\r\n",
        "mean_greedy, mean_beam = 0,0\r\n",
        "mean_topk, mean_topp = 0,0\r\n",
        "for v in self_bleu_greedy.get_score()['quadrigram']:\r\n",
        "  mean_greedy += v\r\n",
        "for v in self_bleu_beam.get_score()['quadrigram']:\r\n",
        "  mean_beam += v\r\n",
        "for v in self_bleu_topk.get_score()['quadrigram']:\r\n",
        "  mean_topk += v\r\n",
        "for v in self_bleu_topp.get_score()['quadrigram']:\r\n",
        "  mean_topp += v\r\n",
        "print('selfBLEU greedy: ', mean_greedy/nb_gen,'selfBLEU beam :', mean_beam/nb_gen)\r\n",
        "print('selfBLEU top k: ',mean_topk/nb_gen, 'selfBLEU top p: ',mean_topp/nb_gen)\r\n",
        "\r\n",
        "nb_rimes_greedy, nb_rimes_beam, nb_rimes_topk, nb_rimes_topp = 0,0,0,0\r\n",
        "\r\n",
        "for k in range(len(rimes_greedy)-1):\r\n",
        "  if rimes_greedy[k] == rimes_greedy[k+1]:\r\n",
        "    nb_rimes_greedy +=1\r\n",
        "for k in range(len(rimes_beam)-1):\r\n",
        "  if rimes_beam[k] == rimes_beam[k+1]:\r\n",
        "    nb_rimes_beam +=1\r\n",
        "for k in range(len(rimes_topk)-1):\r\n",
        "  if rimes_topk[k] == rimes_topk[k+1]:\r\n",
        "    nb_rimes_topk +=1\r\n",
        "for k in range(len(rimes_topp)-1):\r\n",
        "  if rimes_topp[k] == rimes_topp[k+1]:\r\n",
        "    nb_rimes_topp +=1  \r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "selfBLEU greedy:  0.8276116899690857 selfBLEU beam : 0.8706516458826468\n",
            "selfBLEU top k:  0.13457289036683154 selfBLEU top p:  0.1262996272491695\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "LEFlWQC53X5v",
        "outputId": "8b5c4476-174f-4e9d-a46a-b0fb1db1465d"
      },
      "source": [
        "print('rimes greedy :', nb_rimes_greedy/len(rimes_greedy) *200, 'rimes beam :', nb_rimes_beam/len(rimes_beam) *200)\r\n",
        "print('rimes top k :', nb_rimes_topk/len(rimes_topk) *200, 'rimes top p :', nb_rimes_topp/len(rimes_topp) *200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ZeroDivisionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-97f297bbd104>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rimes greedy :'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_rimes_greedy\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrimes_greedy\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rimes beam :'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_rimes_beam\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrimes_beam\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rimes top k :'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_rimes_topk\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrimes_topk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rimes top p :'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_rimes_topp\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrimes_topp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wzf0LKiWPqWb"
      },
      "source": [
        "print(self_bleu_topp.get_score())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taXynI6nFcIR"
      },
      "source": [
        "Et je n’ ai point de part à qui je dois parler , <eos15>\r\n",
        "\r\n",
        "Que le cœur à mes yeux n’ a point lieu de douter . <eos27>\r\n",
        "\r\n",
        "Je ne sais si je puis , et je veux bien juger <eos53>\r\n",
        "\r\n",
        "Que je n’ ai pu douter que pour vous le venger . <eos53>\r\n",
        "\r\n",
        "Mais je ne sais pas bien que l’ on me considère , <eos0>\r\n",
        "\r\n",
        "Que vous n’ avez rien fait de votre gloire entière , <eos0>\r\n",
        "\r\n",
        "top p sampling 0.2 : les distributions sont assez plates : première proba environ 0.15, puis beaucoup de 0.02, sauf pour estimer sos, proba proche de 1. Avec un p de 0.9 on a des fois un sampling sur des milliers de mots, avec 0.7 on ne passe pas la centaine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJ8p8Q9UmLP4"
      },
      "source": [
        "!bash \r\n",
        "#/usr/local/bin/poemlint <(echo '6/6 x x') <greedy.txt 2> erreurs_greedy.txt\r\n",
        "#/usr/local/bin/poemlint <(echo '6/6 x x') <beam.txt 2> erreurs_beam.txt\r\n",
        "#/usr/local/bin/poemlint <(echo '6/6 x x') <topk.txt 2> erreurs_topk.txt\r\n",
        "#/usr/local/bin/poemlint <(echo '6/6 x x') <topp.txt 2> erreurs_topp.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WOq-_h0tSLy"
      },
      "source": [
        "nb_greedy, nb_beam, nb_topk, nb_topp = 0,0,0,0\r\n",
        "with open('greedy.txt', 'r') as f:\r\n",
        "  for line in f:\r\n",
        "    nb_greedy +=1\r\n",
        "with open('beam.txt', 'r') as f:\r\n",
        "  for line in f:\r\n",
        "    nb_beam +=1\r\n",
        "with open('topk.txt', 'r') as f:\r\n",
        "  for line in f:\r\n",
        "    nb_topk +=1\r\n",
        "with open('topp.txt', 'r') as f:\r\n",
        "  for line in f:\r\n",
        "    nb_topp +=1\r\n",
        "\r\n",
        "with open('erreurs_greedy.txt', 'r') as f:\r\n",
        "  nb_err = 0\r\n",
        "  for line in f:\r\n",
        "    nb_err +=0.25\r\n",
        "  print('greedy error : ',nb_err/nb_greedy * 100, ' %')\r\n",
        "with open('erreurs_beam.txt', 'r') as f:\r\n",
        "  nb_err = 0\r\n",
        "  for line in f:\r\n",
        "    nb_err +=0.25\r\n",
        "  print('beam error : ',nb_err/nb_beam * 100, ' %')\r\n",
        "with open('erreurs_topk.txt', 'r') as f:\r\n",
        "  nb_err = 0\r\n",
        "  for line in f:\r\n",
        "    nb_err +=0.25\r\n",
        "  print('topk error : ',nb_err/nb_topk * 100, ' %')\r\n",
        "with open('erreurs_topp.txt', 'r') as f:\r\n",
        "  nb_err = 0\r\n",
        "  for line in f:\r\n",
        "    nb_err +=0.25\r\n",
        "  print('topp error : ',nb_err/nb_topp * 100, ' %')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}