{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "AlexandRNN_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5LqrlD34g3m"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1ACLXSw4g3s"
      },
      "source": [
        "### Tools for data processing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6D83zYN44g3s"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import math\n",
        "from collections import Counter\n",
        "import pprint\n",
        "pp = pprint.PrettyPrinter(indent=1)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6HdDjADEQxS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd871088-65c7-4625-ee24-4316639e5c46"
      },
      "source": [
        "with open('mots_rimes_final.txt', encoding='utf-8') as f:\r\n",
        "  categories, eos_tokens = [], []\r\n",
        "  incr = 0\r\n",
        "  for line in f:\r\n",
        "    if incr % 2 == 1:\r\n",
        "      categories.append(line.split())\r\n",
        "      eos_tokens.append('<eos'+str(incr //2)+'>')\r\n",
        "    incr +=1\r\n",
        "print(categories[1])\r\n",
        "print(eos_tokens[12])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Silence', 'Balance', 'convalescence', 'instance', 'Ordonnance', 'surséance', 'Vengeance', 'Clémence', 'prééminence', 'condescendance', 'Innocence', 'Puissance', 'Danse', 'Dépense', 'indécence', 'Térence', 'Maxence', 'Byzance', 'Constance', 'Florence', 'France', 'abondance', 'absence', 'alliance', 'allégeance', 'apparence', 'arrogance', 'assistance', 'assurance', 'audience', 'avance', 'balance', 'bienséance', 'bienveillance', 'circonstance', 'clémence', 'commence', 'complaisance', 'concurrence', 'confiance', 'confidence', 'conférence', 'connaissance', 'conscience', 'constance', 'conséquence', 'croyance', 'créance', 'danse', 'devance', 'différence', 'diligence', 'dispense', 'distance', 'défense', 'défiance', 'déférence', 'délivrance', 'dépendance', 'désobéissance', 'enfance', 'espérance', 'excellence', 'expérience', 'extravagance', 'ignorance', 'immense', 'impatience', 'importance', 'imprudence', 'impudence', 'impuissance', 'inclémence', 'inconstance', 'indifférence', 'indulgence', 'indépendance', 'influence', 'innocence', 'insolence', 'insuffisance', 'intelligence', 'irrévérence', 'jouissance', 'licence', 'magnificence', 'méconnaissance', 'médisance', 'naissance', 'nonchalance', 'négligence', 'obéissance', 'occurrence', 'offense', 'opulence', 'panse', 'patience', 'pense', 'persévérance', 'providence', 'prudence', 'préférence', 'présence', 'prévoyance', 'puissance', 'reconnaissance', 'ressemblance', 'récompense', 'répugnance', 'résistance', 'science', 'sentence', 'silence', 'souffrance', 'vaillance', 'vengeance', 'vigilance', 'violence', 'véhémence', 'éloquence', 'éminence', 'évidence', 'décadence', 'Hortense', 'aisance', 'révérence', 'décence', 'finance', 'Enfance', 'chance', 'impertinence', 'Science', 'dépense', 'ordonnance', 'indigence', 'semence', 'lance', 'bienfaisance', 'encense', 'existence', 'Conscience', 'Alliance', 'Ignorance', 'suffisance', 'outrance', 'manigance', 'Terence', 'Patience', 'réminiscence', 'compétence', 'potence', 'Influence', 'contenance', 'quittance', 'abstinence', 'pénitence', 'substance', 'incontinence', 'continence', 'excressanse', 'quintessence', 'transparence', 'intempérance', 'démence', 'cadence', 'Préférence', 'Assurance', 'Dépendance', 'Offense', 'résidence', 'Médisance', 'Bienséance', 'Complaisance', 'défaillance']\n",
            "<eos12>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urQQ5WEI4g3s"
      },
      "source": [
        "class Dictionary(object):\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = []\n",
        "        self.counter = {}\n",
        "        self.total = 0\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.idx2word.append(word)\n",
        "            self.word2idx[word] = len(self.idx2word) - 1\n",
        "            self.counter.setdefault(word, 0)\n",
        "        self.counter[word] += 1\n",
        "        self.total += 1\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RG6rsEQB4g3t"
      },
      "source": [
        "class Corpus(object):\n",
        "    def __init__(self, path):\n",
        "        # We create an object Dictionary associated to Corpus\n",
        "        self.dictionary = Dictionary()\n",
        "        # We go through all files, adding all words to the dictionary\n",
        "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
        "        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n",
        "        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n",
        "        \n",
        "    def tokenize(self, path):\n",
        "        \"\"\"Tokenizes a text file, knowing the dictionary, in order to tranform it into a list of indexes\"\"\"\n",
        "        assert os.path.exists(path)\n",
        "        # Add words to the dictionary\n",
        "        with open(path, 'r') as f:\n",
        "            tokens = 0\n",
        "            eos_seq = []\n",
        "            for line in f:\n",
        "              #words = ['<sos>'] + line.split() + ['<eos>']\n",
        "              words = ['<sos>'] + line.split()\n",
        "              rime, inc = words[-1], len(words)-1\n",
        "              while(rime in ['\"', '.', ',', ';', ':', '?', '!', ' ', ')', '»', '-', '\\xa0', '\\n']):\n",
        "                inc -= 1\n",
        "                rime = words[inc]\n",
        "              isInNoCat = True\n",
        "              for i in range(70):\n",
        "                if rime in categories[i]:\n",
        "                  isInNoCat = False\n",
        "                  words.append(eos_tokens[i])\n",
        "                  eos_seq.append(eos_tokens[i])\n",
        "                  break\n",
        "              if isInNoCat :\n",
        "                words.append('<eos_>')\n",
        "                eos_seq.append('<eos_>')\n",
        "              else:\n",
        "                for word in words:\n",
        "                  self.dictionary.add_word(word)\n",
        "                tokens += len(words)\n",
        "        \n",
        "        # Once done, go through the file a second time and fill a Torch Tensor with the associated indexes \n",
        "        with open(path, 'r') as f:\n",
        "            ids = torch.LongTensor(tokens)\n",
        "            token,incr = 0, 0\n",
        "            for line in f:\n",
        "                \n",
        "                #words = ['<sos>'] + line.split() + ['<eos>']\n",
        "                if eos_seq[incr] != '<eos_>':\n",
        "                  words = ['<sos>'] + line.split() + [eos_seq[incr]]\n",
        "                  for word in words:\n",
        "                    ids[token] = self.dictionary.word2idx[word]\n",
        "                    token += 1\n",
        "                incr +=1\n",
        "        return ids"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytRgixof4g3t"
      },
      "source": [
        "###############################################################################\n",
        "# Load data\n",
        "###############################################################################\n",
        "\n",
        "data = './corpus/'\n",
        "corpus = Corpus(data)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHf1bA7C4g3t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ece9f40-20d0-4371-c96f-a0bb0aea8cb1"
      },
      "source": [
        "print(corpus.dictionary.total)\n",
        "print(len(corpus.dictionary.idx2word))\n",
        "print(len(corpus.dictionary.word2idx))\n",
        "\n",
        "print(corpus.train.shape)\n",
        "print(corpus.train[0:7])\n",
        "print([corpus.dictionary.idx2word[corpus.train[i]] for i in range(40)])\n",
        "\n",
        "print(corpus.valid.shape)\n",
        "print(corpus.valid[0:7])\n",
        "print([corpus.dictionary.idx2word[corpus.valid[i]] for i in range(7)])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "938548\n",
            "19215\n",
            "19215\n",
            "torch.Size([565726])\n",
            "tensor([0, 1, 2, 3, 4, 5, 6])\n",
            "['<sos>', 'Impatients', 'désirs', 'd’', 'une', 'illustre', 'vengeance', '<eos1>', '<sos>', 'Dont', 'la', 'mort', 'de', 'mon', 'père', 'a', 'formé', 'la', 'naissance', ',', '<eos1>', '<sos>', 'Enfants', 'impétueux', 'de', 'mon', 'ressentiment', '<eos9>', '<sos>', 'Que', 'ma', 'douleur', 'séduite', 'embrasse', 'aveuglément', ',', '<eos9>', '<sos>', 'Vous', 'régnez']\n",
            "torch.Size([189741])\n",
            "tensor([    0,   252,   214, 11853,     9,   976,    42])\n",
            "['<sos>', 'Je', 'lui', 'prescris', 'la', 'loi', 'que']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIWjBwFj4g3t"
      },
      "source": [
        "# We now have data under a very long list of indexes: the text is as one sequence.\n",
        "# The idea now is to create batches from this. Note that this is absolutely not the best\n",
        "# way to proceed with large quantities of data (where we'll try not to store huge tensors\n",
        "# in memory but read them from file as we go) !\n",
        "# Here, we are looking for simplicity and efficiency with regards to computation time.\n",
        "# That is why we will ignore sentence separations and treat the data as one long stream that\n",
        "# we will cut arbitrarily as we need.\n",
        "# With the alphabet being our data, we currently have the sequence:\n",
        "# [a b c d e f g h i j k l m n o p q r s t u v w x y z]\n",
        "# We want to reorganize it as independant batches that will be processed independantly by the model !\n",
        "# For instance, with the alphabet as the sequence and batch size 4, we'd get the 4 following sequences:\n",
        "# ┌ a g m s ┐\n",
        "# │ b h n t │\n",
        "# │ c i o u │\n",
        "# │ d j p v │\n",
        "# │ e k q w │\n",
        "# └ f l r x ┘\n",
        "# with the last two elements being lost.\n",
        "# Again, these columns are treated as independent by the model, which means that the\n",
        "# dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient processing.\n",
        "\n",
        "def batchify(data, batch_size, cuda = False):\n",
        "    # Cut the elements that are unnecessary\n",
        "    nbatch = data.size(0) // batch_size\n",
        "    data = data.narrow(0, 0, nbatch * batch_size)\n",
        "    # Reorganize the data\n",
        "    data = data.view(batch_size, -1).t().contiguous()\n",
        "    # If we can use a GPU, let's tranfer the tensor to it\n",
        "    return data.to(device)\n",
        "\n",
        "# get_batch subdivides the source data into chunks of the appropriate length.\n",
        "# If source is equal to the example output of the batchify function, with\n",
        "# a sequence length (seq_len) of 3, we'd get the following two variables:\n",
        "# ┌ a g m s ┐ ┌ b h n t ┐\n",
        "# | b h n t | | c i o u │\n",
        "# └ c i o u ┘ └ d j p v ┘\n",
        "# The first variable contains the letters input to the network, while the second\n",
        "# contains the one we want the network to predict (b for a, h for g, v for u, etc..)\n",
        "# Note that despite the name of the function, we are cutting the data in the\n",
        "# temporal dimension, since we already divided data into batches in the previous\n",
        "# function. \n",
        "\n",
        "def get_batch(source, i, seq_len, evaluation=False):\n",
        "    # Deal with the possibility that there's not enough data left for a full sequence\n",
        "    seq_len = min(seq_len, len(source) - 1 - i)\n",
        "    # Take the input data\n",
        "    data = source[i:i+seq_len]\n",
        "    # Shift by one for the target data\n",
        "    target = source[i+1:i+1+seq_len]\n",
        "    return data, target"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMeQ0gjzUOLd"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwfqKiMJ4g3t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "834d4299-ed21-4656-8e98-7883efc3c949"
      },
      "source": [
        "batch_size = 100\n",
        "eval_batch_size = 4\n",
        "train_data = batchify(corpus.train, batch_size)\n",
        "val_data = batchify(corpus.valid, eval_batch_size)\n",
        "test_data = batchify(corpus.test, eval_batch_size)\n",
        "\n",
        "print(train_data.shape)\n",
        "print(val_data.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5657, 100])\n",
            "torch.Size([47435, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9OVIoVJ4g3u"
      },
      "source": [
        "### LSTM Cells in pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztKrssh84g3u"
      },
      "source": [
        "### Creating our own LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zD0K9CQ14g3u"
      },
      "source": [
        "# Models are usually implemented as custom nn.Module subclass\n",
        "# We need to redefine the __init__ method, which creates the object\n",
        "# We also need to redefine the forward method, which transform the input into outputs\n",
        "# We can also add any method that we need: here, in order to initiate weights in the model\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        # Create a dropout object to use on layers for regularization\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        # Create an encoder - which is an embedding layer\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        # Create the LSTM layers - find out how to stack them !\n",
        "        self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
        "        # Create what we call the decoder: a linear transformation to map the hidden state into scores for all words in the vocabulary\n",
        "        # (Note that the softmax application function will be applied out of the model)\n",
        "        self.decoder = nn.Linear(nhid, ntoken)\n",
        "        \n",
        "        # Initialize non-reccurent weights \n",
        "        self.init_weights()\n",
        "\n",
        "        self.ninp = ninp\n",
        "        self.nhid = nhid\n",
        "        self.nlayers = nlayers\n",
        "        \n",
        "    def init_weights(self):\n",
        "        # Initialize the encoder and decoder weights with the uniform distribution\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.fill_(0)\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "        \n",
        "    def init_hidden(self, batch_size):\n",
        "        # Initialize the hidden state and cell state to zero, with the right sizes\n",
        "        weight = next(self.parameters())\n",
        "        return (weight.new_zeros(self.nlayers, batch_size, self.nhid),\n",
        "                weight.new_zeros(self.nlayers, batch_size, self.nhid))    \n",
        "\n",
        "    def forward(self, input, hidden, return_h=False):\n",
        "        # Process the input\n",
        "        emb = self.drop(self.encoder(input))   \n",
        "        \n",
        "        # Apply the LSTMs\n",
        "        output, hidden = self.rnn(emb, hidden)\n",
        "        \n",
        "        # Decode into scores\n",
        "        output = self.drop(output)      \n",
        "        decoded = self.decoder(output)\n",
        "        return decoded, hidden"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucXMYksR4g3u"
      },
      "source": [
        "### Building the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HL-vvLGm4g3u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2aa8b544-7f20-4732-f0ac-fe4304d1a17f"
      },
      "source": [
        "# Set the random seed manually for reproducibility.\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f77696f7b58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S147j0Dx4g3u"
      },
      "source": [
        "embedding_size = 500\n",
        "hidden_size = 500\n",
        "layers = 2\n",
        "dropout = 0.2\n",
        "\n",
        "###############################################################################\n",
        "# Build the model\n",
        "###############################################################################\n",
        "\n",
        "vocab_size = len(corpus.dictionary)\n",
        "model = LSTMModel(vocab_size, embedding_size, hidden_size, layers, dropout).to(device)\n",
        "params = list(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RU2X8VHG4g3v"
      },
      "source": [
        "lr = 10.0\n",
        "optimizer = 'sgd'\n",
        "wdecay = 1.2e-6\n",
        "# For gradient clipping\n",
        "clip = 0.25\n",
        "\n",
        "if optimizer == 'sgd':\n",
        "    optim = torch.optim.SGD(params, lr=lr, weight_decay=wdecay)\n",
        "if optimizer == 'adam':\n",
        "    optim = torch.optim.Adam(params, lr=lr, weight_decay=wdecay)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fGlrwME4g3v"
      },
      "source": [
        "# Let's think about gradient propagation:\n",
        "# We plan to keep the second ouput of the LSTM layer (the hidden/cell states) to initialize\n",
        "# the next call to LSTM. In this way, we can back-propagate the gradient for as long as we want.\n",
        "# However, this put a huge strain on the memory used by the model, since it implies retaining\n",
        "# a always-growing number of tensors of gradients in the cache.\n",
        "# We decide to not backpropagate through time beyond the current sequence ! \n",
        "# We use a specific function to cut the 'hidden/state cell' states from their previous dependencies\n",
        "# before using them to initialize the next call to the LSTM.\n",
        "# This is done with the .detach() function.\n",
        "\n",
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4CGuudp4g3v"
      },
      "source": [
        "# Other global parameters\n",
        "epochs = 50\n",
        "seq_len = 40\n",
        "log_interval = 10\n",
        "save = 'model.pt'"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xx6UKwTO4g3v"
      },
      "source": [
        "def evaluate(data_source):\n",
        "    # Turn on evaluation mode which disables dropout.\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    hidden = model.init_hidden(eval_batch_size)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, seq_len):\n",
        "            data, targets = get_batch(data_source, i, seq_len)\n",
        "            output, hidden = model(data, hidden)\n",
        "            hidden = repackage_hidden(hidden)\n",
        "            total_loss += len(data) * criterion(output.view(-1, vocab_size), targets.view(-1)).item()\n",
        "    return total_loss / (len(data_source) - 1)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EhuQglX4g3v"
      },
      "source": [
        "def train():\n",
        "    # Turn on training mode which enables dropout.\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    hidden = model.init_hidden(batch_size)\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, seq_len)):\n",
        "        data, targets = get_batch(train_data, i, seq_len)\n",
        "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "        hidden = repackage_hidden(hidden)\n",
        "        optim.zero_grad()\n",
        "        \n",
        "        output, hidden = model(data, hidden)\n",
        "        loss = criterion(output.view(-1, vocab_size), targets.view(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        torch.nn.utils.clip_grad_norm_(params, clip)\n",
        "        optim.step()\n",
        "        \n",
        "        total_loss += loss.data\n",
        "\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
        "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                epoch, batch, len(train_data) // seq_len, lr,\n",
        "                elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bN0gBRgf4g3v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd7d615e-a781-41c5-9534-477b7b29c347"
      },
      "source": [
        "# Loop over epochs.\n",
        "best_val_loss = None\n",
        "\n",
        "# At any point you can hit Ctrl + C to break out of training early.\n",
        "try:\n",
        "    for epoch in range(1, epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        train()\n",
        "        val_loss = evaluate(val_data)\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                           val_loss, math.exp(val_loss)))\n",
        "        print('-' * 89)\n",
        "        # Save the model if the validation loss is the best we've seen so far.\n",
        "        if not best_val_loss or val_loss < best_val_loss:\n",
        "            with open(save, 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_val_loss = val_loss\n",
        "        else:\n",
        "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "            lr /= 4.0\n",
        "except KeyboardInterrupt:\n",
        "    print('-' * 89)\n",
        "    print('Exiting from training early')\n",
        "\n",
        "# Load the best saved model.\n",
        "with open(save, 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "    # after load the rnn params are not a continuous chunk of memory\n",
        "    # this makes them a continuous chunk, and will speed up forward pass\n",
        "    model.rnn.flatten_parameters()\n",
        "\n",
        "# Run on test data.\n",
        "test_loss = evaluate(test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, math.exp(test_loss)))\n",
        "print('=' * 89)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| epoch   1 |    10/  141 batches | lr 10.00 | ms/batch 271.42 | loss 10.20 | ppl 27016.21\n",
            "| epoch   1 |    20/  141 batches | lr 10.00 | ms/batch 234.10 | loss  8.06 | ppl  3174.22\n",
            "| epoch   1 |    30/  141 batches | lr 10.00 | ms/batch 234.15 | loss  8.04 | ppl  3109.75\n",
            "| epoch   1 |    40/  141 batches | lr 10.00 | ms/batch 234.22 | loss  7.33 | ppl  1529.60\n",
            "| epoch   1 |    50/  141 batches | lr 10.00 | ms/batch 236.15 | loss  7.39 | ppl  1615.96\n",
            "| epoch   1 |    60/  141 batches | lr 10.00 | ms/batch 234.04 | loss  7.24 | ppl  1393.04\n",
            "| epoch   1 |    70/  141 batches | lr 10.00 | ms/batch 236.02 | loss  7.01 | ppl  1108.71\n",
            "| epoch   1 |    80/  141 batches | lr 10.00 | ms/batch 235.93 | loss  6.99 | ppl  1083.34\n",
            "| epoch   1 |    90/  141 batches | lr 10.00 | ms/batch 236.21 | loss  6.95 | ppl  1041.86\n",
            "| epoch   1 |   100/  141 batches | lr 10.00 | ms/batch 237.95 | loss  6.94 | ppl  1036.89\n",
            "| epoch   1 |   110/  141 batches | lr 10.00 | ms/batch 234.22 | loss  6.82 | ppl   916.05\n",
            "| epoch   1 |   120/  141 batches | lr 10.00 | ms/batch 237.16 | loss  6.78 | ppl   881.27\n",
            "| epoch   1 |   130/  141 batches | lr 10.00 | ms/batch 235.38 | loss  6.85 | ppl   944.64\n",
            "| epoch   1 |   140/  141 batches | lr 10.00 | ms/batch 238.67 | loss  6.86 | ppl   954.50\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 51.62s | valid loss  6.78 | valid ppl   882.97\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |    10/  141 batches | lr 10.00 | ms/batch 259.34 | loss  7.42 | ppl  1675.80\n",
            "| epoch   2 |    20/  141 batches | lr 10.00 | ms/batch 238.26 | loss  6.79 | ppl   887.95\n",
            "| epoch   2 |    30/  141 batches | lr 10.00 | ms/batch 239.40 | loss  6.69 | ppl   806.13\n",
            "| epoch   2 |    40/  141 batches | lr 10.00 | ms/batch 237.57 | loss  6.70 | ppl   811.48\n",
            "| epoch   2 |    50/  141 batches | lr 10.00 | ms/batch 240.67 | loss  6.66 | ppl   783.91\n",
            "| epoch   2 |    60/  141 batches | lr 10.00 | ms/batch 238.18 | loss  6.64 | ppl   762.25\n",
            "| epoch   2 |    70/  141 batches | lr 10.00 | ms/batch 238.12 | loss  6.55 | ppl   696.34\n",
            "| epoch   2 |    80/  141 batches | lr 10.00 | ms/batch 240.46 | loss  6.60 | ppl   736.96\n",
            "| epoch   2 |    90/  141 batches | lr 10.00 | ms/batch 238.71 | loss  6.42 | ppl   613.19\n",
            "| epoch   2 |   100/  141 batches | lr 10.00 | ms/batch 239.84 | loss  6.30 | ppl   546.37\n",
            "| epoch   2 |   110/  141 batches | lr 10.00 | ms/batch 239.85 | loss  6.22 | ppl   503.49\n",
            "| epoch   2 |   120/  141 batches | lr 10.00 | ms/batch 242.07 | loss  6.27 | ppl   526.22\n",
            "| epoch   2 |   130/  141 batches | lr 10.00 | ms/batch 239.92 | loss  6.18 | ppl   482.58\n",
            "| epoch   2 |   140/  141 batches | lr 10.00 | ms/batch 238.45 | loss  6.11 | ppl   448.32\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 52.03s | valid loss  6.12 | valid ppl   453.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |    10/  141 batches | lr 10.00 | ms/batch 259.35 | loss  6.57 | ppl   716.02\n",
            "| epoch   3 |    20/  141 batches | lr 10.00 | ms/batch 238.79 | loss  5.85 | ppl   346.09\n",
            "| epoch   3 |    30/  141 batches | lr 10.00 | ms/batch 238.28 | loss  5.79 | ppl   325.78\n",
            "| epoch   3 |    40/  141 batches | lr 10.00 | ms/batch 240.07 | loss  5.75 | ppl   315.55\n",
            "| epoch   3 |    50/  141 batches | lr 10.00 | ms/batch 239.72 | loss  5.73 | ppl   308.52\n",
            "| epoch   3 |    60/  141 batches | lr 10.00 | ms/batch 238.42 | loss  5.65 | ppl   285.39\n",
            "| epoch   3 |    70/  141 batches | lr 10.00 | ms/batch 239.05 | loss  5.62 | ppl   277.16\n",
            "| epoch   3 |    80/  141 batches | lr 10.00 | ms/batch 237.49 | loss  5.55 | ppl   257.01\n",
            "| epoch   3 |    90/  141 batches | lr 10.00 | ms/batch 240.01 | loss  5.46 | ppl   236.26\n",
            "| epoch   3 |   100/  141 batches | lr 10.00 | ms/batch 238.78 | loss  5.45 | ppl   232.24\n",
            "| epoch   3 |   110/  141 batches | lr 10.00 | ms/batch 238.92 | loss  5.38 | ppl   216.40\n",
            "| epoch   3 |   120/  141 batches | lr 10.00 | ms/batch 237.61 | loss  5.38 | ppl   216.26\n",
            "| epoch   3 |   130/  141 batches | lr 10.00 | ms/batch 237.91 | loss  5.34 | ppl   208.21\n",
            "| epoch   3 |   140/  141 batches | lr 10.00 | ms/batch 238.76 | loss  5.32 | ppl   203.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 51.90s | valid loss  5.32 | valid ppl   204.89\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |    10/  141 batches | lr 10.00 | ms/batch 260.60 | loss  5.82 | ppl   337.10\n",
            "| epoch   4 |    20/  141 batches | lr 10.00 | ms/batch 239.20 | loss  5.25 | ppl   190.26\n",
            "| epoch   4 |    30/  141 batches | lr 10.00 | ms/batch 239.64 | loss  5.20 | ppl   180.58\n",
            "| epoch   4 |    40/  141 batches | lr 10.00 | ms/batch 239.59 | loss  5.20 | ppl   181.29\n",
            "| epoch   4 |    50/  141 batches | lr 10.00 | ms/batch 238.76 | loss  5.13 | ppl   168.96\n",
            "| epoch   4 |    60/  141 batches | lr 10.00 | ms/batch 238.16 | loss  5.15 | ppl   172.15\n",
            "| epoch   4 |    70/  141 batches | lr 10.00 | ms/batch 238.34 | loss  5.14 | ppl   171.42\n",
            "| epoch   4 |    80/  141 batches | lr 10.00 | ms/batch 238.71 | loss  5.10 | ppl   163.22\n",
            "| epoch   4 |    90/  141 batches | lr 10.00 | ms/batch 239.10 | loss  5.03 | ppl   153.07\n",
            "| epoch   4 |   100/  141 batches | lr 10.00 | ms/batch 239.03 | loss  5.02 | ppl   150.85\n",
            "| epoch   4 |   110/  141 batches | lr 10.00 | ms/batch 238.66 | loss  4.99 | ppl   146.57\n",
            "| epoch   4 |   120/  141 batches | lr 10.00 | ms/batch 239.65 | loss  4.98 | ppl   145.18\n",
            "| epoch   4 |   130/  141 batches | lr 10.00 | ms/batch 237.30 | loss  4.97 | ppl   144.18\n",
            "| epoch   4 |   140/  141 batches | lr 10.00 | ms/batch 239.69 | loss  4.96 | ppl   142.10\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 51.90s | valid loss  4.97 | valid ppl   144.33\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |    10/  141 batches | lr 10.00 | ms/batch 261.22 | loss  5.44 | ppl   231.09\n",
            "| epoch   5 |    20/  141 batches | lr 10.00 | ms/batch 237.70 | loss  4.87 | ppl   130.37\n",
            "| epoch   5 |    30/  141 batches | lr 10.00 | ms/batch 239.23 | loss  4.86 | ppl   128.80\n",
            "| epoch   5 |    40/  141 batches | lr 10.00 | ms/batch 237.11 | loss  4.88 | ppl   131.17\n",
            "| epoch   5 |    50/  141 batches | lr 10.00 | ms/batch 239.58 | loss  4.83 | ppl   124.71\n",
            "| epoch   5 |    60/  141 batches | lr 10.00 | ms/batch 237.56 | loss  4.85 | ppl   128.07\n",
            "| epoch   5 |    70/  141 batches | lr 10.00 | ms/batch 239.27 | loss  4.85 | ppl   127.13\n",
            "| epoch   5 |    80/  141 batches | lr 10.00 | ms/batch 239.03 | loss  4.81 | ppl   122.20\n",
            "| epoch   5 |    90/  141 batches | lr 10.00 | ms/batch 238.52 | loss  4.74 | ppl   114.55\n",
            "| epoch   5 |   100/  141 batches | lr 10.00 | ms/batch 239.66 | loss  4.73 | ppl   113.60\n",
            "| epoch   5 |   110/  141 batches | lr 10.00 | ms/batch 237.24 | loss  4.68 | ppl   108.09\n",
            "| epoch   5 |   120/  141 batches | lr 10.00 | ms/batch 238.39 | loss  4.71 | ppl   111.20\n",
            "| epoch   5 |   130/  141 batches | lr 10.00 | ms/batch 236.88 | loss  4.70 | ppl   109.78\n",
            "| epoch   5 |   140/  141 batches | lr 10.00 | ms/batch 239.30 | loss  4.73 | ppl   113.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 51.82s | valid loss  4.73 | valid ppl   113.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |    10/  141 batches | lr 10.00 | ms/batch 257.89 | loss  5.13 | ppl   168.28\n",
            "| epoch   6 |    20/  141 batches | lr 10.00 | ms/batch 237.06 | loss  4.65 | ppl   104.24\n",
            "| epoch   6 |    30/  141 batches | lr 10.00 | ms/batch 236.23 | loss  4.60 | ppl    99.53\n",
            "| epoch   6 |    40/  141 batches | lr 10.00 | ms/batch 238.40 | loss  4.64 | ppl   103.27\n",
            "| epoch   6 |    50/  141 batches | lr 10.00 | ms/batch 237.80 | loss  4.58 | ppl    97.20\n",
            "| epoch   6 |    60/  141 batches | lr 10.00 | ms/batch 238.19 | loss  4.60 | ppl    99.36\n",
            "| epoch   6 |    70/  141 batches | lr 10.00 | ms/batch 238.12 | loss  4.59 | ppl    98.79\n",
            "| epoch   6 |    80/  141 batches | lr 10.00 | ms/batch 238.60 | loss  4.57 | ppl    96.79\n",
            "| epoch   6 |    90/  141 batches | lr 10.00 | ms/batch 238.42 | loss  4.51 | ppl    91.29\n",
            "| epoch   6 |   100/  141 batches | lr 10.00 | ms/batch 238.38 | loss  4.50 | ppl    90.15\n",
            "| epoch   6 |   110/  141 batches | lr 10.00 | ms/batch 238.87 | loss  4.46 | ppl    86.12\n",
            "| epoch   6 |   120/  141 batches | lr 10.00 | ms/batch 238.09 | loss  4.49 | ppl    89.37\n",
            "| epoch   6 |   130/  141 batches | lr 10.00 | ms/batch 238.31 | loss  4.46 | ppl    86.84\n",
            "| epoch   6 |   140/  141 batches | lr 10.00 | ms/batch 237.04 | loss  4.50 | ppl    90.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 51.74s | valid loss  4.52 | valid ppl    91.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |    10/  141 batches | lr 10.00 | ms/batch 258.50 | loss  4.92 | ppl   136.51\n",
            "| epoch   7 |    20/  141 batches | lr 10.00 | ms/batch 238.60 | loss  4.41 | ppl    82.27\n",
            "| epoch   7 |    30/  141 batches | lr 10.00 | ms/batch 240.82 | loss  4.40 | ppl    81.57\n",
            "| epoch   7 |    40/  141 batches | lr 10.00 | ms/batch 241.26 | loss  4.41 | ppl    82.56\n",
            "| epoch   7 |    50/  141 batches | lr 10.00 | ms/batch 239.74 | loss  4.36 | ppl    78.44\n",
            "| epoch   7 |    60/  141 batches | lr 10.00 | ms/batch 239.66 | loss  4.39 | ppl    80.51\n",
            "| epoch   7 |    70/  141 batches | lr 10.00 | ms/batch 239.96 | loss  4.40 | ppl    81.06\n",
            "| epoch   7 |    80/  141 batches | lr 10.00 | ms/batch 241.36 | loss  4.35 | ppl    77.20\n",
            "| epoch   7 |    90/  141 batches | lr 10.00 | ms/batch 241.08 | loss  4.32 | ppl    75.33\n",
            "| epoch   7 |   100/  141 batches | lr 10.00 | ms/batch 240.59 | loss  4.30 | ppl    73.51\n",
            "| epoch   7 |   110/  141 batches | lr 10.00 | ms/batch 241.54 | loss  4.26 | ppl    71.10\n",
            "| epoch   7 |   120/  141 batches | lr 10.00 | ms/batch 241.17 | loss  4.26 | ppl    71.14\n",
            "| epoch   7 |   130/  141 batches | lr 10.00 | ms/batch 240.83 | loss  4.30 | ppl    73.57\n",
            "| epoch   7 |   140/  141 batches | lr 10.00 | ms/batch 240.40 | loss  4.30 | ppl    73.69\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 52.27s | valid loss  4.36 | valid ppl    78.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |    10/  141 batches | lr 10.00 | ms/batch 262.40 | loss  4.71 | ppl   111.35\n",
            "| epoch   8 |    20/  141 batches | lr 10.00 | ms/batch 239.70 | loss  4.24 | ppl    69.74\n",
            "| epoch   8 |    30/  141 batches | lr 10.00 | ms/batch 242.91 | loss  4.22 | ppl    67.77\n",
            "| epoch   8 |    40/  141 batches | lr 10.00 | ms/batch 241.44 | loss  4.24 | ppl    69.49\n",
            "| epoch   8 |    50/  141 batches | lr 10.00 | ms/batch 240.80 | loss  4.20 | ppl    66.66\n",
            "| epoch   8 |    60/  141 batches | lr 10.00 | ms/batch 240.45 | loss  4.25 | ppl    69.76\n",
            "| epoch   8 |    70/  141 batches | lr 10.00 | ms/batch 242.93 | loss  4.25 | ppl    69.87\n",
            "| epoch   8 |    80/  141 batches | lr 10.00 | ms/batch 241.21 | loss  4.20 | ppl    66.48\n",
            "| epoch   8 |    90/  141 batches | lr 10.00 | ms/batch 241.80 | loss  4.16 | ppl    64.10\n",
            "| epoch   8 |   100/  141 batches | lr 10.00 | ms/batch 241.48 | loss  4.13 | ppl    61.96\n",
            "| epoch   8 |   110/  141 batches | lr 10.00 | ms/batch 241.09 | loss  4.12 | ppl    61.81\n",
            "| epoch   8 |   120/  141 batches | lr 10.00 | ms/batch 242.80 | loss  4.13 | ppl    62.38\n",
            "| epoch   8 |   130/  141 batches | lr 10.00 | ms/batch 242.76 | loss  4.13 | ppl    61.99\n",
            "| epoch   8 |   140/  141 batches | lr 10.00 | ms/batch 239.65 | loss  4.16 | ppl    63.76\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 52.56s | valid loss  4.23 | valid ppl    68.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |    10/  141 batches | lr 10.00 | ms/batch 262.50 | loss  4.56 | ppl    95.63\n",
            "| epoch   9 |    20/  141 batches | lr 10.00 | ms/batch 242.61 | loss  4.10 | ppl    60.59\n",
            "| epoch   9 |    30/  141 batches | lr 10.00 | ms/batch 241.30 | loss  4.06 | ppl    57.95\n",
            "| epoch   9 |    40/  141 batches | lr 10.00 | ms/batch 242.00 | loss  4.10 | ppl    60.52\n",
            "| epoch   9 |    50/  141 batches | lr 10.00 | ms/batch 241.38 | loss  4.07 | ppl    58.44\n",
            "| epoch   9 |    60/  141 batches | lr 10.00 | ms/batch 241.41 | loss  4.12 | ppl    61.26\n",
            "| epoch   9 |    70/  141 batches | lr 10.00 | ms/batch 240.92 | loss  4.10 | ppl    60.37\n",
            "| epoch   9 |    80/  141 batches | lr 10.00 | ms/batch 241.88 | loss  4.05 | ppl    57.19\n",
            "| epoch   9 |    90/  141 batches | lr 10.00 | ms/batch 241.59 | loss  4.01 | ppl    55.05\n",
            "| epoch   9 |   100/  141 batches | lr 10.00 | ms/batch 240.49 | loss  4.01 | ppl    55.40\n",
            "| epoch   9 |   110/  141 batches | lr 10.00 | ms/batch 243.32 | loss  3.99 | ppl    54.25\n",
            "| epoch   9 |   120/  141 batches | lr 10.00 | ms/batch 242.16 | loss  4.00 | ppl    54.41\n",
            "| epoch   9 |   130/  141 batches | lr 10.00 | ms/batch 241.26 | loss  4.01 | ppl    54.88\n",
            "| epoch   9 |   140/  141 batches | lr 10.00 | ms/batch 240.54 | loss  4.03 | ppl    56.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 52.59s | valid loss  4.14 | valid ppl    62.89\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |    10/  141 batches | lr 10.00 | ms/batch 263.66 | loss  4.43 | ppl    83.81\n",
            "| epoch  10 |    20/  141 batches | lr 10.00 | ms/batch 241.59 | loss  3.98 | ppl    53.61\n",
            "| epoch  10 |    30/  141 batches | lr 10.00 | ms/batch 241.35 | loss  3.94 | ppl    51.67\n",
            "| epoch  10 |    40/  141 batches | lr 10.00 | ms/batch 241.58 | loss  3.97 | ppl    52.83\n",
            "| epoch  10 |    50/  141 batches | lr 10.00 | ms/batch 241.58 | loss  3.95 | ppl    52.16\n",
            "| epoch  10 |    60/  141 batches | lr 10.00 | ms/batch 241.13 | loss  4.00 | ppl    54.52\n",
            "| epoch  10 |    70/  141 batches | lr 10.00 | ms/batch 241.09 | loss  3.98 | ppl    53.47\n",
            "| epoch  10 |    80/  141 batches | lr 10.00 | ms/batch 241.78 | loss  3.94 | ppl    51.60\n",
            "| epoch  10 |    90/  141 batches | lr 10.00 | ms/batch 242.84 | loss  3.91 | ppl    49.88\n",
            "| epoch  10 |   100/  141 batches | lr 10.00 | ms/batch 239.88 | loss  3.89 | ppl    49.12\n",
            "| epoch  10 |   110/  141 batches | lr 10.00 | ms/batch 242.53 | loss  3.88 | ppl    48.25\n",
            "| epoch  10 |   120/  141 batches | lr 10.00 | ms/batch 241.41 | loss  3.90 | ppl    49.28\n",
            "| epoch  10 |   130/  141 batches | lr 10.00 | ms/batch 241.89 | loss  3.92 | ppl    50.28\n",
            "| epoch  10 |   140/  141 batches | lr 10.00 | ms/batch 240.97 | loss  3.94 | ppl    51.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 52.57s | valid loss  4.04 | valid ppl    57.08\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  11 |    10/  141 batches | lr 10.00 | ms/batch 262.30 | loss  4.30 | ppl    73.88\n",
            "| epoch  11 |    20/  141 batches | lr 10.00 | ms/batch 241.70 | loss  3.88 | ppl    48.37\n",
            "| epoch  11 |    30/  141 batches | lr 10.00 | ms/batch 243.15 | loss  3.86 | ppl    47.26\n",
            "| epoch  11 |    40/  141 batches | lr 10.00 | ms/batch 240.64 | loss  3.87 | ppl    47.71\n",
            "| epoch  11 |    50/  141 batches | lr 10.00 | ms/batch 242.01 | loss  3.85 | ppl    46.88\n",
            "| epoch  11 |    60/  141 batches | lr 10.00 | ms/batch 241.46 | loss  3.90 | ppl    49.27\n",
            "| epoch  11 |    70/  141 batches | lr 10.00 | ms/batch 241.45 | loss  3.90 | ppl    49.33\n",
            "| epoch  11 |    80/  141 batches | lr 10.00 | ms/batch 242.90 | loss  3.86 | ppl    47.37\n",
            "| epoch  11 |    90/  141 batches | lr 10.00 | ms/batch 241.64 | loss  3.81 | ppl    44.94\n",
            "| epoch  11 |   100/  141 batches | lr 10.00 | ms/batch 240.36 | loss  3.79 | ppl    44.15\n",
            "| epoch  11 |   110/  141 batches | lr 10.00 | ms/batch 242.53 | loss  3.78 | ppl    43.78\n",
            "| epoch  11 |   120/  141 batches | lr 10.00 | ms/batch 242.82 | loss  3.80 | ppl    44.64\n",
            "| epoch  11 |   130/  141 batches | lr 10.00 | ms/batch 239.21 | loss  3.82 | ppl    45.73\n",
            "| epoch  11 |   140/  141 batches | lr 10.00 | ms/batch 244.06 | loss  3.84 | ppl    46.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time: 52.61s | valid loss  4.00 | valid ppl    54.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  12 |    10/  141 batches | lr 10.00 | ms/batch 262.47 | loss  4.21 | ppl    67.17\n",
            "| epoch  12 |    20/  141 batches | lr 10.00 | ms/batch 241.09 | loss  3.79 | ppl    44.22\n",
            "| epoch  12 |    30/  141 batches | lr 10.00 | ms/batch 241.04 | loss  3.78 | ppl    43.87\n",
            "| epoch  12 |    40/  141 batches | lr 10.00 | ms/batch 240.50 | loss  3.78 | ppl    43.97\n",
            "| epoch  12 |    50/  141 batches | lr 10.00 | ms/batch 242.88 | loss  3.75 | ppl    42.46\n",
            "| epoch  12 |    60/  141 batches | lr 10.00 | ms/batch 240.34 | loss  3.82 | ppl    45.60\n",
            "| epoch  12 |    70/  141 batches | lr 10.00 | ms/batch 242.45 | loss  3.82 | ppl    45.47\n",
            "| epoch  12 |    80/  141 batches | lr 10.00 | ms/batch 241.90 | loss  3.77 | ppl    43.53\n",
            "| epoch  12 |    90/  141 batches | lr 10.00 | ms/batch 240.68 | loss  3.72 | ppl    41.46\n",
            "| epoch  12 |   100/  141 batches | lr 10.00 | ms/batch 240.81 | loss  3.73 | ppl    41.70\n",
            "| epoch  12 |   110/  141 batches | lr 10.00 | ms/batch 242.60 | loss  3.71 | ppl    40.74\n",
            "| epoch  12 |   120/  141 batches | lr 10.00 | ms/batch 242.03 | loss  3.71 | ppl    41.05\n",
            "| epoch  12 |   130/  141 batches | lr 10.00 | ms/batch 241.01 | loss  3.73 | ppl    41.81\n",
            "| epoch  12 |   140/  141 batches | lr 10.00 | ms/batch 241.00 | loss  3.75 | ppl    42.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time: 52.53s | valid loss  3.94 | valid ppl    51.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  13 |    10/  141 batches | lr 10.00 | ms/batch 263.28 | loss  4.13 | ppl    62.09\n",
            "| epoch  13 |    20/  141 batches | lr 10.00 | ms/batch 241.29 | loss  3.71 | ppl    40.96\n",
            "| epoch  13 |    30/  141 batches | lr 10.00 | ms/batch 240.78 | loss  3.72 | ppl    41.10\n",
            "| epoch  13 |    40/  141 batches | lr 10.00 | ms/batch 240.71 | loss  3.71 | ppl    41.01\n",
            "| epoch  13 |    50/  141 batches | lr 10.00 | ms/batch 240.41 | loss  3.69 | ppl    40.14\n",
            "| epoch  13 |    60/  141 batches | lr 10.00 | ms/batch 240.57 | loss  3.74 | ppl    42.25\n",
            "| epoch  13 |    70/  141 batches | lr 10.00 | ms/batch 240.37 | loss  3.72 | ppl    41.40\n",
            "| epoch  13 |    80/  141 batches | lr 10.00 | ms/batch 241.27 | loss  3.71 | ppl    40.88\n",
            "| epoch  13 |    90/  141 batches | lr 10.00 | ms/batch 240.96 | loss  3.65 | ppl    38.36\n",
            "| epoch  13 |   100/  141 batches | lr 10.00 | ms/batch 242.19 | loss  3.66 | ppl    38.91\n",
            "| epoch  13 |   110/  141 batches | lr 10.00 | ms/batch 241.49 | loss  3.65 | ppl    38.46\n",
            "| epoch  13 |   120/  141 batches | lr 10.00 | ms/batch 241.00 | loss  3.66 | ppl    39.04\n",
            "| epoch  13 |   130/  141 batches | lr 10.00 | ms/batch 241.48 | loss  3.66 | ppl    38.82\n",
            "| epoch  13 |   140/  141 batches | lr 10.00 | ms/batch 240.78 | loss  3.68 | ppl    39.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time: 52.50s | valid loss  3.91 | valid ppl    50.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  14 |    10/  141 batches | lr 10.00 | ms/batch 262.24 | loss  4.06 | ppl    57.72\n",
            "| epoch  14 |    20/  141 batches | lr 10.00 | ms/batch 240.38 | loss  3.67 | ppl    39.28\n",
            "| epoch  14 |    30/  141 batches | lr 10.00 | ms/batch 242.89 | loss  3.64 | ppl    37.95\n",
            "| epoch  14 |    40/  141 batches | lr 10.00 | ms/batch 241.16 | loss  3.65 | ppl    38.32\n",
            "| epoch  14 |    50/  141 batches | lr 10.00 | ms/batch 241.52 | loss  3.62 | ppl    37.27\n",
            "| epoch  14 |    60/  141 batches | lr 10.00 | ms/batch 242.50 | loss  3.67 | ppl    39.28\n",
            "| epoch  14 |    70/  141 batches | lr 10.00 | ms/batch 238.83 | loss  3.66 | ppl    38.75\n",
            "| epoch  14 |    80/  141 batches | lr 10.00 | ms/batch 241.10 | loss  3.65 | ppl    38.40\n",
            "| epoch  14 |    90/  141 batches | lr 10.00 | ms/batch 242.08 | loss  3.58 | ppl    36.04\n",
            "| epoch  14 |   100/  141 batches | lr 10.00 | ms/batch 240.74 | loss  3.59 | ppl    36.14\n",
            "| epoch  14 |   110/  141 batches | lr 10.00 | ms/batch 242.87 | loss  3.57 | ppl    35.48\n",
            "| epoch  14 |   120/  141 batches | lr 10.00 | ms/batch 241.70 | loss  3.60 | ppl    36.47\n",
            "| epoch  14 |   130/  141 batches | lr 10.00 | ms/batch 240.19 | loss  3.60 | ppl    36.52\n",
            "| epoch  14 |   140/  141 batches | lr 10.00 | ms/batch 241.02 | loss  3.62 | ppl    37.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time: 52.51s | valid loss  3.87 | valid ppl    47.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  15 |    10/  141 batches | lr 10.00 | ms/batch 264.29 | loss  3.97 | ppl    53.06\n",
            "| epoch  15 |    20/  141 batches | lr 10.00 | ms/batch 243.36 | loss  3.61 | ppl    37.13\n",
            "| epoch  15 |    30/  141 batches | lr 10.00 | ms/batch 241.03 | loss  3.57 | ppl    35.61\n",
            "| epoch  15 |    40/  141 batches | lr 10.00 | ms/batch 239.74 | loss  3.58 | ppl    35.93\n",
            "| epoch  15 |    50/  141 batches | lr 10.00 | ms/batch 241.69 | loss  3.56 | ppl    35.21\n",
            "| epoch  15 |    60/  141 batches | lr 10.00 | ms/batch 240.16 | loss  3.61 | ppl    37.10\n",
            "| epoch  15 |    70/  141 batches | lr 10.00 | ms/batch 240.59 | loss  3.59 | ppl    36.33\n",
            "| epoch  15 |    80/  141 batches | lr 10.00 | ms/batch 241.76 | loss  3.56 | ppl    35.08\n",
            "| epoch  15 |    90/  141 batches | lr 10.00 | ms/batch 239.72 | loss  3.55 | ppl    34.79\n",
            "| epoch  15 |   100/  141 batches | lr 10.00 | ms/batch 241.48 | loss  3.52 | ppl    33.65\n",
            "| epoch  15 |   110/  141 batches | lr 10.00 | ms/batch 240.61 | loss  3.50 | ppl    33.21\n",
            "| epoch  15 |   120/  141 batches | lr 10.00 | ms/batch 240.60 | loss  3.53 | ppl    34.06\n",
            "| epoch  15 |   130/  141 batches | lr 10.00 | ms/batch 242.19 | loss  3.54 | ppl    34.34\n",
            "| epoch  15 |   140/  141 batches | lr 10.00 | ms/batch 241.22 | loss  3.56 | ppl    35.10\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time: 52.52s | valid loss  3.84 | valid ppl    46.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  16 |    10/  141 batches | lr 10.00 | ms/batch 262.54 | loss  3.92 | ppl    50.61\n",
            "| epoch  16 |    20/  141 batches | lr 10.00 | ms/batch 239.68 | loss  3.53 | ppl    34.21\n",
            "| epoch  16 |    30/  141 batches | lr 10.00 | ms/batch 242.27 | loss  3.52 | ppl    33.78\n",
            "| epoch  16 |    40/  141 batches | lr 10.00 | ms/batch 242.35 | loss  3.53 | ppl    34.17\n",
            "| epoch  16 |    50/  141 batches | lr 10.00 | ms/batch 240.69 | loss  3.50 | ppl    33.24\n",
            "| epoch  16 |    60/  141 batches | lr 10.00 | ms/batch 240.83 | loss  3.57 | ppl    35.47\n",
            "| epoch  16 |    70/  141 batches | lr 10.00 | ms/batch 241.08 | loss  3.53 | ppl    34.13\n",
            "| epoch  16 |    80/  141 batches | lr 10.00 | ms/batch 241.13 | loss  3.51 | ppl    33.50\n",
            "| epoch  16 |    90/  141 batches | lr 10.00 | ms/batch 243.22 | loss  3.48 | ppl    32.52\n",
            "| epoch  16 |   100/  141 batches | lr 10.00 | ms/batch 241.82 | loss  3.48 | ppl    32.49\n",
            "| epoch  16 |   110/  141 batches | lr 10.00 | ms/batch 241.24 | loss  3.45 | ppl    31.64\n",
            "| epoch  16 |   120/  141 batches | lr 10.00 | ms/batch 243.09 | loss  3.47 | ppl    31.98\n",
            "| epoch  16 |   130/  141 batches | lr 10.00 | ms/batch 241.18 | loss  3.48 | ppl    32.49\n",
            "| epoch  16 |   140/  141 batches | lr 10.00 | ms/batch 241.76 | loss  3.51 | ppl    33.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time: 52.57s | valid loss  3.82 | valid ppl    45.66\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  17 |    10/  141 batches | lr 10.00 | ms/batch 264.07 | loss  3.86 | ppl    47.56\n",
            "| epoch  17 |    20/  141 batches | lr 10.00 | ms/batch 241.15 | loss  3.48 | ppl    32.47\n",
            "| epoch  17 |    30/  141 batches | lr 10.00 | ms/batch 241.02 | loss  3.46 | ppl    31.84\n",
            "| epoch  17 |    40/  141 batches | lr 10.00 | ms/batch 241.61 | loss  3.48 | ppl    32.61\n",
            "| epoch  17 |    50/  141 batches | lr 10.00 | ms/batch 240.85 | loss  3.44 | ppl    31.19\n",
            "| epoch  17 |    60/  141 batches | lr 10.00 | ms/batch 242.60 | loss  3.49 | ppl    32.86\n",
            "| epoch  17 |    70/  141 batches | lr 10.00 | ms/batch 240.53 | loss  3.47 | ppl    32.13\n",
            "| epoch  17 |    80/  141 batches | lr 10.00 | ms/batch 241.18 | loss  3.45 | ppl    31.66\n",
            "| epoch  17 |    90/  141 batches | lr 10.00 | ms/batch 242.29 | loss  3.42 | ppl    30.43\n",
            "| epoch  17 |   100/  141 batches | lr 10.00 | ms/batch 240.45 | loss  3.41 | ppl    30.27\n",
            "| epoch  17 |   110/  141 batches | lr 10.00 | ms/batch 240.15 | loss  3.40 | ppl    30.01\n",
            "| epoch  17 |   120/  141 batches | lr 10.00 | ms/batch 241.45 | loss  3.42 | ppl    30.65\n",
            "| epoch  17 |   130/  141 batches | lr 10.00 | ms/batch 240.75 | loss  3.43 | ppl    31.02\n",
            "| epoch  17 |   140/  141 batches | lr 10.00 | ms/batch 241.04 | loss  3.45 | ppl    31.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time: 52.51s | valid loss  3.80 | valid ppl    44.81\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  18 |    10/  141 batches | lr 10.00 | ms/batch 262.41 | loss  3.79 | ppl    44.36\n",
            "| epoch  18 |    20/  141 batches | lr 10.00 | ms/batch 241.29 | loss  3.42 | ppl    30.69\n",
            "| epoch  18 |    30/  141 batches | lr 10.00 | ms/batch 240.32 | loss  3.40 | ppl    29.98\n",
            "| epoch  18 |    40/  141 batches | lr 10.00 | ms/batch 241.44 | loss  3.41 | ppl    30.32\n",
            "| epoch  18 |    50/  141 batches | lr 10.00 | ms/batch 242.35 | loss  3.42 | ppl    30.54\n",
            "| epoch  18 |    60/  141 batches | lr 10.00 | ms/batch 239.83 | loss  3.45 | ppl    31.64\n",
            "| epoch  18 |    70/  141 batches | lr 10.00 | ms/batch 242.77 | loss  3.43 | ppl    30.74\n",
            "| epoch  18 |    80/  141 batches | lr 10.00 | ms/batch 241.43 | loss  3.39 | ppl    29.66\n",
            "| epoch  18 |    90/  141 batches | lr 10.00 | ms/batch 240.25 | loss  3.37 | ppl    28.97\n",
            "| epoch  18 |   100/  141 batches | lr 10.00 | ms/batch 241.99 | loss  3.36 | ppl    28.69\n",
            "| epoch  18 |   110/  141 batches | lr 10.00 | ms/batch 241.02 | loss  3.36 | ppl    28.75\n",
            "| epoch  18 |   120/  141 batches | lr 10.00 | ms/batch 241.64 | loss  3.38 | ppl    29.25\n",
            "| epoch  18 |   130/  141 batches | lr 10.00 | ms/batch 242.28 | loss  3.38 | ppl    29.33\n",
            "| epoch  18 |   140/  141 batches | lr 10.00 | ms/batch 242.41 | loss  3.40 | ppl    29.86\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time: 52.54s | valid loss  3.80 | valid ppl    44.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  19 |    10/  141 batches | lr 10.00 | ms/batch 264.55 | loss  3.74 | ppl    41.90\n",
            "| epoch  19 |    20/  141 batches | lr 10.00 | ms/batch 242.34 | loss  3.36 | ppl    28.90\n",
            "| epoch  19 |    30/  141 batches | lr 10.00 | ms/batch 240.99 | loss  3.36 | ppl    28.84\n",
            "| epoch  19 |    40/  141 batches | lr 10.00 | ms/batch 242.45 | loss  3.36 | ppl    28.84\n",
            "| epoch  19 |    50/  141 batches | lr 10.00 | ms/batch 241.16 | loss  3.35 | ppl    28.48\n",
            "| epoch  19 |    60/  141 batches | lr 10.00 | ms/batch 242.22 | loss  3.39 | ppl    29.64\n",
            "| epoch  19 |    70/  141 batches | lr 10.00 | ms/batch 243.06 | loss  3.37 | ppl    29.19\n",
            "| epoch  19 |    80/  141 batches | lr 10.00 | ms/batch 242.40 | loss  3.33 | ppl    28.03\n",
            "| epoch  19 |    90/  141 batches | lr 10.00 | ms/batch 241.12 | loss  3.34 | ppl    28.26\n",
            "| epoch  19 |   100/  141 batches | lr 10.00 | ms/batch 242.11 | loss  3.31 | ppl    27.45\n",
            "| epoch  19 |   110/  141 batches | lr 10.00 | ms/batch 240.63 | loss  3.29 | ppl    26.79\n",
            "| epoch  19 |   120/  141 batches | lr 10.00 | ms/batch 240.75 | loss  3.32 | ppl    27.54\n",
            "| epoch  19 |   130/  141 batches | lr 10.00 | ms/batch 241.19 | loss  3.32 | ppl    27.57\n",
            "| epoch  19 |   140/  141 batches | lr 10.00 | ms/batch 240.58 | loss  3.35 | ppl    28.47\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time: 52.57s | valid loss  3.79 | valid ppl    44.20\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  20 |    10/  141 batches | lr 10.00 | ms/batch 263.52 | loss  3.68 | ppl    39.75\n",
            "| epoch  20 |    20/  141 batches | lr 10.00 | ms/batch 240.59 | loss  3.31 | ppl    27.50\n",
            "| epoch  20 |    30/  141 batches | lr 10.00 | ms/batch 240.58 | loss  3.31 | ppl    27.25\n",
            "| epoch  20 |    40/  141 batches | lr 10.00 | ms/batch 242.39 | loss  3.32 | ppl    27.57\n",
            "| epoch  20 |    50/  141 batches | lr 10.00 | ms/batch 239.61 | loss  3.30 | ppl    27.05\n",
            "| epoch  20 |    60/  141 batches | lr 10.00 | ms/batch 242.10 | loss  3.34 | ppl    28.24\n",
            "| epoch  20 |    70/  141 batches | lr 10.00 | ms/batch 242.95 | loss  3.33 | ppl    27.91\n",
            "| epoch  20 |    80/  141 batches | lr 10.00 | ms/batch 241.14 | loss  3.30 | ppl    27.11\n",
            "| epoch  20 |    90/  141 batches | lr 10.00 | ms/batch 239.80 | loss  3.27 | ppl    26.42\n",
            "| epoch  20 |   100/  141 batches | lr 10.00 | ms/batch 240.34 | loss  3.26 | ppl    26.04\n",
            "| epoch  20 |   110/  141 batches | lr 10.00 | ms/batch 241.19 | loss  3.25 | ppl    25.85\n",
            "| epoch  20 |   120/  141 batches | lr 10.00 | ms/batch 240.95 | loss  3.27 | ppl    26.25\n",
            "| epoch  20 |   130/  141 batches | lr 10.00 | ms/batch 240.86 | loss  3.28 | ppl    26.52\n",
            "| epoch  20 |   140/  141 batches | lr 10.00 | ms/batch 241.71 | loss  3.29 | ppl    26.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time: 52.50s | valid loss  3.77 | valid ppl    43.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  21 |    10/  141 batches | lr 10.00 | ms/batch 263.17 | loss  3.62 | ppl    37.49\n",
            "| epoch  21 |    20/  141 batches | lr 10.00 | ms/batch 242.64 | loss  3.27 | ppl    26.35\n",
            "| epoch  21 |    30/  141 batches | lr 10.00 | ms/batch 242.87 | loss  3.26 | ppl    26.01\n",
            "| epoch  21 |    40/  141 batches | lr 10.00 | ms/batch 240.80 | loss  3.26 | ppl    26.18\n",
            "| epoch  21 |    50/  141 batches | lr 10.00 | ms/batch 241.65 | loss  3.26 | ppl    25.96\n",
            "| epoch  21 |    60/  141 batches | lr 10.00 | ms/batch 241.39 | loss  3.29 | ppl    26.97\n",
            "| epoch  21 |    70/  141 batches | lr 10.00 | ms/batch 241.29 | loss  3.28 | ppl    26.61\n",
            "| epoch  21 |    80/  141 batches | lr 10.00 | ms/batch 241.56 | loss  3.24 | ppl    25.66\n",
            "| epoch  21 |    90/  141 batches | lr 10.00 | ms/batch 241.66 | loss  3.22 | ppl    25.00\n",
            "| epoch  21 |   100/  141 batches | lr 10.00 | ms/batch 241.38 | loss  3.21 | ppl    24.71\n",
            "| epoch  21 |   110/  141 batches | lr 10.00 | ms/batch 242.21 | loss  3.19 | ppl    24.29\n",
            "| epoch  21 |   120/  141 batches | lr 10.00 | ms/batch 240.35 | loss  3.23 | ppl    25.20\n",
            "| epoch  21 |   130/  141 batches | lr 10.00 | ms/batch 240.96 | loss  3.24 | ppl    25.52\n",
            "| epoch  21 |   140/  141 batches | lr 10.00 | ms/batch 243.50 | loss  3.24 | ppl    25.64\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  21 | time: 52.57s | valid loss  3.77 | valid ppl    43.46\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  22 |    10/  141 batches | lr 10.00 | ms/batch 263.60 | loss  3.57 | ppl    35.56\n",
            "| epoch  22 |    20/  141 batches | lr 10.00 | ms/batch 240.61 | loss  3.21 | ppl    24.89\n",
            "| epoch  22 |    30/  141 batches | lr 10.00 | ms/batch 240.28 | loss  3.21 | ppl    24.72\n",
            "| epoch  22 |    40/  141 batches | lr 10.00 | ms/batch 241.10 | loss  3.23 | ppl    25.21\n",
            "| epoch  22 |    50/  141 batches | lr 10.00 | ms/batch 240.78 | loss  3.20 | ppl    24.57\n",
            "| epoch  22 |    60/  141 batches | lr 10.00 | ms/batch 243.22 | loss  3.24 | ppl    25.65\n",
            "| epoch  22 |    70/  141 batches | lr 10.00 | ms/batch 243.00 | loss  3.23 | ppl    25.33\n",
            "| epoch  22 |    80/  141 batches | lr 10.00 | ms/batch 243.01 | loss  3.21 | ppl    24.77\n",
            "| epoch  22 |    90/  141 batches | lr 10.00 | ms/batch 239.79 | loss  3.17 | ppl    23.70\n",
            "| epoch  22 |   100/  141 batches | lr 10.00 | ms/batch 243.45 | loss  3.17 | ppl    23.75\n",
            "| epoch  22 |   110/  141 batches | lr 10.00 | ms/batch 242.40 | loss  3.15 | ppl    23.36\n",
            "| epoch  22 |   120/  141 batches | lr 10.00 | ms/batch 241.68 | loss  3.17 | ppl    23.84\n",
            "| epoch  22 |   130/  141 batches | lr 10.00 | ms/batch 242.14 | loss  3.19 | ppl    24.31\n",
            "| epoch  22 |   140/  141 batches | lr 10.00 | ms/batch 241.95 | loss  3.21 | ppl    24.80\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  22 | time: 52.60s | valid loss  3.77 | valid ppl    43.18\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  23 |    10/  141 batches | lr 10.00 | ms/batch 264.69 | loss  3.51 | ppl    33.60\n",
            "| epoch  23 |    20/  141 batches | lr 10.00 | ms/batch 243.38 | loss  3.17 | ppl    23.80\n",
            "| epoch  23 |    30/  141 batches | lr 10.00 | ms/batch 240.63 | loss  3.16 | ppl    23.54\n",
            "| epoch  23 |    40/  141 batches | lr 10.00 | ms/batch 242.00 | loss  3.17 | ppl    23.76\n",
            "| epoch  23 |    50/  141 batches | lr 10.00 | ms/batch 241.77 | loss  3.15 | ppl    23.40\n",
            "| epoch  23 |    60/  141 batches | lr 10.00 | ms/batch 241.84 | loss  3.19 | ppl    24.32\n",
            "| epoch  23 |    70/  141 batches | lr 10.00 | ms/batch 242.70 | loss  3.18 | ppl    24.04\n",
            "| epoch  23 |    80/  141 batches | lr 10.00 | ms/batch 244.18 | loss  3.16 | ppl    23.62\n",
            "| epoch  23 |    90/  141 batches | lr 10.00 | ms/batch 241.66 | loss  3.13 | ppl    22.91\n",
            "| epoch  23 |   100/  141 batches | lr 10.00 | ms/batch 240.46 | loss  3.11 | ppl    22.40\n",
            "| epoch  23 |   110/  141 batches | lr 10.00 | ms/batch 242.68 | loss  3.11 | ppl    22.39\n",
            "| epoch  23 |   120/  141 batches | lr 10.00 | ms/batch 241.65 | loss  3.12 | ppl    22.69\n",
            "| epoch  23 |   130/  141 batches | lr 10.00 | ms/batch 242.80 | loss  3.13 | ppl    22.98\n",
            "| epoch  23 |   140/  141 batches | lr 10.00 | ms/batch 243.39 | loss  3.17 | ppl    23.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  23 | time: 52.68s | valid loss  3.76 | valid ppl    43.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  24 |    10/  141 batches | lr 10.00 | ms/batch 261.61 | loss  3.47 | ppl    31.98\n",
            "| epoch  24 |    20/  141 batches | lr 10.00 | ms/batch 242.23 | loss  3.11 | ppl    22.53\n",
            "| epoch  24 |    30/  141 batches | lr 10.00 | ms/batch 241.97 | loss  3.11 | ppl    22.43\n",
            "| epoch  24 |    40/  141 batches | lr 10.00 | ms/batch 241.83 | loss  3.13 | ppl    22.82\n",
            "| epoch  24 |    50/  141 batches | lr 10.00 | ms/batch 243.05 | loss  3.11 | ppl    22.36\n",
            "| epoch  24 |    60/  141 batches | lr 10.00 | ms/batch 243.24 | loss  3.14 | ppl    23.16\n",
            "| epoch  24 |    70/  141 batches | lr 10.00 | ms/batch 241.25 | loss  3.13 | ppl    22.87\n",
            "| epoch  24 |    80/  141 batches | lr 10.00 | ms/batch 242.06 | loss  3.11 | ppl    22.42\n",
            "| epoch  24 |    90/  141 batches | lr 10.00 | ms/batch 241.93 | loss  3.08 | ppl    21.71\n",
            "| epoch  24 |   100/  141 batches | lr 10.00 | ms/batch 240.45 | loss  3.07 | ppl    21.62\n",
            "| epoch  24 |   110/  141 batches | lr 10.00 | ms/batch 243.85 | loss  3.06 | ppl    21.40\n",
            "| epoch  24 |   120/  141 batches | lr 10.00 | ms/batch 241.91 | loss  3.07 | ppl    21.56\n",
            "| epoch  24 |   130/  141 batches | lr 10.00 | ms/batch 240.81 | loss  3.08 | ppl    21.82\n",
            "| epoch  24 |   140/  141 batches | lr 10.00 | ms/batch 242.92 | loss  3.12 | ppl    22.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  24 | time: 52.63s | valid loss  3.76 | valid ppl    43.12\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  25 |    10/  141 batches | lr 10.00 | ms/batch 264.25 | loss  3.42 | ppl    30.47\n",
            "| epoch  25 |    20/  141 batches | lr 10.00 | ms/batch 242.34 | loss  3.07 | ppl    21.63\n",
            "| epoch  25 |    30/  141 batches | lr 10.00 | ms/batch 243.80 | loss  3.06 | ppl    21.37\n",
            "| epoch  25 |    40/  141 batches | lr 10.00 | ms/batch 242.15 | loss  3.08 | ppl    21.86\n",
            "| epoch  25 |    50/  141 batches | lr 10.00 | ms/batch 243.04 | loss  3.06 | ppl    21.43\n",
            "| epoch  25 |    60/  141 batches | lr 10.00 | ms/batch 244.35 | loss  3.10 | ppl    22.09\n",
            "| epoch  25 |    70/  141 batches | lr 10.00 | ms/batch 243.65 | loss  3.08 | ppl    21.81\n",
            "| epoch  25 |    80/  141 batches | lr 10.00 | ms/batch 241.18 | loss  3.06 | ppl    21.31\n",
            "| epoch  25 |    90/  141 batches | lr 10.00 | ms/batch 242.75 | loss  3.03 | ppl    20.77\n",
            "| epoch  25 |   100/  141 batches | lr 10.00 | ms/batch 243.90 | loss  3.02 | ppl    20.57\n",
            "| epoch  25 |   110/  141 batches | lr 10.00 | ms/batch 241.65 | loss  3.01 | ppl    20.37\n",
            "| epoch  25 |   120/  141 batches | lr 10.00 | ms/batch 244.55 | loss  3.02 | ppl    20.55\n",
            "| epoch  25 |   130/  141 batches | lr 10.00 | ms/batch 244.02 | loss  3.04 | ppl    20.81\n",
            "| epoch  25 |   140/  141 batches | lr 10.00 | ms/batch 243.95 | loss  3.06 | ppl    21.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  25 | time: 52.79s | valid loss  3.77 | valid ppl    43.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  26 |    10/  141 batches | lr 2.50 | ms/batch 265.95 | loss  3.36 | ppl    28.84\n",
            "| epoch  26 |    20/  141 batches | lr 2.50 | ms/batch 242.52 | loss  3.02 | ppl    20.54\n",
            "| epoch  26 |    30/  141 batches | lr 2.50 | ms/batch 243.35 | loss  3.02 | ppl    20.46\n",
            "| epoch  26 |    40/  141 batches | lr 2.50 | ms/batch 241.21 | loss  3.04 | ppl    20.82\n",
            "| epoch  26 |    50/  141 batches | lr 2.50 | ms/batch 245.13 | loss  3.02 | ppl    20.47\n",
            "| epoch  26 |    60/  141 batches | lr 2.50 | ms/batch 243.05 | loss  3.04 | ppl    21.01\n",
            "| epoch  26 |    70/  141 batches | lr 2.50 | ms/batch 243.57 | loss  3.04 | ppl    20.83\n",
            "| epoch  26 |    80/  141 batches | lr 2.50 | ms/batch 241.59 | loss  3.01 | ppl    20.29\n",
            "| epoch  26 |    90/  141 batches | lr 2.50 | ms/batch 242.67 | loss  2.99 | ppl    19.80\n",
            "| epoch  26 |   100/  141 batches | lr 2.50 | ms/batch 242.14 | loss  2.97 | ppl    19.48\n",
            "| epoch  26 |   110/  141 batches | lr 2.50 | ms/batch 242.15 | loss  2.97 | ppl    19.48\n",
            "| epoch  26 |   120/  141 batches | lr 2.50 | ms/batch 242.35 | loss  2.98 | ppl    19.72\n",
            "| epoch  26 |   130/  141 batches | lr 2.50 | ms/batch 243.38 | loss  2.99 | ppl    19.92\n",
            "| epoch  26 |   140/  141 batches | lr 2.50 | ms/batch 243.58 | loss  3.02 | ppl    20.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  26 | time: 52.75s | valid loss  3.76 | valid ppl    43.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  27 |    10/  141 batches | lr 2.50 | ms/batch 264.25 | loss  3.32 | ppl    27.65\n",
            "| epoch  27 |    20/  141 batches | lr 2.50 | ms/batch 241.61 | loss  2.98 | ppl    19.71\n",
            "| epoch  27 |    30/  141 batches | lr 2.50 | ms/batch 242.92 | loss  2.98 | ppl    19.70\n",
            "| epoch  27 |    40/  141 batches | lr 2.50 | ms/batch 242.44 | loss  2.99 | ppl    19.80\n",
            "| epoch  27 |    50/  141 batches | lr 2.50 | ms/batch 240.81 | loss  2.98 | ppl    19.60\n",
            "| epoch  27 |    60/  141 batches | lr 2.50 | ms/batch 242.71 | loss  3.00 | ppl    20.08\n",
            "| epoch  27 |    70/  141 batches | lr 2.50 | ms/batch 243.30 | loss  2.99 | ppl    19.79\n",
            "| epoch  27 |    80/  141 batches | lr 2.50 | ms/batch 242.15 | loss  2.96 | ppl    19.28\n",
            "| epoch  27 |    90/  141 batches | lr 2.50 | ms/batch 241.48 | loss  2.94 | ppl    18.92\n",
            "| epoch  27 |   100/  141 batches | lr 2.50 | ms/batch 241.56 | loss  2.93 | ppl    18.69\n",
            "| epoch  27 |   110/  141 batches | lr 2.50 | ms/batch 241.45 | loss  2.93 | ppl    18.79\n",
            "| epoch  27 |   120/  141 batches | lr 2.50 | ms/batch 242.79 | loss  2.94 | ppl    18.84\n",
            "| epoch  27 |   130/  141 batches | lr 2.50 | ms/batch 242.19 | loss  2.94 | ppl    18.93\n",
            "| epoch  27 |   140/  141 batches | lr 2.50 | ms/batch 241.88 | loss  2.98 | ppl    19.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  27 | time: 52.65s | valid loss  3.77 | valid ppl    43.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  28 |    10/  141 batches | lr 0.62 | ms/batch 265.93 | loss  3.27 | ppl    26.27\n",
            "| epoch  28 |    20/  141 batches | lr 0.62 | ms/batch 241.94 | loss  2.94 | ppl    18.94\n",
            "| epoch  28 |    30/  141 batches | lr 0.62 | ms/batch 241.34 | loss  2.93 | ppl    18.65\n",
            "| epoch  28 |    40/  141 batches | lr 0.62 | ms/batch 243.90 | loss  2.93 | ppl    18.79\n",
            "| epoch  28 |    50/  141 batches | lr 0.62 | ms/batch 243.41 | loss  2.93 | ppl    18.70\n",
            "| epoch  28 |    60/  141 batches | lr 0.62 | ms/batch 243.00 | loss  2.96 | ppl    19.35\n",
            "| epoch  28 |    70/  141 batches | lr 0.62 | ms/batch 241.95 | loss  2.94 | ppl    18.98\n",
            "| epoch  28 |    80/  141 batches | lr 0.62 | ms/batch 243.05 | loss  2.92 | ppl    18.53\n",
            "| epoch  28 |    90/  141 batches | lr 0.62 | ms/batch 242.44 | loss  2.90 | ppl    18.11\n",
            "| epoch  28 |   100/  141 batches | lr 0.62 | ms/batch 242.63 | loss  2.89 | ppl    17.97\n",
            "| epoch  28 |   110/  141 batches | lr 0.62 | ms/batch 244.46 | loss  2.88 | ppl    17.77\n",
            "| epoch  28 |   120/  141 batches | lr 0.62 | ms/batch 243.27 | loss  2.90 | ppl    18.19\n",
            "| epoch  28 |   130/  141 batches | lr 0.62 | ms/batch 241.96 | loss  2.90 | ppl    18.15\n",
            "| epoch  28 |   140/  141 batches | lr 0.62 | ms/batch 242.31 | loss  2.94 | ppl    18.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  28 | time: 52.74s | valid loss  3.78 | valid ppl    43.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  29 |    10/  141 batches | lr 0.16 | ms/batch 266.61 | loss  3.23 | ppl    25.28\n",
            "| epoch  29 |    20/  141 batches | lr 0.16 | ms/batch 241.84 | loss  2.89 | ppl    18.02\n",
            "| epoch  29 |    30/  141 batches | lr 0.16 | ms/batch 242.82 | loss  2.89 | ppl    17.95\n",
            "| epoch  29 |    40/  141 batches | lr 0.16 | ms/batch 242.96 | loss  2.90 | ppl    18.12\n",
            "| epoch  29 |    50/  141 batches | lr 0.16 | ms/batch 241.26 | loss  2.88 | ppl    17.82\n",
            "| epoch  29 |    60/  141 batches | lr 0.16 | ms/batch 241.28 | loss  2.91 | ppl    18.39\n",
            "| epoch  29 |    70/  141 batches | lr 0.16 | ms/batch 241.80 | loss  2.90 | ppl    18.19\n",
            "| epoch  29 |    80/  141 batches | lr 0.16 | ms/batch 241.78 | loss  2.87 | ppl    17.72\n",
            "| epoch  29 |    90/  141 batches | lr 0.16 | ms/batch 243.82 | loss  2.85 | ppl    17.32\n",
            "| epoch  29 |   100/  141 batches | lr 0.16 | ms/batch 241.83 | loss  2.84 | ppl    17.07\n",
            "| epoch  29 |   110/  141 batches | lr 0.16 | ms/batch 243.58 | loss  2.84 | ppl    17.12\n",
            "| epoch  29 |   120/  141 batches | lr 0.16 | ms/batch 241.39 | loss  2.85 | ppl    17.25\n",
            "| epoch  29 |   130/  141 batches | lr 0.16 | ms/batch 243.30 | loss  2.86 | ppl    17.49\n",
            "| epoch  29 |   140/  141 batches | lr 0.16 | ms/batch 242.96 | loss  2.89 | ppl    17.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  29 | time: 52.68s | valid loss  3.79 | valid ppl    44.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  30 |    10/  141 batches | lr 0.04 | ms/batch 266.36 | loss  3.17 | ppl    23.87\n",
            "| epoch  30 |    20/  141 batches | lr 0.04 | ms/batch 242.04 | loss  2.85 | ppl    17.35\n",
            "| epoch  30 |    30/  141 batches | lr 0.04 | ms/batch 241.13 | loss  2.85 | ppl    17.35\n",
            "| epoch  30 |    40/  141 batches | lr 0.04 | ms/batch 243.43 | loss  2.85 | ppl    17.21\n",
            "| epoch  30 |    50/  141 batches | lr 0.04 | ms/batch 242.11 | loss  2.84 | ppl    17.15\n",
            "| epoch  30 |    60/  141 batches | lr 0.04 | ms/batch 243.50 | loss  2.87 | ppl    17.64\n",
            "| epoch  30 |    70/  141 batches | lr 0.04 | ms/batch 240.93 | loss  2.86 | ppl    17.38\n",
            "| epoch  30 |    80/  141 batches | lr 0.04 | ms/batch 243.38 | loss  2.84 | ppl    17.04\n",
            "| epoch  30 |    90/  141 batches | lr 0.04 | ms/batch 242.53 | loss  2.81 | ppl    16.63\n",
            "| epoch  30 |   100/  141 batches | lr 0.04 | ms/batch 242.43 | loss  2.80 | ppl    16.51\n",
            "| epoch  30 |   110/  141 batches | lr 0.04 | ms/batch 243.68 | loss  2.80 | ppl    16.38\n",
            "| epoch  30 |   120/  141 batches | lr 0.04 | ms/batch 243.24 | loss  2.81 | ppl    16.54\n",
            "| epoch  30 |   130/  141 batches | lr 0.04 | ms/batch 243.17 | loss  2.82 | ppl    16.75\n",
            "| epoch  30 |   140/  141 batches | lr 0.04 | ms/batch 241.64 | loss  2.84 | ppl    17.17\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  30 | time: 52.72s | valid loss  3.80 | valid ppl    44.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  31 |    10/  141 batches | lr 0.01 | ms/batch 266.10 | loss  3.13 | ppl    22.86\n",
            "| epoch  31 |    20/  141 batches | lr 0.01 | ms/batch 241.35 | loss  2.81 | ppl    16.56\n",
            "| epoch  31 |    30/  141 batches | lr 0.01 | ms/batch 242.67 | loss  2.81 | ppl    16.67\n",
            "| epoch  31 |    40/  141 batches | lr 0.01 | ms/batch 242.83 | loss  2.81 | ppl    16.64\n",
            "| epoch  31 |    50/  141 batches | lr 0.01 | ms/batch 242.69 | loss  2.80 | ppl    16.49\n",
            "| epoch  31 |    60/  141 batches | lr 0.01 | ms/batch 243.34 | loss  2.83 | ppl    16.92\n",
            "| epoch  31 |    70/  141 batches | lr 0.01 | ms/batch 242.13 | loss  2.82 | ppl    16.73\n",
            "| epoch  31 |    80/  141 batches | lr 0.01 | ms/batch 242.71 | loss  2.80 | ppl    16.38\n",
            "| epoch  31 |    90/  141 batches | lr 0.01 | ms/batch 242.09 | loss  2.77 | ppl    15.98\n",
            "| epoch  31 |   100/  141 batches | lr 0.01 | ms/batch 243.26 | loss  2.76 | ppl    15.83\n",
            "| epoch  31 |   110/  141 batches | lr 0.01 | ms/batch 243.12 | loss  2.76 | ppl    15.81\n",
            "| epoch  31 |   120/  141 batches | lr 0.01 | ms/batch 243.02 | loss  2.78 | ppl    16.07\n",
            "| epoch  31 |   130/  141 batches | lr 0.01 | ms/batch 242.32 | loss  2.77 | ppl    16.03\n",
            "| epoch  31 |   140/  141 batches | lr 0.01 | ms/batch 241.70 | loss  2.81 | ppl    16.53\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  31 | time: 52.72s | valid loss  3.80 | valid ppl    44.87\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  32 |    10/  141 batches | lr 0.00 | ms/batch 265.17 | loss  3.09 | ppl    22.03\n",
            "| epoch  32 |    20/  141 batches | lr 0.00 | ms/batch 242.47 | loss  2.77 | ppl    15.97\n",
            "| epoch  32 |    30/  141 batches | lr 0.00 | ms/batch 242.19 | loss  2.77 | ppl    15.99\n",
            "| epoch  32 |    40/  141 batches | lr 0.00 | ms/batch 243.74 | loss  2.78 | ppl    16.07\n",
            "| epoch  32 |    50/  141 batches | lr 0.00 | ms/batch 242.84 | loss  2.77 | ppl    15.89\n",
            "| epoch  32 |    60/  141 batches | lr 0.00 | ms/batch 244.15 | loss  2.79 | ppl    16.32\n",
            "| epoch  32 |    70/  141 batches | lr 0.00 | ms/batch 241.81 | loss  2.76 | ppl    15.77\n",
            "| epoch  32 |    80/  141 batches | lr 0.00 | ms/batch 242.74 | loss  2.75 | ppl    15.69\n",
            "| epoch  32 |    90/  141 batches | lr 0.00 | ms/batch 242.85 | loss  2.74 | ppl    15.48\n",
            "| epoch  32 |   100/  141 batches | lr 0.00 | ms/batch 242.49 | loss  2.72 | ppl    15.15\n",
            "| epoch  32 |   110/  141 batches | lr 0.00 | ms/batch 243.44 | loss  2.72 | ppl    15.18\n",
            "| epoch  32 |   120/  141 batches | lr 0.00 | ms/batch 242.82 | loss  2.74 | ppl    15.43\n",
            "| epoch  32 |   130/  141 batches | lr 0.00 | ms/batch 242.25 | loss  2.73 | ppl    15.40\n",
            "| epoch  32 |   140/  141 batches | lr 0.00 | ms/batch 242.07 | loss  2.76 | ppl    15.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  32 | time: 52.73s | valid loss  3.81 | valid ppl    45.17\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  33 |    10/  141 batches | lr 0.00 | ms/batch 265.66 | loss  3.06 | ppl    21.24\n",
            "| epoch  33 |    20/  141 batches | lr 0.00 | ms/batch 242.46 | loss  2.74 | ppl    15.42\n",
            "| epoch  33 |    30/  141 batches | lr 0.00 | ms/batch 242.35 | loss  2.73 | ppl    15.35\n",
            "| epoch  33 |    40/  141 batches | lr 0.00 | ms/batch 242.81 | loss  2.73 | ppl    15.41\n",
            "| epoch  33 |    50/  141 batches | lr 0.00 | ms/batch 243.29 | loss  2.72 | ppl    15.17\n",
            "| epoch  33 |    60/  141 batches | lr 0.00 | ms/batch 241.81 | loss  2.75 | ppl    15.68\n",
            "| epoch  33 |    70/  141 batches | lr 0.00 | ms/batch 243.30 | loss  2.74 | ppl    15.43\n",
            "| epoch  33 |    80/  141 batches | lr 0.00 | ms/batch 242.15 | loss  2.71 | ppl    15.09\n",
            "| epoch  33 |    90/  141 batches | lr 0.00 | ms/batch 240.95 | loss  2.70 | ppl    14.81\n",
            "| epoch  33 |   100/  141 batches | lr 0.00 | ms/batch 243.00 | loss  2.68 | ppl    14.58\n",
            "| epoch  33 |   110/  141 batches | lr 0.00 | ms/batch 243.15 | loss  2.68 | ppl    14.63\n",
            "| epoch  33 |   120/  141 batches | lr 0.00 | ms/batch 241.67 | loss  2.70 | ppl    14.84\n",
            "| epoch  33 |   130/  141 batches | lr 0.00 | ms/batch 242.42 | loss  2.70 | ppl    14.83\n",
            "| epoch  33 |   140/  141 batches | lr 0.00 | ms/batch 241.89 | loss  2.73 | ppl    15.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  33 | time: 52.69s | valid loss  3.83 | valid ppl    45.99\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  34 |    10/  141 batches | lr 0.00 | ms/batch 266.69 | loss  3.01 | ppl    20.25\n",
            "| epoch  34 |    20/  141 batches | lr 0.00 | ms/batch 243.55 | loss  2.70 | ppl    14.85\n",
            "| epoch  34 |    30/  141 batches | lr 0.00 | ms/batch 242.99 | loss  2.70 | ppl    14.85\n",
            "| epoch  34 |    40/  141 batches | lr 0.00 | ms/batch 241.19 | loss  2.70 | ppl    14.92\n",
            "| epoch  34 |    50/  141 batches | lr 0.00 | ms/batch 241.62 | loss  2.69 | ppl    14.70\n",
            "| epoch  34 |    60/  141 batches | lr 0.00 | ms/batch 242.83 | loss  2.71 | ppl    15.10\n",
            "| epoch  34 |    70/  141 batches | lr 0.00 | ms/batch 240.20 | loss  2.70 | ppl    14.81\n",
            "| epoch  34 |    80/  141 batches | lr 0.00 | ms/batch 242.63 | loss  2.68 | ppl    14.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "Exiting from training early\n",
            "=========================================================================================\n",
            "| End of training | test loss  3.75 | test ppl    42.69\n",
            "=========================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tU8fnxYxMsWX"
      },
      "source": [
        "def pretty_print(words):\r\n",
        "  pretty_mat = []\r\n",
        "  i,last_eos = 0, 0\r\n",
        "  for j in range(len(words)-1,-1,-1):\r\n",
        "    if words[j][:4] == '<eos':\r\n",
        "      last_eos = j\r\n",
        "      break\r\n",
        "  while i<last_eos:\r\n",
        "    line = []\r\n",
        "    while (words[i][:4] != '<eos'):\r\n",
        "      line.append(words[i])\r\n",
        "      i +=1\r\n",
        "    line.append(words[i])\r\n",
        "    i +=1\r\n",
        "    pretty_mat.append(line[1:])\r\n",
        "  for line in pretty_mat :\r\n",
        "    print(' '.join(line))\r\n",
        "  print('\\n')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ikajkc9I3Sl2"
      },
      "source": [
        "def beam_search_decode(device, net, words, vocab_to_int, int_to_vocab, top_k, temperature):\n",
        "  net.eval()\n",
        "  softmax = nn.Softmax(dim = 1)\n",
        "  words = words.split(' ')\n",
        "  words.append('<sos>')\n",
        "  hidden = net.init_hidden(1)\n",
        "  for v in hidden:\n",
        "    v = v.to(device)\n",
        "  for w in words:\n",
        "    ix = torch.tensor([[vocab_to_int[w]]]).to(device)\n",
        "    output, hidden = net(ix, hidden)\n",
        "  output = output / temperature\n",
        "  prob, top_ix = torch.topk(softmax(output[0]), k=top_k)\n",
        "  prob = torch.log(prob)\n",
        "  #print(top_ix)\n",
        "  list_ids = [[id] for id in top_ix[0].tolist()]\n",
        "  outputs = [output for _ in range(top_k)]\n",
        "  hiddens = [hidden for _ in range(top_k)] \n",
        "  #print(\"avant beam search, top indices : \",top_ix.tolist(), \"de proba\", prob.tolist())\n",
        "  #print(list_ids)\n",
        "  for i in range(100):\n",
        "    probas = torch.zeros(top_k, top_k).float().to(device)\n",
        "    indxes = torch.zeros(top_k, top_k).to(device)\n",
        "    for k in range(top_k):\n",
        "      ix = torch.tensor([[top_ix[0][k]]]).to(device)\n",
        "      output, hiddens[k] = net(ix, hiddens[k])\n",
        "      output = output / temperature\n",
        "      pro, indxes[k] = torch.topk(softmax(output[0]), k=top_k)\n",
        "      pro = torch.log(pro)\n",
        "      #print(\"probas du choix \", k+1,\" : \", pro.tolist())\n",
        "      probas[k] = torch.add(pro[0], prob[0][k])\n",
        "    #print(indxes.tolist())\n",
        "    #print(list_ids)\n",
        "    prob, indices = torch.topk(probas.flatten(), top_k)\n",
        "    prob = torch.unsqueeze(prob, 0)\n",
        "    for k in range(top_k):\n",
        "      top_ix[0][k] = indxes.flatten()[indices[k]]\n",
        "    indices = indices // top_k\n",
        "    temp1 = []\n",
        "    temp2 = []\n",
        "    for k in range(top_k):\n",
        "      temp1.append(hiddens[indices.tolist()[k]])\n",
        "      temp2.append(list_ids[indices.tolist()[k]] + [top_ix[0].tolist()[k]])\n",
        "    hiddens = temp1\n",
        "    list_ids = temp2\n",
        "    #print(\"top indices : \",top_ix.tolist(), \"de proba\", prob.tolist())\n",
        "  best_branch = list_ids[torch.argmax(prob)]\n",
        "  words = []\n",
        "  for id in best_branch:\n",
        "    words.append(int_to_vocab[id])\n",
        "  pretty_print(words)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLCMetacxM6v"
      },
      "source": [
        "def predict(device, net, words, vocab_to_int, int_to_vocab, temperature):\n",
        "  net.eval()\n",
        "  softmax = nn.Softmax(dim=1)\n",
        "  words = words.split(' ')\n",
        "  hidden = net.init_hidden(1)\n",
        "  for v in hidden:\n",
        "    v = v.to(device)\n",
        "  for w in words:\n",
        "    ix = torch.tensor([[vocab_to_int[w]]]).to(device)\n",
        "    output, hidden = net(ix, hidden)\n",
        "  output = output / temperature\n",
        "  idx_max = torch.argmax(softmax(output[0]))\n",
        "  words = []\n",
        "  words.append(int_to_vocab[idx_max])\n",
        "  for i in range(0, 100):\n",
        "      ix = torch.tensor([[idx_max]]).to(device)\n",
        "      output, hidden = net(ix, hidden)\n",
        "      output = output / temperature\n",
        "      idx_max = torch.argmax(softmax(output[0]))\n",
        "      words.append(int_to_vocab[idx_max])\n",
        "  pretty_print(words)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s22J_29pRd5r"
      },
      "source": [
        "def top_k_sampling(device, net, words, vocab_to_int, int_to_vocab, top_k, temperature):\r\n",
        "  net.eval()\r\n",
        "  softmax = nn.Softmax(dim=-1)\r\n",
        "  words = words.split(' ')\r\n",
        "  hidden = net.init_hidden(1)\r\n",
        "  for v in hidden:\r\n",
        "    v = v.to(device)\r\n",
        "  for w in words:\r\n",
        "    ix = torch.tensor([[vocab_to_int[w]]]).to(device)\r\n",
        "    output, hidden = net(ix, hidden)\r\n",
        "  output = output / temperature\r\n",
        "  indices_to_remove = output[0] < torch.topk(output[0], top_k)[0][..., -1, None]\r\n",
        "  output[0][indices_to_remove] = -float('Inf')\r\n",
        "  prob = softmax(output[0])\r\n",
        "  idx_max = torch.multinomial(prob, 1)\r\n",
        "  words = []\r\n",
        "  words.append(int_to_vocab[idx_max])\r\n",
        "  for i in range(0, 100):\r\n",
        "      ix = torch.tensor([[idx_max]]).to(device)\r\n",
        "      output, hidden = net(ix, hidden)\r\n",
        "      output = output[0][0] / temperature\r\n",
        "      indices_to_remove = output < torch.topk(output, top_k)[0][..., -1, None]\r\n",
        "      output[indices_to_remove] = -float('Inf')\r\n",
        "      prob = softmax(output)\r\n",
        "      idx_max = torch.multinomial(prob, 1)\r\n",
        "      words.append(int_to_vocab[idx_max])\r\n",
        "  pretty_print(words)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9V_Q2YLG0qCB",
        "outputId": "98ecab82-9ebc-4956-c30d-5ea6753e5b26"
      },
      "source": [
        "words = '<sos> Je ne t’ en parle plus , va , sers la tyrannie , <eos3> <sos> Abandonne ton âme à son lâche génie ; <eos3>'\n",
        "predict(device, model, words, corpus.dictionary.word2idx, corpus.dictionary.idx2word,1)\n",
        "beam_search_decode(device, model, words, corpus.dictionary.word2idx, corpus.dictionary.idx2word,10,0.2)\n",
        "beam_search_decode(device, model, words, corpus.dictionary.word2idx, corpus.dictionary.idx2word,10,0.5)\n",
        "beam_search_decode(device, model, words, corpus.dictionary.word2idx, corpus.dictionary.idx2word,10,1)\n",
        "top_k_sampling(device, model, words, corpus.dictionary.word2idx, corpus.dictionary.idx2word, 10, 1)\n",
        "top_k_sampling(device, model, words, corpus.dictionary.word2idx, corpus.dictionary.idx2word, 50, 1)\n",
        "top_k_sampling(device, model, words, corpus.dictionary.word2idx, corpus.dictionary.idx2word, 50, 0.5)\n",
        "top_k_sampling(device, model, words, corpus.dictionary.word2idx, corpus.dictionary.idx2word, 250, 0.7)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mais si de ce grand rang le ciel est de l’ empire , <eos13>\n",
            "Et que de sa vertu les plaisirs de l’ empire , <eos13>\n",
            "Ne me donne point à vous , et ne puis rien de vous . <eos4>\n",
            "Je ne sais point d’ état , et ne m’ en peut défendre ; <eos16>\n",
            "Et je ne puis souffrir qu’ un si grand changement <eos9>\n",
            "Ne peut être à l’ amour de l’ amour de l’ amour . <eos26>\n",
            "\n",
            "\n",
            "si de ce grand rang le ciel m’ a fait naître , <eos18>\n",
            "Je ne sais qu’ un rival , et le Roi m’ a fait naître . <eos18>\n",
            "Et si l’ on m’ en veut croire , il faut qu’ il s’ en souvienne . <eos17>\n",
            "Et bien , pour le tirer , il est toujours aimable , <eos32>\n",
            "Et je ne puis souffrir qu’ un si grand changement <eos9>\n",
            "Ne peut être à l’ amour d’ un pouvoir absolu . <eos46>\n",
            "\n",
            "\n",
            "si de ce grand rang la fortune est suivie , <eos3>\n",
            "Je ne m’ en défends point , et mon cœur est à moi , <eos6>\n",
            "Et je ne puis souffrir qu’ un autre que je suis . <eos12>\n",
            "Je ne sais si je suis , et ce que je puis croire <eos33>\n",
            "À ce que je puis voir , et ce n’ est qu’ un faux crime , <eos38>\n",
            "Et je ne puis souffrir qu’ un si grand malheur <eos5>\n",
            "\n",
            "\n",
            "si vous m’ en croyez , je ne suis plus à moi , <eos6>\n",
            "Je ne veux qu’ un remède , et je n’ en suis plus à moi . <eos6>\n",
            "Je sais ce que je suis , et ce que j’ ai promis , <eos12>\n",
            "C’ est ce que je puis voir , et ce n’ est plus à moi . <eos6>\n",
            "Je sais ce que je suis , et ce n’ est qu’ à moi - même ; <eos19>\n",
            "Et je ne puis souffrir qu’ à ce que j’ en ordonne , <eos45>\n",
            "\n",
            "\n",
            "Car je n’ ai pas le don , et je ne suis la tête . <eos36>\n",
            "Si j’ ai fait pour son père à la fin d’ Aristie , <eos3>\n",
            "Par la mort de la mienne ont déjà la poursuite , <eos44>\n",
            "Ne me faites plus , rien , qu’ un autre est bien bien , <eos30>\n",
            "Mais le nom de votre cœur n’ est pas moins à vous . <eos4>\n",
            "Ah ! Que mon bonheur en est , le ciel , de sa puissance , <eos1>\n",
            "\n",
            "\n",
            "Mais si pour l’ autre offert , m’ a cru tout oublier . <eos61>\n",
            "Quand vous la naissance , il est à votre roi . <eos6>\n",
            "Si d’ amour j’ ai l’ amour , et le Fils de l’ amour , <eos26>\n",
            "C’ est la même main qu’ on lui pût souffrir la main . <eos28>\n",
            "Je ne sais point ici le plus grand des humains <eos49>\n",
            "Ne puisse à ce prix de vous mettre aux tyrans . <eos21>\n",
            "Et dans les sentiments de l’ amour , de nos feux , <eos2>\n",
            "\n",
            "\n",
            "J’ ai fait un autre choix que mon affection <eos22>\n",
            "Ne peut pour l’ acquérir qu’ un absolu amant . <eos9>\n",
            "Et l’ on fait , comme moi , l’ affaire de sa main , <eos28>\n",
            "Et le moindre éclat de sa main et le jour . <eos26>\n",
            "Et le vôtre , qui perd tout , et qu’ il ne veut qu’ un jour , <eos26>\n",
            "Je ne sais qu’ un rival , et de sa main sa main . <eos28>\n",
            "Et que vous voulez croire à ce cœur si fidèle , <eos14>\n",
            "\n",
            "\n",
            "Et puisque son amour ne se rend pas permis <eos12>\n",
            "Qu’ il ne donne à l’ amour d’ écouter mes vœux . <eos2>\n",
            "Et pour la liberté , j’ en suis toujours capable , <eos3>\n",
            "Et je ne puis souffrir qu’ à cette lâcheté <eos10>\n",
            "Qui ne s’ est permis d’ un tel amour d’ amour . <eos26>\n",
            "Et que notre amour , si le sort de sa vie <eos3>\n",
            "Ne doit , avec ses mains , s’ obstine à sa constance . <eos1>\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "it6lUAwIXOcO"
      },
      "source": [
        "Je veux , et l’ en a vu . . Mais il faut à l’ entendre : . . ; <eos> \n",
        "\n",
        "Je vous dois voir à vous . Et que j’ ai su l’ attendre <eos> \n",
        "\n",
        "Ce que j’ avais pu voir , et l’ autre à l’ honneur ; <eos> \n",
        "\n",
        "Il n’ a rien dit qu’ en ce mot je n’ en dois point de rien\n",
        "\n",
        "**Ajout du SoS**\n",
        "\n",
        ". . . Ah ! Seigneur , c’ est moi - même . <eos> \n",
        "\n",
        "<sos> Ah ! Madame , il est vrai , je n’ en veux point douter , <eos>\n",
        "\n",
        "<sos> Et je n’ ai pas besoin de m’ en faire haïr ."
      ]
    }
  ]
}