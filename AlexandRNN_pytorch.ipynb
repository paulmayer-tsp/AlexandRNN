{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "AlexandRNN_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5LqrlD34g3m"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1ACLXSw4g3s"
      },
      "source": [
        "### Tools for data processing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6D83zYN44g3s"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import math\n",
        "from collections import Counter\n",
        "import pprint\n",
        "pp = pprint.PrettyPrinter(indent=1)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urQQ5WEI4g3s"
      },
      "source": [
        "class Dictionary(object):\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = []\n",
        "        self.counter = {}\n",
        "        self.total = 0\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.idx2word.append(word)\n",
        "            self.word2idx[word] = len(self.idx2word) - 1\n",
        "            self.counter.setdefault(word, 0)\n",
        "        self.counter[word] += 1\n",
        "        self.total += 1\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RG6rsEQB4g3t"
      },
      "source": [
        "class Corpus(object):\n",
        "    def __init__(self, path):\n",
        "        # We create an object Dictionary associated to Corpus\n",
        "        self.dictionary = Dictionary()\n",
        "        # We go through all files, adding all words to the dictionary\n",
        "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
        "        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n",
        "        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n",
        "        \n",
        "    def tokenize(self, path):\n",
        "        \"\"\"Tokenizes a text file, knowing the dictionary, in order to tranform it into a list of indexes\"\"\"\n",
        "        assert os.path.exists(path)\n",
        "        # Add words to the dictionary\n",
        "        with open(path, 'r') as f:\n",
        "            tokens, incr = 0, 0\n",
        "            for line in f:\n",
        "                words = ['<sos>'] + line.split() + ['<eos1>']*(incr % 2 == 0) + ['<eos2>']*(incr % 2 == 1)\n",
        "                incr +=1\n",
        "                for word in words:\n",
        "                    self.dictionary.add_word(word)\n",
        "                tokens += len(words)\n",
        "        \n",
        "        # Once done, go through the file a second time and fill a Torch Tensor with the associated indexes \n",
        "        with open(path, 'r') as f:\n",
        "            ids = torch.LongTensor(tokens)\n",
        "            token, incr = 0, 0\n",
        "            for line in f:\n",
        "                words = ['<sos>'] + line.split() + ['<eos1>']*(incr % 2 == 0) + ['<eos2>']*(incr % 2 == 1)\n",
        "                incr +=1\n",
        "                for word in words:\n",
        "                    ids[token] = self.dictionary.word2idx[word]\n",
        "                    token += 1\n",
        "        return ids"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytRgixof4g3t"
      },
      "source": [
        "###############################################################################\n",
        "# Load data\n",
        "###############################################################################\n",
        "\n",
        "data = './corpus/'\n",
        "corpus = Corpus(data)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHf1bA7C4g3t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b533d29b-73fc-4461-e216-23f717a8109b"
      },
      "source": [
        "print(corpus.dictionary.total)\n",
        "print(len(corpus.dictionary.idx2word))\n",
        "print(len(corpus.dictionary.word2idx))\n",
        "\n",
        "print(corpus.train.shape)\n",
        "print(corpus.train[0:7])\n",
        "print([corpus.dictionary.idx2word[corpus.train[i]] for i in range(40)])\n",
        "\n",
        "print(corpus.valid.shape)\n",
        "print(corpus.valid[0:7])\n",
        "print([corpus.dictionary.idx2word[corpus.valid[i]] for i in range(7)])"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020173\n",
            "29669\n",
            "29669\n",
            "torch.Size([1518729])\n",
            "tensor([0, 1, 2, 3, 4, 5, 6])\n",
            "['<sos>', 'Impatients', 'désirs', 'd’', 'une', 'illustre', 'vengeance', '<eos1>', '<sos>', 'Dont', 'la', 'mort', 'de', 'mon', 'père', 'a', 'formé', 'la', 'naissance', ',', '<eos2>', '<sos>', 'Enfants', 'impétueux', 'de', 'mon', 'ressentiment', '<eos1>', '<sos>', 'Que', 'ma', 'douleur', 'séduite', 'embrasse', 'aveuglément', ',', '<eos2>', '<sos>', 'Vous', 'régnez']\n",
            "torch.Size([366404])\n",
            "tensor([   0,   44,  145,  146,   64,  731, 1053])\n",
            "['<sos>', 'Et', 'c’', 'est', 'à', 'faire', 'enfin']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIWjBwFj4g3t"
      },
      "source": [
        "# We now have data under a very long list of indexes: the text is as one sequence.\n",
        "# The idea now is to create batches from this. Note that this is absolutely not the best\n",
        "# way to proceed with large quantities of data (where we'll try not to store huge tensors\n",
        "# in memory but read them from file as we go) !\n",
        "# Here, we are looking for simplicity and efficiency with regards to computation time.\n",
        "# That is why we will ignore sentence separations and treat the data as one long stream that\n",
        "# we will cut arbitrarily as we need.\n",
        "# With the alphabet being our data, we currently have the sequence:\n",
        "# [a b c d e f g h i j k l m n o p q r s t u v w x y z]\n",
        "# We want to reorganize it as independant batches that will be processed independantly by the model !\n",
        "# For instance, with the alphabet as the sequence and batch size 4, we'd get the 4 following sequences:\n",
        "# ┌ a g m s ┐\n",
        "# │ b h n t │\n",
        "# │ c i o u │\n",
        "# │ d j p v │\n",
        "# │ e k q w │\n",
        "# └ f l r x ┘\n",
        "# with the last two elements being lost.\n",
        "# Again, these columns are treated as independent by the model, which means that the\n",
        "# dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient processing.\n",
        "\n",
        "def batchify(data, batch_size, cuda = False):\n",
        "    # Cut the elements that are unnecessary\n",
        "    nbatch = data.size(0) // batch_size\n",
        "    data = data.narrow(0, 0, nbatch * batch_size)\n",
        "    # Reorganize the data\n",
        "    data = data.view(batch_size, -1).t().contiguous()\n",
        "    # If we can use a GPU, let's tranfer the tensor to it\n",
        "    return data.to(device)\n",
        "\n",
        "# get_batch subdivides the source data into chunks of the appropriate length.\n",
        "# If source is equal to the example output of the batchify function, with\n",
        "# a sequence length (seq_len) of 3, we'd get the following two variables:\n",
        "# ┌ a g m s ┐ ┌ b h n t ┐\n",
        "# | b h n t | | c i o u │\n",
        "# └ c i o u ┘ └ d j p v ┘\n",
        "# The first variable contains the letters input to the network, while the second\n",
        "# contains the one we want the network to predict (b for a, h for g, v for u, etc..)\n",
        "# Note that despite the name of the function, we are cutting the data in the\n",
        "# temporal dimension, since we already divided data into batches in the previous\n",
        "# function. \n",
        "\n",
        "def get_batch(source, i, seq_len, evaluation=False):\n",
        "    # Deal with the possibility that there's not enough data left for a full sequence\n",
        "    seq_len = min(seq_len, len(source) - 1 - i)\n",
        "    # Take the input data\n",
        "    data = source[i:i+seq_len]\n",
        "    # Shift by one for the target data\n",
        "    target = source[i+1:i+1+seq_len]\n",
        "    return data, target"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMeQ0gjzUOLd"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwfqKiMJ4g3t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f502249c-94d7-4005-9f33-04e4b82eec68"
      },
      "source": [
        "batch_size = 100\n",
        "eval_batch_size = 4\n",
        "train_data = batchify(corpus.train, batch_size)\n",
        "val_data = batchify(corpus.valid, eval_batch_size)\n",
        "test_data = batchify(corpus.test, eval_batch_size)\n",
        "\n",
        "print(train_data.shape)\n",
        "print(val_data.shape)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([15187, 100])\n",
            "torch.Size([91601, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9OVIoVJ4g3u"
      },
      "source": [
        "### LSTM Cells in pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztKrssh84g3u"
      },
      "source": [
        "### Creating our own LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zD0K9CQ14g3u"
      },
      "source": [
        "# Models are usually implemented as custom nn.Module subclass\n",
        "# We need to redefine the __init__ method, which creates the object\n",
        "# We also need to redefine the forward method, which transform the input into outputs\n",
        "# We can also add any method that we need: here, in order to initiate weights in the model\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        # Create a dropout object to use on layers for regularization\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        # Create an encoder - which is an embedding layer\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        # Create the LSTM layers - find out how to stack them !\n",
        "        self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
        "        # Create what we call the decoder: a linear transformation to map the hidden state into scores for all words in the vocabulary\n",
        "        # (Note that the softmax application function will be applied out of the model)\n",
        "        self.decoder = nn.Linear(nhid, ntoken)\n",
        "        \n",
        "        # Initialize non-reccurent weights \n",
        "        self.init_weights()\n",
        "\n",
        "        self.ninp = ninp\n",
        "        self.nhid = nhid\n",
        "        self.nlayers = nlayers\n",
        "        \n",
        "    def init_weights(self):\n",
        "        # Initialize the encoder and decoder weights with the uniform distribution\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.fill_(0)\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "        \n",
        "    def init_hidden(self, batch_size):\n",
        "        # Initialize the hidden state and cell state to zero, with the right sizes\n",
        "        weight = next(self.parameters())\n",
        "        return (weight.new_zeros(self.nlayers, batch_size, self.nhid),\n",
        "                weight.new_zeros(self.nlayers, batch_size, self.nhid))    \n",
        "\n",
        "    def forward(self, input, hidden, return_h=False):\n",
        "        # Process the input\n",
        "        emb = self.drop(self.encoder(input))   \n",
        "        \n",
        "        # Apply the LSTMs\n",
        "        output, hidden = self.rnn(emb, hidden)\n",
        "        \n",
        "        # Decode into scores\n",
        "        output = self.drop(output)      \n",
        "        decoded = self.decoder(output)\n",
        "        return decoded, hidden"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucXMYksR4g3u"
      },
      "source": [
        "### Building the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HL-vvLGm4g3u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ea3b8fd-5c14-4abd-c3f7-7eb84d34e786"
      },
      "source": [
        "# Set the random seed manually for reproducibility.\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f0e45faeb10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S147j0Dx4g3u"
      },
      "source": [
        "embedding_size = 500\n",
        "hidden_size = 500\n",
        "layers = 2\n",
        "dropout = 0.3\n",
        "\n",
        "###############################################################################\n",
        "# Build the model\n",
        "###############################################################################\n",
        "\n",
        "vocab_size = len(corpus.dictionary)\n",
        "model = LSTMModel(vocab_size, embedding_size, hidden_size, layers, dropout).to(device)\n",
        "params = list(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RU2X8VHG4g3v"
      },
      "source": [
        "lr = 10.0\n",
        "optimizer = 'sgd'\n",
        "wdecay = 1.2e-6\n",
        "# For gradient clipping\n",
        "clip = 0.25\n",
        "\n",
        "if optimizer == 'sgd':\n",
        "    optim = torch.optim.SGD(params, lr=lr, weight_decay=wdecay)\n",
        "if optimizer == 'adam':\n",
        "    optim = torch.optim.Adam(params, lr=lr, weight_decay=wdecay)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fGlrwME4g3v"
      },
      "source": [
        "# Let's think about gradient propagation:\n",
        "# We plan to keep the second ouput of the LSTM layer (the hidden/cell states) to initialize\n",
        "# the next call to LSTM. In this way, we can back-propagate the gradient for as long as we want.\n",
        "# However, this put a huge strain on the memory used by the model, since it implies retaining\n",
        "# a always-growing number of tensors of gradients in the cache.\n",
        "# We decide to not backpropagate through time beyond the current sequence ! \n",
        "# We use a specific function to cut the 'hidden/state cell' states from their previous dependencies\n",
        "# before using them to initialize the next call to the LSTM.\n",
        "# This is done with the .detach() function.\n",
        "\n",
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4CGuudp4g3v"
      },
      "source": [
        "# Other global parameters\n",
        "epochs = 50\n",
        "seq_len = 30\n",
        "log_interval = 10\n",
        "save = 'model.pt'"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xx6UKwTO4g3v"
      },
      "source": [
        "def evaluate(data_source):\n",
        "    # Turn on evaluation mode which disables dropout.\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    hidden = model.init_hidden(eval_batch_size)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, seq_len):\n",
        "            data, targets = get_batch(data_source, i, seq_len)\n",
        "            output, hidden = model(data, hidden)\n",
        "            hidden = repackage_hidden(hidden)\n",
        "            total_loss += len(data) * criterion(output.view(-1, vocab_size), targets.view(-1)).item()\n",
        "    return total_loss / (len(data_source) - 1)"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EhuQglX4g3v"
      },
      "source": [
        "def train():\n",
        "    # Turn on training mode which enables dropout.\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    hidden = model.init_hidden(batch_size)\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, seq_len)):\n",
        "        data, targets = get_batch(train_data, i, seq_len)\n",
        "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "        hidden = repackage_hidden(hidden)\n",
        "        optim.zero_grad()\n",
        "        \n",
        "        output, hidden = model(data, hidden)\n",
        "        loss = criterion(output.view(-1, vocab_size), targets.view(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        torch.nn.utils.clip_grad_norm_(params, clip)\n",
        "        optim.step()\n",
        "        \n",
        "        total_loss += loss.data\n",
        "\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
        "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                epoch, batch, len(train_data) // seq_len, lr,\n",
        "                elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bN0gBRgf4g3v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1f1eed3-c102-4c21-8bc7-f06542861b0f"
      },
      "source": [
        "# Loop over epochs.\n",
        "best_val_loss = None\n",
        "\n",
        "# At any point you can hit Ctrl + C to break out of training early.\n",
        "try:\n",
        "    for epoch in range(1, epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        train()\n",
        "        val_loss = evaluate(val_data)\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                           val_loss, math.exp(val_loss)))\n",
        "        print('-' * 89)\n",
        "        # Save the model if the validation loss is the best we've seen so far.\n",
        "        if not best_val_loss or val_loss < best_val_loss:\n",
        "            with open(save, 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_val_loss = val_loss\n",
        "        else:\n",
        "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "            lr /= 4.0\n",
        "except KeyboardInterrupt:\n",
        "    print('-' * 89)\n",
        "    print('Exiting from training early')\n",
        "\n",
        "# Load the best saved model.\n",
        "with open(save, 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "    # after load the rnn params are not a continuous chunk of memory\n",
        "    # this makes them a continuous chunk, and will speed up forward pass\n",
        "    model.rnn.flatten_parameters()\n",
        "\n",
        "# Run on test data.\n",
        "test_loss = evaluate(test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, math.exp(test_loss)))\n",
        "print('=' * 89)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| epoch   1 |    10/  506 batches | lr 10.00 | ms/batch 136.57 | loss 10.65 | ppl 42175.68\n",
            "| epoch   1 |    20/  506 batches | lr 10.00 | ms/batch 118.57 | loss  8.33 | ppl  4159.53\n",
            "| epoch   1 |    30/  506 batches | lr 10.00 | ms/batch 119.70 | loss  7.83 | ppl  2518.34\n",
            "| epoch   1 |    40/  506 batches | lr 10.00 | ms/batch 118.45 | loss  7.39 | ppl  1625.37\n",
            "| epoch   1 |    50/  506 batches | lr 10.00 | ms/batch 118.47 | loss  7.29 | ppl  1471.76\n",
            "| epoch   1 |    60/  506 batches | lr 10.00 | ms/batch 117.92 | loss  7.16 | ppl  1284.64\n",
            "| epoch   1 |    70/  506 batches | lr 10.00 | ms/batch 118.93 | loss  7.06 | ppl  1167.76\n",
            "| epoch   1 |    80/  506 batches | lr 10.00 | ms/batch 119.33 | loss  6.99 | ppl  1083.31\n",
            "| epoch   1 |    90/  506 batches | lr 10.00 | ms/batch 119.73 | loss  6.96 | ppl  1055.49\n",
            "| epoch   1 |   100/  506 batches | lr 10.00 | ms/batch 119.64 | loss  6.80 | ppl   901.43\n",
            "| epoch   1 |   110/  506 batches | lr 10.00 | ms/batch 119.76 | loss  6.71 | ppl   817.03\n",
            "| epoch   1 |   120/  506 batches | lr 10.00 | ms/batch 120.86 | loss  6.77 | ppl   872.94\n",
            "| epoch   1 |   130/  506 batches | lr 10.00 | ms/batch 119.74 | loss  6.78 | ppl   878.61\n",
            "| epoch   1 |   140/  506 batches | lr 10.00 | ms/batch 121.36 | loss  6.59 | ppl   727.13\n",
            "| epoch   1 |   150/  506 batches | lr 10.00 | ms/batch 121.36 | loss  6.44 | ppl   627.47\n",
            "| epoch   1 |   160/  506 batches | lr 10.00 | ms/batch 119.61 | loss  6.26 | ppl   523.34\n",
            "| epoch   1 |   170/  506 batches | lr 10.00 | ms/batch 121.33 | loss  6.10 | ppl   444.09\n",
            "| epoch   1 |   180/  506 batches | lr 10.00 | ms/batch 121.33 | loss  6.00 | ppl   402.94\n",
            "| epoch   1 |   190/  506 batches | lr 10.00 | ms/batch 121.31 | loss  5.92 | ppl   372.09\n",
            "| epoch   1 |   200/  506 batches | lr 10.00 | ms/batch 121.43 | loss  5.86 | ppl   351.78\n",
            "| epoch   1 |   210/  506 batches | lr 10.00 | ms/batch 121.34 | loss  5.77 | ppl   320.95\n",
            "| epoch   1 |   220/  506 batches | lr 10.00 | ms/batch 121.52 | loss  5.71 | ppl   301.49\n",
            "| epoch   1 |   230/  506 batches | lr 10.00 | ms/batch 121.24 | loss  5.71 | ppl   302.15\n",
            "| epoch   1 |   240/  506 batches | lr 10.00 | ms/batch 121.22 | loss  5.56 | ppl   258.68\n",
            "| epoch   1 |   250/  506 batches | lr 10.00 | ms/batch 121.41 | loss  5.56 | ppl   258.95\n",
            "| epoch   1 |   260/  506 batches | lr 10.00 | ms/batch 121.57 | loss  5.53 | ppl   251.59\n",
            "| epoch   1 |   270/  506 batches | lr 10.00 | ms/batch 122.93 | loss  5.49 | ppl   242.68\n",
            "| epoch   1 |   280/  506 batches | lr 10.00 | ms/batch 122.01 | loss  5.48 | ppl   239.67\n",
            "| epoch   1 |   290/  506 batches | lr 10.00 | ms/batch 122.97 | loss  5.45 | ppl   232.18\n",
            "| epoch   1 |   300/  506 batches | lr 10.00 | ms/batch 123.13 | loss  5.40 | ppl   221.54\n",
            "| epoch   1 |   310/  506 batches | lr 10.00 | ms/batch 123.26 | loss  5.41 | ppl   223.18\n",
            "| epoch   1 |   320/  506 batches | lr 10.00 | ms/batch 123.61 | loss  5.40 | ppl   220.35\n",
            "| epoch   1 |   330/  506 batches | lr 10.00 | ms/batch 124.20 | loss  5.33 | ppl   205.62\n",
            "| epoch   1 |   340/  506 batches | lr 10.00 | ms/batch 124.31 | loss  5.31 | ppl   203.11\n",
            "| epoch   1 |   350/  506 batches | lr 10.00 | ms/batch 124.24 | loss  5.25 | ppl   190.69\n",
            "| epoch   1 |   360/  506 batches | lr 10.00 | ms/batch 125.05 | loss  5.27 | ppl   193.89\n",
            "| epoch   1 |   370/  506 batches | lr 10.00 | ms/batch 125.53 | loss  5.27 | ppl   193.97\n",
            "| epoch   1 |   380/  506 batches | lr 10.00 | ms/batch 125.61 | loss  5.20 | ppl   182.13\n",
            "| epoch   1 |   390/  506 batches | lr 10.00 | ms/batch 126.32 | loss  5.21 | ppl   182.92\n",
            "| epoch   1 |   400/  506 batches | lr 10.00 | ms/batch 125.96 | loss  5.16 | ppl   174.42\n",
            "| epoch   1 |   410/  506 batches | lr 10.00 | ms/batch 127.63 | loss  5.15 | ppl   171.76\n",
            "| epoch   1 |   420/  506 batches | lr 10.00 | ms/batch 127.14 | loss  5.08 | ppl   160.09\n",
            "| epoch   1 |   430/  506 batches | lr 10.00 | ms/batch 128.45 | loss  5.05 | ppl   156.25\n",
            "| epoch   1 |   440/  506 batches | lr 10.00 | ms/batch 128.28 | loss  5.10 | ppl   163.68\n",
            "| epoch   1 |   450/  506 batches | lr 10.00 | ms/batch 128.15 | loss  5.06 | ppl   157.58\n",
            "| epoch   1 |   460/  506 batches | lr 10.00 | ms/batch 128.03 | loss  5.05 | ppl   156.56\n",
            "| epoch   1 |   470/  506 batches | lr 10.00 | ms/batch 128.86 | loss  5.00 | ppl   148.41\n",
            "| epoch   1 |   480/  506 batches | lr 10.00 | ms/batch 128.94 | loss  5.00 | ppl   147.89\n",
            "| epoch   1 |   490/  506 batches | lr 10.00 | ms/batch 130.18 | loss  4.97 | ppl   144.05\n",
            "| epoch   1 |   500/  506 batches | lr 10.00 | ms/batch 129.73 | loss  4.95 | ppl   140.69\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 74.48s | valid loss  4.87 | valid ppl   130.34\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |    10/  506 batches | lr 10.00 | ms/batch 139.97 | loss  5.47 | ppl   236.57\n",
            "| epoch   2 |    20/  506 batches | lr 10.00 | ms/batch 128.47 | loss  4.90 | ppl   133.68\n",
            "| epoch   2 |    30/  506 batches | lr 10.00 | ms/batch 128.30 | loss  4.90 | ppl   134.16\n",
            "| epoch   2 |    40/  506 batches | lr 10.00 | ms/batch 127.54 | loss  4.88 | ppl   131.09\n",
            "| epoch   2 |    50/  506 batches | lr 10.00 | ms/batch 127.35 | loss  4.87 | ppl   130.96\n",
            "| epoch   2 |    60/  506 batches | lr 10.00 | ms/batch 125.97 | loss  4.84 | ppl   126.65\n",
            "| epoch   2 |    70/  506 batches | lr 10.00 | ms/batch 126.55 | loss  4.84 | ppl   126.44\n",
            "| epoch   2 |    80/  506 batches | lr 10.00 | ms/batch 126.71 | loss  4.82 | ppl   123.69\n",
            "| epoch   2 |    90/  506 batches | lr 10.00 | ms/batch 124.84 | loss  4.84 | ppl   126.43\n",
            "| epoch   2 |   100/  506 batches | lr 10.00 | ms/batch 126.44 | loss  4.81 | ppl   122.91\n",
            "| epoch   2 |   110/  506 batches | lr 10.00 | ms/batch 126.13 | loss  4.74 | ppl   114.99\n",
            "| epoch   2 |   120/  506 batches | lr 10.00 | ms/batch 125.40 | loss  4.78 | ppl   119.42\n",
            "| epoch   2 |   130/  506 batches | lr 10.00 | ms/batch 125.26 | loss  4.76 | ppl   116.78\n",
            "| epoch   2 |   140/  506 batches | lr 10.00 | ms/batch 124.31 | loss  4.75 | ppl   115.64\n",
            "| epoch   2 |   150/  506 batches | lr 10.00 | ms/batch 124.50 | loss  4.73 | ppl   112.89\n",
            "| epoch   2 |   160/  506 batches | lr 10.00 | ms/batch 125.01 | loss  4.72 | ppl   112.21\n",
            "| epoch   2 |   170/  506 batches | lr 10.00 | ms/batch 124.05 | loss  4.70 | ppl   110.21\n",
            "| epoch   2 |   180/  506 batches | lr 10.00 | ms/batch 124.62 | loss  4.70 | ppl   109.54\n",
            "| epoch   2 |   190/  506 batches | lr 10.00 | ms/batch 124.76 | loss  4.72 | ppl   111.70\n",
            "| epoch   2 |   200/  506 batches | lr 10.00 | ms/batch 124.47 | loss  4.67 | ppl   107.02\n",
            "| epoch   2 |   210/  506 batches | lr 10.00 | ms/batch 123.87 | loss  4.67 | ppl   107.03\n",
            "| epoch   2 |   220/  506 batches | lr 10.00 | ms/batch 124.30 | loss  4.63 | ppl   102.74\n",
            "| epoch   2 |   230/  506 batches | lr 10.00 | ms/batch 124.53 | loss  4.64 | ppl   103.32\n",
            "| epoch   2 |   240/  506 batches | lr 10.00 | ms/batch 124.18 | loss  4.60 | ppl    99.93\n",
            "| epoch   2 |   250/  506 batches | lr 10.00 | ms/batch 124.36 | loss  4.59 | ppl    98.67\n",
            "| epoch   2 |   260/  506 batches | lr 10.00 | ms/batch 124.07 | loss  4.58 | ppl    97.45\n",
            "| epoch   2 |   270/  506 batches | lr 10.00 | ms/batch 124.31 | loss  4.57 | ppl    96.73\n",
            "| epoch   2 |   280/  506 batches | lr 10.00 | ms/batch 124.96 | loss  4.60 | ppl    99.53\n",
            "| epoch   2 |   290/  506 batches | lr 10.00 | ms/batch 124.47 | loss  4.58 | ppl    97.38\n",
            "| epoch   2 |   300/  506 batches | lr 10.00 | ms/batch 124.83 | loss  4.59 | ppl    98.15\n",
            "| epoch   2 |   310/  506 batches | lr 10.00 | ms/batch 125.42 | loss  4.58 | ppl    97.28\n",
            "| epoch   2 |   320/  506 batches | lr 10.00 | ms/batch 125.35 | loss  4.59 | ppl    98.34\n",
            "| epoch   2 |   330/  506 batches | lr 10.00 | ms/batch 125.55 | loss  4.54 | ppl    93.56\n",
            "| epoch   2 |   340/  506 batches | lr 10.00 | ms/batch 126.30 | loss  4.55 | ppl    94.86\n",
            "| epoch   2 |   350/  506 batches | lr 10.00 | ms/batch 125.50 | loss  4.52 | ppl    91.79\n",
            "| epoch   2 |   360/  506 batches | lr 10.00 | ms/batch 126.42 | loss  4.53 | ppl    92.54\n",
            "| epoch   2 |   370/  506 batches | lr 10.00 | ms/batch 126.55 | loss  4.52 | ppl    91.59\n",
            "| epoch   2 |   380/  506 batches | lr 10.00 | ms/batch 126.55 | loss  4.51 | ppl    90.84\n",
            "| epoch   2 |   390/  506 batches | lr 10.00 | ms/batch 127.06 | loss  4.49 | ppl    89.18\n",
            "| epoch   2 |   400/  506 batches | lr 10.00 | ms/batch 127.24 | loss  4.47 | ppl    87.12\n",
            "| epoch   2 |   410/  506 batches | lr 10.00 | ms/batch 126.95 | loss  4.47 | ppl    87.66\n",
            "| epoch   2 |   420/  506 batches | lr 10.00 | ms/batch 127.30 | loss  4.44 | ppl    84.78\n",
            "| epoch   2 |   430/  506 batches | lr 10.00 | ms/batch 126.98 | loss  4.38 | ppl    80.10\n",
            "| epoch   2 |   440/  506 batches | lr 10.00 | ms/batch 127.97 | loss  4.45 | ppl    85.39\n",
            "| epoch   2 |   450/  506 batches | lr 10.00 | ms/batch 127.77 | loss  4.45 | ppl    85.67\n",
            "| epoch   2 |   460/  506 batches | lr 10.00 | ms/batch 127.37 | loss  4.41 | ppl    82.67\n",
            "| epoch   2 |   470/  506 batches | lr 10.00 | ms/batch 127.34 | loss  4.41 | ppl    82.63\n",
            "| epoch   2 |   480/  506 batches | lr 10.00 | ms/batch 127.52 | loss  4.42 | ppl    82.92\n",
            "| epoch   2 |   490/  506 batches | lr 10.00 | ms/batch 127.41 | loss  4.41 | ppl    82.12\n",
            "| epoch   2 |   500/  506 batches | lr 10.00 | ms/batch 128.12 | loss  4.37 | ppl    79.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 75.75s | valid loss  4.35 | valid ppl    77.29\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |    10/  506 batches | lr 10.00 | ms/batch 138.18 | loss  4.85 | ppl   128.04\n",
            "| epoch   3 |    20/  506 batches | lr 10.00 | ms/batch 126.57 | loss  4.35 | ppl    77.68\n",
            "| epoch   3 |    30/  506 batches | lr 10.00 | ms/batch 126.56 | loss  4.34 | ppl    76.68\n",
            "| epoch   3 |    40/  506 batches | lr 10.00 | ms/batch 126.71 | loss  4.36 | ppl    78.48\n",
            "| epoch   3 |    50/  506 batches | lr 10.00 | ms/batch 126.00 | loss  4.36 | ppl    78.57\n",
            "| epoch   3 |    60/  506 batches | lr 10.00 | ms/batch 126.90 | loss  4.35 | ppl    77.71\n",
            "| epoch   3 |    70/  506 batches | lr 10.00 | ms/batch 126.88 | loss  4.33 | ppl    75.69\n",
            "| epoch   3 |    80/  506 batches | lr 10.00 | ms/batch 126.10 | loss  4.33 | ppl    76.19\n",
            "| epoch   3 |    90/  506 batches | lr 10.00 | ms/batch 126.21 | loss  4.36 | ppl    78.46\n",
            "| epoch   3 |   100/  506 batches | lr 10.00 | ms/batch 125.88 | loss  4.33 | ppl    75.75\n",
            "| epoch   3 |   110/  506 batches | lr 10.00 | ms/batch 125.77 | loss  4.28 | ppl    71.89\n",
            "| epoch   3 |   120/  506 batches | lr 10.00 | ms/batch 125.49 | loss  4.28 | ppl    71.88\n",
            "| epoch   3 |   130/  506 batches | lr 10.00 | ms/batch 126.23 | loss  4.31 | ppl    74.44\n",
            "| epoch   3 |   140/  506 batches | lr 10.00 | ms/batch 125.60 | loss  4.31 | ppl    74.20\n",
            "| epoch   3 |   150/  506 batches | lr 10.00 | ms/batch 125.64 | loss  4.29 | ppl    73.04\n",
            "| epoch   3 |   160/  506 batches | lr 10.00 | ms/batch 125.40 | loss  4.32 | ppl    75.52\n",
            "| epoch   3 |   170/  506 batches | lr 10.00 | ms/batch 125.09 | loss  4.27 | ppl    71.67\n",
            "| epoch   3 |   180/  506 batches | lr 10.00 | ms/batch 125.40 | loss  4.30 | ppl    73.98\n",
            "| epoch   3 |   190/  506 batches | lr 10.00 | ms/batch 125.38 | loss  4.31 | ppl    74.73\n",
            "| epoch   3 |   200/  506 batches | lr 10.00 | ms/batch 125.57 | loss  4.25 | ppl    70.40\n",
            "| epoch   3 |   210/  506 batches | lr 10.00 | ms/batch 125.37 | loss  4.26 | ppl    70.55\n",
            "| epoch   3 |   220/  506 batches | lr 10.00 | ms/batch 125.59 | loss  4.24 | ppl    69.54\n",
            "| epoch   3 |   230/  506 batches | lr 10.00 | ms/batch 125.63 | loss  4.25 | ppl    70.22\n",
            "| epoch   3 |   240/  506 batches | lr 10.00 | ms/batch 125.82 | loss  4.22 | ppl    68.19\n",
            "| epoch   3 |   250/  506 batches | lr 10.00 | ms/batch 126.46 | loss  4.21 | ppl    67.31\n",
            "| epoch   3 |   260/  506 batches | lr 10.00 | ms/batch 126.29 | loss  4.22 | ppl    67.73\n",
            "| epoch   3 |   270/  506 batches | lr 10.00 | ms/batch 126.46 | loss  4.23 | ppl    68.75\n",
            "| epoch   3 |   280/  506 batches | lr 10.00 | ms/batch 126.48 | loss  4.22 | ppl    68.25\n",
            "| epoch   3 |   290/  506 batches | lr 10.00 | ms/batch 126.24 | loss  4.20 | ppl    66.47\n",
            "| epoch   3 |   300/  506 batches | lr 10.00 | ms/batch 125.89 | loss  4.21 | ppl    67.40\n",
            "| epoch   3 |   310/  506 batches | lr 10.00 | ms/batch 126.52 | loss  4.18 | ppl    65.16\n",
            "| epoch   3 |   320/  506 batches | lr 10.00 | ms/batch 126.55 | loss  4.21 | ppl    67.58\n",
            "| epoch   3 |   330/  506 batches | lr 10.00 | ms/batch 126.89 | loss  4.17 | ppl    64.99\n",
            "| epoch   3 |   340/  506 batches | lr 10.00 | ms/batch 126.87 | loss  4.17 | ppl    64.99\n",
            "| epoch   3 |   350/  506 batches | lr 10.00 | ms/batch 126.52 | loss  4.16 | ppl    63.76\n",
            "| epoch   3 |   360/  506 batches | lr 10.00 | ms/batch 127.22 | loss  4.16 | ppl    64.30\n",
            "| epoch   3 |   370/  506 batches | lr 10.00 | ms/batch 126.77 | loss  4.16 | ppl    64.16\n",
            "| epoch   3 |   380/  506 batches | lr 10.00 | ms/batch 126.77 | loss  4.15 | ppl    63.63\n",
            "| epoch   3 |   390/  506 batches | lr 10.00 | ms/batch 126.57 | loss  4.15 | ppl    63.31\n",
            "| epoch   3 |   400/  506 batches | lr 10.00 | ms/batch 126.61 | loss  4.12 | ppl    61.77\n",
            "| epoch   3 |   410/  506 batches | lr 10.00 | ms/batch 126.75 | loss  4.11 | ppl    60.98\n",
            "| epoch   3 |   420/  506 batches | lr 10.00 | ms/batch 126.35 | loss  4.09 | ppl    59.49\n",
            "| epoch   3 |   430/  506 batches | lr 10.00 | ms/batch 126.31 | loss  4.04 | ppl    56.64\n",
            "| epoch   3 |   440/  506 batches | lr 10.00 | ms/batch 127.23 | loss  4.13 | ppl    62.31\n",
            "| epoch   3 |   450/  506 batches | lr 10.00 | ms/batch 126.49 | loss  4.14 | ppl    62.57\n",
            "| epoch   3 |   460/  506 batches | lr 10.00 | ms/batch 126.65 | loss  4.09 | ppl    59.92\n",
            "| epoch   3 |   470/  506 batches | lr 10.00 | ms/batch 127.06 | loss  4.08 | ppl    58.95\n",
            "| epoch   3 |   480/  506 batches | lr 10.00 | ms/batch 127.04 | loss  4.09 | ppl    59.51\n",
            "| epoch   3 |   490/  506 batches | lr 10.00 | ms/batch 126.70 | loss  4.08 | ppl    59.30\n",
            "| epoch   3 |   500/  506 batches | lr 10.00 | ms/batch 126.92 | loss  4.07 | ppl    58.49\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 75.88s | valid loss  4.04 | valid ppl    56.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |    10/  506 batches | lr 10.00 | ms/batch 140.17 | loss  4.50 | ppl    90.46\n",
            "| epoch   4 |    20/  506 batches | lr 10.00 | ms/batch 126.89 | loss  4.04 | ppl    56.81\n",
            "| epoch   4 |    30/  506 batches | lr 10.00 | ms/batch 127.60 | loss  4.03 | ppl    56.28\n",
            "| epoch   4 |    40/  506 batches | lr 10.00 | ms/batch 127.38 | loss  4.02 | ppl    55.87\n",
            "| epoch   4 |    50/  506 batches | lr 10.00 | ms/batch 127.39 | loss  4.08 | ppl    59.07\n",
            "| epoch   4 |    60/  506 batches | lr 10.00 | ms/batch 127.53 | loss  4.06 | ppl    57.75\n",
            "| epoch   4 |    70/  506 batches | lr 10.00 | ms/batch 126.98 | loss  4.05 | ppl    57.12\n",
            "| epoch   4 |    80/  506 batches | lr 10.00 | ms/batch 127.48 | loss  4.04 | ppl    56.75\n",
            "| epoch   4 |    90/  506 batches | lr 10.00 | ms/batch 127.68 | loss  4.06 | ppl    57.74\n",
            "| epoch   4 |   100/  506 batches | lr 10.00 | ms/batch 127.60 | loss  4.06 | ppl    58.16\n",
            "| epoch   4 |   110/  506 batches | lr 10.00 | ms/batch 128.34 | loss  3.99 | ppl    54.02\n",
            "| epoch   4 |   120/  506 batches | lr 10.00 | ms/batch 127.75 | loss  4.02 | ppl    55.44\n",
            "| epoch   4 |   130/  506 batches | lr 10.00 | ms/batch 127.69 | loss  4.03 | ppl    56.42\n",
            "| epoch   4 |   140/  506 batches | lr 10.00 | ms/batch 127.69 | loss  4.01 | ppl    55.05\n",
            "| epoch   4 |   150/  506 batches | lr 10.00 | ms/batch 127.53 | loss  4.03 | ppl    56.22\n",
            "| epoch   4 |   160/  506 batches | lr 10.00 | ms/batch 127.09 | loss  4.03 | ppl    56.45\n",
            "| epoch   4 |   170/  506 batches | lr 10.00 | ms/batch 126.98 | loss  3.99 | ppl    54.25\n",
            "| epoch   4 |   180/  506 batches | lr 10.00 | ms/batch 127.54 | loss  4.03 | ppl    56.01\n",
            "| epoch   4 |   190/  506 batches | lr 10.00 | ms/batch 127.11 | loss  4.04 | ppl    57.10\n",
            "| epoch   4 |   200/  506 batches | lr 10.00 | ms/batch 126.76 | loss  4.00 | ppl    54.43\n",
            "| epoch   4 |   210/  506 batches | lr 10.00 | ms/batch 127.46 | loss  3.98 | ppl    53.71\n",
            "| epoch   4 |   220/  506 batches | lr 10.00 | ms/batch 127.52 | loss  4.02 | ppl    55.89\n",
            "| epoch   4 |   230/  506 batches | lr 10.00 | ms/batch 127.23 | loss  3.99 | ppl    53.99\n",
            "| epoch   4 |   240/  506 batches | lr 10.00 | ms/batch 126.98 | loss  3.95 | ppl    52.09\n",
            "| epoch   4 |   250/  506 batches | lr 10.00 | ms/batch 127.31 | loss  3.95 | ppl    52.04\n",
            "| epoch   4 |   260/  506 batches | lr 10.00 | ms/batch 127.45 | loss  3.97 | ppl    53.21\n",
            "| epoch   4 |   270/  506 batches | lr 10.00 | ms/batch 127.41 | loss  3.97 | ppl    52.95\n",
            "| epoch   4 |   280/  506 batches | lr 10.00 | ms/batch 127.22 | loss  3.99 | ppl    53.89\n",
            "| epoch   4 |   290/  506 batches | lr 10.00 | ms/batch 126.64 | loss  3.99 | ppl    53.86\n",
            "| epoch   4 |   300/  506 batches | lr 10.00 | ms/batch 127.43 | loss  4.00 | ppl    54.68\n",
            "| epoch   4 |   310/  506 batches | lr 10.00 | ms/batch 126.78 | loss  3.98 | ppl    53.39\n",
            "| epoch   4 |   320/  506 batches | lr 10.00 | ms/batch 126.76 | loss  4.02 | ppl    55.48\n",
            "| epoch   4 |   330/  506 batches | lr 10.00 | ms/batch 127.20 | loss  3.96 | ppl    52.70\n",
            "| epoch   4 |   340/  506 batches | lr 10.00 | ms/batch 127.16 | loss  3.97 | ppl    53.17\n",
            "| epoch   4 |   350/  506 batches | lr 10.00 | ms/batch 126.56 | loss  4.00 | ppl    54.76\n",
            "| epoch   4 |   360/  506 batches | lr 10.00 | ms/batch 126.93 | loss  3.97 | ppl    53.09\n",
            "| epoch   4 |   370/  506 batches | lr 10.00 | ms/batch 127.05 | loss  3.98 | ppl    53.40\n",
            "| epoch   4 |   380/  506 batches | lr 10.00 | ms/batch 127.29 | loss  3.96 | ppl    52.69\n",
            "| epoch   4 |   390/  506 batches | lr 10.00 | ms/batch 127.46 | loss  3.97 | ppl    52.78\n",
            "| epoch   4 |   400/  506 batches | lr 10.00 | ms/batch 126.33 | loss  3.94 | ppl    51.29\n",
            "| epoch   4 |   410/  506 batches | lr 10.00 | ms/batch 127.15 | loss  3.94 | ppl    51.20\n",
            "| epoch   4 |   420/  506 batches | lr 10.00 | ms/batch 127.74 | loss  3.91 | ppl    49.65\n",
            "| epoch   4 |   430/  506 batches | lr 10.00 | ms/batch 126.84 | loss  3.86 | ppl    47.53\n",
            "| epoch   4 |   440/  506 batches | lr 10.00 | ms/batch 126.87 | loss  3.94 | ppl    51.40\n",
            "| epoch   4 |   450/  506 batches | lr 10.00 | ms/batch 126.12 | loss  3.97 | ppl    53.06\n",
            "| epoch   4 |   460/  506 batches | lr 10.00 | ms/batch 126.68 | loss  3.92 | ppl    50.53\n",
            "| epoch   4 |   470/  506 batches | lr 10.00 | ms/batch 126.03 | loss  3.91 | ppl    49.83\n",
            "| epoch   4 |   480/  506 batches | lr 10.00 | ms/batch 126.51 | loss  3.92 | ppl    50.27\n",
            "| epoch   4 |   490/  506 batches | lr 10.00 | ms/batch 126.70 | loss  3.91 | ppl    49.95\n",
            "| epoch   4 |   500/  506 batches | lr 10.00 | ms/batch 127.34 | loss  3.91 | ppl    49.99\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 76.27s | valid loss  3.89 | valid ppl    49.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |    10/  506 batches | lr 10.00 | ms/batch 138.05 | loss  4.33 | ppl    75.77\n",
            "| epoch   5 |    20/  506 batches | lr 10.00 | ms/batch 126.23 | loss  3.90 | ppl    49.17\n",
            "| epoch   5 |    30/  506 batches | lr 10.00 | ms/batch 126.58 | loss  3.89 | ppl    48.73\n",
            "| epoch   5 |    40/  506 batches | lr 10.00 | ms/batch 126.55 | loss  3.88 | ppl    48.47\n",
            "| epoch   5 |    50/  506 batches | lr 10.00 | ms/batch 126.12 | loss  3.91 | ppl    50.04\n",
            "| epoch   5 |    60/  506 batches | lr 10.00 | ms/batch 126.35 | loss  3.89 | ppl    49.04\n",
            "| epoch   5 |    70/  506 batches | lr 10.00 | ms/batch 126.51 | loss  3.90 | ppl    49.44\n",
            "| epoch   5 |    80/  506 batches | lr 10.00 | ms/batch 126.39 | loss  3.88 | ppl    48.33\n",
            "| epoch   5 |    90/  506 batches | lr 10.00 | ms/batch 126.82 | loss  3.92 | ppl    50.65\n",
            "| epoch   5 |   100/  506 batches | lr 10.00 | ms/batch 126.00 | loss  3.91 | ppl    49.71\n",
            "| epoch   5 |   110/  506 batches | lr 10.00 | ms/batch 126.88 | loss  3.84 | ppl    46.48\n",
            "| epoch   5 |   120/  506 batches | lr 10.00 | ms/batch 126.69 | loss  3.87 | ppl    47.81\n",
            "| epoch   5 |   130/  506 batches | lr 10.00 | ms/batch 127.58 | loss  3.88 | ppl    48.35\n",
            "| epoch   5 |   140/  506 batches | lr 10.00 | ms/batch 126.37 | loss  3.87 | ppl    47.81\n",
            "| epoch   5 |   150/  506 batches | lr 10.00 | ms/batch 127.13 | loss  3.87 | ppl    47.98\n",
            "| epoch   5 |   160/  506 batches | lr 10.00 | ms/batch 126.90 | loss  3.88 | ppl    48.54\n",
            "| epoch   5 |   170/  506 batches | lr 10.00 | ms/batch 126.79 | loss  3.85 | ppl    46.88\n",
            "| epoch   5 |   180/  506 batches | lr 10.00 | ms/batch 127.82 | loss  3.88 | ppl    48.37\n",
            "| epoch   5 |   190/  506 batches | lr 10.00 | ms/batch 126.69 | loss  3.88 | ppl    48.40\n",
            "| epoch   5 |   200/  506 batches | lr 10.00 | ms/batch 126.65 | loss  3.86 | ppl    47.64\n",
            "| epoch   5 |   210/  506 batches | lr 10.00 | ms/batch 126.84 | loss  3.84 | ppl    46.61\n",
            "| epoch   5 |   220/  506 batches | lr 10.00 | ms/batch 127.55 | loss  3.87 | ppl    47.75\n",
            "| epoch   5 |   230/  506 batches | lr 10.00 | ms/batch 126.97 | loss  3.84 | ppl    46.70\n",
            "| epoch   5 |   240/  506 batches | lr 10.00 | ms/batch 126.73 | loss  3.83 | ppl    46.03\n",
            "| epoch   5 |   250/  506 batches | lr 10.00 | ms/batch 127.10 | loss  3.83 | ppl    46.06\n",
            "| epoch   5 |   260/  506 batches | lr 10.00 | ms/batch 126.61 | loss  3.85 | ppl    46.84\n",
            "| epoch   5 |   270/  506 batches | lr 10.00 | ms/batch 127.35 | loss  3.83 | ppl    46.13\n",
            "| epoch   5 |   280/  506 batches | lr 10.00 | ms/batch 127.01 | loss  3.85 | ppl    46.97\n",
            "| epoch   5 |   290/  506 batches | lr 10.00 | ms/batch 127.06 | loss  3.86 | ppl    47.45\n",
            "| epoch   5 |   300/  506 batches | lr 10.00 | ms/batch 127.10 | loss  3.87 | ppl    47.92\n",
            "| epoch   5 |   310/  506 batches | lr 10.00 | ms/batch 127.18 | loss  3.85 | ppl    46.77\n",
            "| epoch   5 |   320/  506 batches | lr 10.00 | ms/batch 126.66 | loss  3.88 | ppl    48.41\n",
            "| epoch   5 |   330/  506 batches | lr 10.00 | ms/batch 127.71 | loss  3.85 | ppl    46.99\n",
            "| epoch   5 |   340/  506 batches | lr 10.00 | ms/batch 127.05 | loss  3.84 | ppl    46.71\n",
            "| epoch   5 |   350/  506 batches | lr 10.00 | ms/batch 127.35 | loss  3.86 | ppl    47.25\n",
            "| epoch   5 |   360/  506 batches | lr 10.00 | ms/batch 127.16 | loss  3.85 | ppl    46.94\n",
            "| epoch   5 |   370/  506 batches | lr 10.00 | ms/batch 127.34 | loss  3.85 | ppl    46.96\n",
            "| epoch   5 |   380/  506 batches | lr 10.00 | ms/batch 126.81 | loss  3.83 | ppl    46.16\n",
            "| epoch   5 |   390/  506 batches | lr 10.00 | ms/batch 126.90 | loss  3.84 | ppl    46.37\n",
            "| epoch   5 |   400/  506 batches | lr 10.00 | ms/batch 127.82 | loss  3.82 | ppl    45.49\n",
            "| epoch   5 |   410/  506 batches | lr 10.00 | ms/batch 127.33 | loss  3.80 | ppl    44.77\n",
            "| epoch   5 |   420/  506 batches | lr 10.00 | ms/batch 127.52 | loss  3.79 | ppl    44.15\n",
            "| epoch   5 |   430/  506 batches | lr 10.00 | ms/batch 127.31 | loss  3.75 | ppl    42.54\n",
            "| epoch   5 |   440/  506 batches | lr 10.00 | ms/batch 127.62 | loss  3.81 | ppl    45.31\n",
            "| epoch   5 |   450/  506 batches | lr 10.00 | ms/batch 127.54 | loss  3.84 | ppl    46.44\n",
            "| epoch   5 |   460/  506 batches | lr 10.00 | ms/batch 127.51 | loss  3.80 | ppl    44.63\n",
            "| epoch   5 |   470/  506 batches | lr 10.00 | ms/batch 127.14 | loss  3.79 | ppl    44.19\n",
            "| epoch   5 |   480/  506 batches | lr 10.00 | ms/batch 127.35 | loss  3.79 | ppl    44.39\n",
            "| epoch   5 |   490/  506 batches | lr 10.00 | ms/batch 127.82 | loss  3.79 | ppl    44.06\n",
            "| epoch   5 |   500/  506 batches | lr 10.00 | ms/batch 127.19 | loss  3.79 | ppl    44.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 76.23s | valid loss  3.79 | valid ppl    44.24\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |    10/  506 batches | lr 10.00 | ms/batch 140.14 | loss  4.33 | ppl    76.16\n",
            "| epoch   6 |    20/  506 batches | lr 10.00 | ms/batch 127.25 | loss  3.82 | ppl    45.55\n",
            "| epoch   6 |    30/  506 batches | lr 10.00 | ms/batch 127.83 | loss  3.77 | ppl    43.36\n",
            "| epoch   6 |    40/  506 batches | lr 10.00 | ms/batch 126.75 | loss  3.77 | ppl    43.21\n",
            "| epoch   6 |    50/  506 batches | lr 10.00 | ms/batch 127.43 | loss  3.80 | ppl    44.53\n",
            "| epoch   6 |    60/  506 batches | lr 10.00 | ms/batch 126.40 | loss  3.77 | ppl    43.49\n",
            "| epoch   6 |    70/  506 batches | lr 10.00 | ms/batch 128.28 | loss  3.79 | ppl    44.35\n",
            "| epoch   6 |    80/  506 batches | lr 10.00 | ms/batch 127.28 | loss  3.77 | ppl    43.59\n",
            "| epoch   6 |    90/  506 batches | lr 10.00 | ms/batch 128.22 | loss  3.80 | ppl    44.66\n",
            "| epoch   6 |   100/  506 batches | lr 10.00 | ms/batch 127.63 | loss  3.78 | ppl    43.73\n",
            "| epoch   6 |   110/  506 batches | lr 10.00 | ms/batch 127.45 | loss  3.73 | ppl    41.62\n",
            "| epoch   6 |   120/  506 batches | lr 10.00 | ms/batch 127.62 | loss  3.77 | ppl    43.40\n",
            "| epoch   6 |   130/  506 batches | lr 10.00 | ms/batch 127.45 | loss  3.77 | ppl    43.30\n",
            "| epoch   6 |   140/  506 batches | lr 10.00 | ms/batch 127.92 | loss  3.76 | ppl    42.85\n",
            "| epoch   6 |   150/  506 batches | lr 10.00 | ms/batch 127.79 | loss  3.77 | ppl    43.19\n",
            "| epoch   6 |   160/  506 batches | lr 10.00 | ms/batch 127.29 | loss  3.78 | ppl    43.76\n",
            "| epoch   6 |   170/  506 batches | lr 10.00 | ms/batch 127.64 | loss  3.74 | ppl    42.27\n",
            "| epoch   6 |   180/  506 batches | lr 10.00 | ms/batch 127.31 | loss  3.77 | ppl    43.26\n",
            "| epoch   6 |   190/  506 batches | lr 10.00 | ms/batch 127.36 | loss  3.78 | ppl    43.92\n",
            "| epoch   6 |   200/  506 batches | lr 10.00 | ms/batch 127.96 | loss  3.76 | ppl    42.97\n",
            "| epoch   6 |   210/  506 batches | lr 10.00 | ms/batch 127.90 | loss  3.75 | ppl    42.35\n",
            "| epoch   6 |   220/  506 batches | lr 10.00 | ms/batch 127.58 | loss  3.75 | ppl    42.60\n",
            "| epoch   6 |   230/  506 batches | lr 10.00 | ms/batch 127.49 | loss  3.75 | ppl    42.42\n",
            "| epoch   6 |   240/  506 batches | lr 10.00 | ms/batch 127.74 | loss  3.71 | ppl    41.05\n",
            "| epoch   6 |   250/  506 batches | lr 10.00 | ms/batch 127.28 | loss  3.71 | ppl    40.79\n",
            "| epoch   6 |   260/  506 batches | lr 10.00 | ms/batch 127.59 | loss  3.74 | ppl    42.17\n",
            "| epoch   6 |   270/  506 batches | lr 10.00 | ms/batch 127.79 | loss  3.73 | ppl    41.50\n",
            "| epoch   6 |   280/  506 batches | lr 10.00 | ms/batch 127.72 | loss  3.74 | ppl    42.16\n",
            "| epoch   6 |   290/  506 batches | lr 10.00 | ms/batch 127.00 | loss  3.75 | ppl    42.66\n",
            "| epoch   6 |   300/  506 batches | lr 10.00 | ms/batch 127.19 | loss  3.77 | ppl    43.26\n",
            "| epoch   6 |   310/  506 batches | lr 10.00 | ms/batch 128.04 | loss  3.76 | ppl    42.95\n",
            "| epoch   6 |   320/  506 batches | lr 10.00 | ms/batch 127.36 | loss  3.78 | ppl    43.86\n",
            "| epoch   6 |   330/  506 batches | lr 10.00 | ms/batch 127.34 | loss  3.73 | ppl    41.88\n",
            "| epoch   6 |   340/  506 batches | lr 10.00 | ms/batch 127.60 | loss  3.74 | ppl    42.16\n",
            "| epoch   6 |   350/  506 batches | lr 10.00 | ms/batch 128.11 | loss  3.74 | ppl    42.08\n",
            "| epoch   6 |   360/  506 batches | lr 10.00 | ms/batch 128.38 | loss  3.74 | ppl    42.25\n",
            "| epoch   6 |   370/  506 batches | lr 10.00 | ms/batch 127.44 | loss  3.75 | ppl    42.55\n",
            "| epoch   6 |   380/  506 batches | lr 10.00 | ms/batch 126.78 | loss  3.73 | ppl    41.80\n",
            "| epoch   6 |   390/  506 batches | lr 10.00 | ms/batch 127.51 | loss  3.74 | ppl    41.93\n",
            "| epoch   6 |   400/  506 batches | lr 10.00 | ms/batch 127.81 | loss  3.71 | ppl    40.90\n",
            "| epoch   6 |   410/  506 batches | lr 10.00 | ms/batch 127.37 | loss  3.70 | ppl    40.64\n",
            "| epoch   6 |   420/  506 batches | lr 10.00 | ms/batch 128.03 | loss  3.68 | ppl    39.51\n",
            "| epoch   6 |   430/  506 batches | lr 10.00 | ms/batch 128.22 | loss  3.64 | ppl    38.00\n",
            "| epoch   6 |   440/  506 batches | lr 10.00 | ms/batch 128.36 | loss  3.71 | ppl    40.80\n",
            "| epoch   6 |   450/  506 batches | lr 10.00 | ms/batch 127.61 | loss  3.75 | ppl    42.72\n",
            "| epoch   6 |   460/  506 batches | lr 10.00 | ms/batch 127.18 | loss  3.70 | ppl    40.42\n",
            "| epoch   6 |   470/  506 batches | lr 10.00 | ms/batch 128.00 | loss  3.69 | ppl    40.04\n",
            "| epoch   6 |   480/  506 batches | lr 10.00 | ms/batch 127.90 | loss  3.70 | ppl    40.32\n",
            "| epoch   6 |   490/  506 batches | lr 10.00 | ms/batch 128.18 | loss  3.69 | ppl    40.10\n",
            "| epoch   6 |   500/  506 batches | lr 10.00 | ms/batch 127.71 | loss  3.68 | ppl    39.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 76.54s | valid loss  3.71 | valid ppl    40.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |    10/  506 batches | lr 10.00 | ms/batch 140.21 | loss  4.10 | ppl    60.30\n",
            "| epoch   7 |    20/  506 batches | lr 10.00 | ms/batch 127.85 | loss  3.69 | ppl    40.16\n",
            "| epoch   7 |    30/  506 batches | lr 10.00 | ms/batch 128.07 | loss  3.67 | ppl    39.32\n",
            "| epoch   7 |    40/  506 batches | lr 10.00 | ms/batch 127.56 | loss  3.67 | ppl    39.29\n",
            "| epoch   7 |    50/  506 batches | lr 10.00 | ms/batch 127.49 | loss  3.71 | ppl    40.66\n",
            "| epoch   7 |    60/  506 batches | lr 10.00 | ms/batch 127.66 | loss  3.68 | ppl    39.84\n",
            "| epoch   7 |    70/  506 batches | lr 10.00 | ms/batch 128.70 | loss  3.70 | ppl    40.48\n",
            "| epoch   7 |    80/  506 batches | lr 10.00 | ms/batch 128.22 | loss  3.68 | ppl    39.56\n",
            "| epoch   7 |    90/  506 batches | lr 10.00 | ms/batch 128.51 | loss  3.71 | ppl    40.73\n",
            "| epoch   7 |   100/  506 batches | lr 10.00 | ms/batch 128.51 | loss  3.68 | ppl    39.82\n",
            "| epoch   7 |   110/  506 batches | lr 10.00 | ms/batch 128.24 | loss  3.66 | ppl    38.84\n",
            "| epoch   7 |   120/  506 batches | lr 10.00 | ms/batch 128.27 | loss  3.67 | ppl    39.16\n",
            "| epoch   7 |   130/  506 batches | lr 10.00 | ms/batch 127.50 | loss  3.68 | ppl    39.65\n",
            "| epoch   7 |   140/  506 batches | lr 10.00 | ms/batch 128.13 | loss  3.68 | ppl    39.53\n",
            "| epoch   7 |   150/  506 batches | lr 10.00 | ms/batch 128.08 | loss  3.67 | ppl    39.36\n",
            "| epoch   7 |   160/  506 batches | lr 10.00 | ms/batch 127.10 | loss  3.69 | ppl    40.00\n",
            "| epoch   7 |   170/  506 batches | lr 10.00 | ms/batch 127.33 | loss  3.66 | ppl    38.73\n",
            "| epoch   7 |   180/  506 batches | lr 10.00 | ms/batch 127.07 | loss  3.68 | ppl    39.74\n",
            "| epoch   7 |   190/  506 batches | lr 10.00 | ms/batch 129.00 | loss  3.70 | ppl    40.25\n",
            "| epoch   7 |   200/  506 batches | lr 10.00 | ms/batch 127.24 | loss  3.67 | ppl    39.12\n",
            "| epoch   7 |   210/  506 batches | lr 10.00 | ms/batch 127.68 | loss  3.66 | ppl    39.05\n",
            "| epoch   7 |   220/  506 batches | lr 10.00 | ms/batch 127.26 | loss  3.66 | ppl    39.03\n",
            "| epoch   7 |   230/  506 batches | lr 10.00 | ms/batch 127.54 | loss  3.65 | ppl    38.48\n",
            "| epoch   7 |   240/  506 batches | lr 10.00 | ms/batch 128.00 | loss  3.63 | ppl    37.70\n",
            "| epoch   7 |   250/  506 batches | lr 10.00 | ms/batch 127.40 | loss  3.62 | ppl    37.48\n",
            "| epoch   7 |   260/  506 batches | lr 10.00 | ms/batch 127.14 | loss  3.66 | ppl    38.74\n",
            "| epoch   7 |   270/  506 batches | lr 10.00 | ms/batch 127.76 | loss  3.65 | ppl    38.45\n",
            "| epoch   7 |   280/  506 batches | lr 10.00 | ms/batch 127.52 | loss  3.66 | ppl    38.81\n",
            "| epoch   7 |   290/  506 batches | lr 10.00 | ms/batch 127.49 | loss  3.66 | ppl    39.02\n",
            "| epoch   7 |   300/  506 batches | lr 10.00 | ms/batch 127.90 | loss  3.68 | ppl    39.72\n",
            "| epoch   7 |   310/  506 batches | lr 10.00 | ms/batch 127.46 | loss  3.65 | ppl    38.66\n",
            "| epoch   7 |   320/  506 batches | lr 10.00 | ms/batch 127.74 | loss  3.69 | ppl    39.99\n",
            "| epoch   7 |   330/  506 batches | lr 10.00 | ms/batch 128.11 | loss  3.65 | ppl    38.43\n",
            "| epoch   7 |   340/  506 batches | lr 10.00 | ms/batch 127.29 | loss  3.66 | ppl    38.91\n",
            "| epoch   7 |   350/  506 batches | lr 10.00 | ms/batch 127.32 | loss  3.65 | ppl    38.67\n",
            "| epoch   7 |   360/  506 batches | lr 10.00 | ms/batch 127.06 | loss  3.67 | ppl    39.15\n",
            "| epoch   7 |   370/  506 batches | lr 10.00 | ms/batch 126.85 | loss  3.67 | ppl    39.16\n",
            "| epoch   7 |   380/  506 batches | lr 10.00 | ms/batch 127.49 | loss  3.65 | ppl    38.44\n",
            "| epoch   7 |   390/  506 batches | lr 10.00 | ms/batch 127.49 | loss  3.67 | ppl    39.20\n",
            "| epoch   7 |   400/  506 batches | lr 10.00 | ms/batch 127.01 | loss  3.63 | ppl    37.82\n",
            "| epoch   7 |   410/  506 batches | lr 10.00 | ms/batch 128.11 | loss  3.63 | ppl    37.62\n",
            "| epoch   7 |   420/  506 batches | lr 10.00 | ms/batch 127.14 | loss  3.60 | ppl    36.42\n",
            "| epoch   7 |   430/  506 batches | lr 10.00 | ms/batch 126.75 | loss  3.57 | ppl    35.39\n",
            "| epoch   7 |   440/  506 batches | lr 10.00 | ms/batch 127.30 | loss  3.62 | ppl    37.49\n",
            "| epoch   7 |   450/  506 batches | lr 10.00 | ms/batch 127.72 | loss  3.66 | ppl    38.92\n",
            "| epoch   7 |   460/  506 batches | lr 10.00 | ms/batch 127.48 | loss  3.61 | ppl    37.13\n",
            "| epoch   7 |   470/  506 batches | lr 10.00 | ms/batch 127.32 | loss  3.61 | ppl    36.99\n",
            "| epoch   7 |   480/  506 batches | lr 10.00 | ms/batch 126.92 | loss  3.61 | ppl    36.91\n",
            "| epoch   7 |   490/  506 batches | lr 10.00 | ms/batch 127.25 | loss  3.61 | ppl    36.99\n",
            "| epoch   7 |   500/  506 batches | lr 10.00 | ms/batch 127.34 | loss  3.60 | ppl    36.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 76.51s | valid loss  3.65 | valid ppl    38.34\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |    10/  506 batches | lr 10.00 | ms/batch 138.57 | loss  4.01 | ppl    55.32\n",
            "| epoch   8 |    20/  506 batches | lr 10.00 | ms/batch 126.03 | loss  3.61 | ppl    37.07\n",
            "| epoch   8 |    30/  506 batches | lr 10.00 | ms/batch 127.49 | loss  3.60 | ppl    36.66\n",
            "| epoch   8 |    40/  506 batches | lr 10.00 | ms/batch 127.09 | loss  3.59 | ppl    36.18\n",
            "| epoch   8 |    50/  506 batches | lr 10.00 | ms/batch 126.77 | loss  3.61 | ppl    37.15\n",
            "| epoch   8 |    60/  506 batches | lr 10.00 | ms/batch 126.59 | loss  3.61 | ppl    36.81\n",
            "| epoch   8 |    70/  506 batches | lr 10.00 | ms/batch 127.37 | loss  3.62 | ppl    37.37\n",
            "| epoch   8 |    80/  506 batches | lr 10.00 | ms/batch 126.55 | loss  3.60 | ppl    36.72\n",
            "| epoch   8 |    90/  506 batches | lr 10.00 | ms/batch 127.41 | loss  3.63 | ppl    37.80\n",
            "| epoch   8 |   100/  506 batches | lr 10.00 | ms/batch 127.16 | loss  3.61 | ppl    36.79\n",
            "| epoch   8 |   110/  506 batches | lr 10.00 | ms/batch 126.88 | loss  3.56 | ppl    35.22\n",
            "| epoch   8 |   120/  506 batches | lr 10.00 | ms/batch 127.51 | loss  3.59 | ppl    36.28\n",
            "| epoch   8 |   130/  506 batches | lr 10.00 | ms/batch 126.70 | loss  3.60 | ppl    36.68\n",
            "| epoch   8 |   140/  506 batches | lr 10.00 | ms/batch 128.05 | loss  3.60 | ppl    36.49\n",
            "| epoch   8 |   150/  506 batches | lr 10.00 | ms/batch 126.59 | loss  3.60 | ppl    36.64\n",
            "| epoch   8 |   160/  506 batches | lr 10.00 | ms/batch 127.24 | loss  3.61 | ppl    37.02\n",
            "| epoch   8 |   170/  506 batches | lr 10.00 | ms/batch 127.43 | loss  3.58 | ppl    35.75\n",
            "| epoch   8 |   180/  506 batches | lr 10.00 | ms/batch 127.61 | loss  3.60 | ppl    36.69\n",
            "| epoch   8 |   190/  506 batches | lr 10.00 | ms/batch 127.48 | loss  3.61 | ppl    37.08\n",
            "| epoch   8 |   200/  506 batches | lr 10.00 | ms/batch 127.59 | loss  3.58 | ppl    35.93\n",
            "| epoch   8 |   210/  506 batches | lr 10.00 | ms/batch 128.30 | loss  3.57 | ppl    35.67\n",
            "| epoch   8 |   220/  506 batches | lr 10.00 | ms/batch 127.63 | loss  3.59 | ppl    36.09\n",
            "| epoch   8 |   230/  506 batches | lr 10.00 | ms/batch 127.42 | loss  3.58 | ppl    35.88\n",
            "| epoch   8 |   240/  506 batches | lr 10.00 | ms/batch 127.39 | loss  3.55 | ppl    34.95\n",
            "| epoch   8 |   250/  506 batches | lr 10.00 | ms/batch 127.38 | loss  3.55 | ppl    34.69\n",
            "| epoch   8 |   260/  506 batches | lr 10.00 | ms/batch 127.48 | loss  3.59 | ppl    36.26\n",
            "| epoch   8 |   270/  506 batches | lr 10.00 | ms/batch 127.65 | loss  3.57 | ppl    35.48\n",
            "| epoch   8 |   280/  506 batches | lr 10.00 | ms/batch 128.53 | loss  3.59 | ppl    36.08\n",
            "| epoch   8 |   290/  506 batches | lr 10.00 | ms/batch 127.98 | loss  3.59 | ppl    36.30\n",
            "| epoch   8 |   300/  506 batches | lr 10.00 | ms/batch 127.65 | loss  3.61 | ppl    36.89\n",
            "| epoch   8 |   310/  506 batches | lr 10.00 | ms/batch 128.15 | loss  3.58 | ppl    35.79\n",
            "| epoch   8 |   320/  506 batches | lr 10.00 | ms/batch 128.31 | loss  3.63 | ppl    37.61\n",
            "| epoch   8 |   330/  506 batches | lr 10.00 | ms/batch 128.16 | loss  3.58 | ppl    35.93\n",
            "| epoch   8 |   340/  506 batches | lr 10.00 | ms/batch 128.55 | loss  3.58 | ppl    35.97\n",
            "| epoch   8 |   350/  506 batches | lr 10.00 | ms/batch 127.93 | loss  3.59 | ppl    36.34\n",
            "| epoch   8 |   360/  506 batches | lr 10.00 | ms/batch 128.95 | loss  3.60 | ppl    36.52\n",
            "| epoch   8 |   370/  506 batches | lr 10.00 | ms/batch 127.46 | loss  3.59 | ppl    36.07\n",
            "| epoch   8 |   380/  506 batches | lr 10.00 | ms/batch 127.51 | loss  3.58 | ppl    35.80\n",
            "| epoch   8 |   390/  506 batches | lr 10.00 | ms/batch 128.11 | loss  3.58 | ppl    35.87\n",
            "| epoch   8 |   400/  506 batches | lr 10.00 | ms/batch 127.67 | loss  3.55 | ppl    34.72\n",
            "| epoch   8 |   410/  506 batches | lr 10.00 | ms/batch 128.15 | loss  3.57 | ppl    35.35\n",
            "| epoch   8 |   420/  506 batches | lr 10.00 | ms/batch 127.58 | loss  3.52 | ppl    33.84\n",
            "| epoch   8 |   430/  506 batches | lr 10.00 | ms/batch 126.89 | loss  3.50 | ppl    33.11\n",
            "| epoch   8 |   440/  506 batches | lr 10.00 | ms/batch 127.64 | loss  3.56 | ppl    35.18\n",
            "| epoch   8 |   450/  506 batches | lr 10.00 | ms/batch 127.61 | loss  3.59 | ppl    36.33\n",
            "| epoch   8 |   460/  506 batches | lr 10.00 | ms/batch 127.54 | loss  3.55 | ppl    34.67\n",
            "| epoch   8 |   470/  506 batches | lr 10.00 | ms/batch 127.12 | loss  3.54 | ppl    34.57\n",
            "| epoch   8 |   480/  506 batches | lr 10.00 | ms/batch 127.87 | loss  3.55 | ppl    34.84\n",
            "| epoch   8 |   490/  506 batches | lr 10.00 | ms/batch 127.34 | loss  3.53 | ppl    34.14\n",
            "| epoch   8 |   500/  506 batches | lr 10.00 | ms/batch 127.08 | loss  3.53 | ppl    34.20\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 76.43s | valid loss  3.60 | valid ppl    36.50\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |    10/  506 batches | lr 10.00 | ms/batch 137.85 | loss  3.93 | ppl    50.76\n",
            "| epoch   9 |    20/  506 batches | lr 10.00 | ms/batch 126.07 | loss  3.54 | ppl    34.47\n",
            "| epoch   9 |    30/  506 batches | lr 10.00 | ms/batch 127.39 | loss  3.53 | ppl    34.21\n",
            "| epoch   9 |    40/  506 batches | lr 10.00 | ms/batch 126.28 | loss  3.52 | ppl    33.71\n",
            "| epoch   9 |    50/  506 batches | lr 10.00 | ms/batch 126.24 | loss  3.55 | ppl    34.92\n",
            "| epoch   9 |    60/  506 batches | lr 10.00 | ms/batch 125.94 | loss  3.54 | ppl    34.49\n",
            "| epoch   9 |    70/  506 batches | lr 10.00 | ms/batch 126.70 | loss  3.57 | ppl    35.37\n",
            "| epoch   9 |    80/  506 batches | lr 10.00 | ms/batch 126.56 | loss  3.54 | ppl    34.50\n",
            "| epoch   9 |    90/  506 batches | lr 10.00 | ms/batch 126.83 | loss  3.56 | ppl    35.15\n",
            "| epoch   9 |   100/  506 batches | lr 10.00 | ms/batch 126.97 | loss  3.56 | ppl    35.14\n",
            "| epoch   9 |   110/  506 batches | lr 10.00 | ms/batch 126.53 | loss  3.49 | ppl    32.77\n",
            "| epoch   9 |   120/  506 batches | lr 10.00 | ms/batch 127.30 | loss  3.53 | ppl    34.19\n",
            "| epoch   9 |   130/  506 batches | lr 10.00 | ms/batch 126.60 | loss  3.54 | ppl    34.46\n",
            "| epoch   9 |   140/  506 batches | lr 10.00 | ms/batch 127.18 | loss  3.52 | ppl    33.90\n",
            "| epoch   9 |   150/  506 batches | lr 10.00 | ms/batch 126.69 | loss  3.53 | ppl    34.24\n",
            "| epoch   9 |   160/  506 batches | lr 10.00 | ms/batch 127.20 | loss  3.55 | ppl    34.83\n",
            "| epoch   9 |   170/  506 batches | lr 10.00 | ms/batch 127.72 | loss  3.51 | ppl    33.49\n",
            "| epoch   9 |   180/  506 batches | lr 10.00 | ms/batch 126.77 | loss  3.53 | ppl    34.25\n",
            "| epoch   9 |   190/  506 batches | lr 10.00 | ms/batch 127.04 | loss  3.55 | ppl    34.95\n",
            "| epoch   9 |   200/  506 batches | lr 10.00 | ms/batch 126.33 | loss  3.51 | ppl    33.46\n",
            "| epoch   9 |   210/  506 batches | lr 10.00 | ms/batch 127.30 | loss  3.52 | ppl    33.70\n",
            "| epoch   9 |   220/  506 batches | lr 10.00 | ms/batch 127.12 | loss  3.52 | ppl    33.77\n",
            "| epoch   9 |   230/  506 batches | lr 10.00 | ms/batch 126.94 | loss  3.52 | ppl    33.68\n",
            "| epoch   9 |   240/  506 batches | lr 10.00 | ms/batch 127.21 | loss  3.49 | ppl    32.86\n",
            "| epoch   9 |   250/  506 batches | lr 10.00 | ms/batch 127.01 | loss  3.49 | ppl    32.83\n",
            "| epoch   9 |   260/  506 batches | lr 10.00 | ms/batch 128.09 | loss  3.52 | ppl    33.90\n",
            "| epoch   9 |   270/  506 batches | lr 10.00 | ms/batch 126.82 | loss  3.51 | ppl    33.31\n",
            "| epoch   9 |   280/  506 batches | lr 10.00 | ms/batch 127.63 | loss  3.52 | ppl    33.85\n",
            "| epoch   9 |   290/  506 batches | lr 10.00 | ms/batch 127.24 | loss  3.53 | ppl    33.96\n",
            "| epoch   9 |   300/  506 batches | lr 10.00 | ms/batch 127.59 | loss  3.54 | ppl    34.41\n",
            "| epoch   9 |   310/  506 batches | lr 10.00 | ms/batch 127.26 | loss  3.51 | ppl    33.44\n",
            "| epoch   9 |   320/  506 batches | lr 10.00 | ms/batch 127.16 | loss  3.55 | ppl    34.95\n",
            "| epoch   9 |   330/  506 batches | lr 10.00 | ms/batch 127.28 | loss  3.51 | ppl    33.60\n",
            "| epoch   9 |   340/  506 batches | lr 10.00 | ms/batch 127.85 | loss  3.52 | ppl    33.86\n",
            "| epoch   9 |   350/  506 batches | lr 10.00 | ms/batch 127.60 | loss  3.53 | ppl    34.07\n",
            "| epoch   9 |   360/  506 batches | lr 10.00 | ms/batch 126.59 | loss  3.54 | ppl    34.33\n",
            "| epoch   9 |   370/  506 batches | lr 10.00 | ms/batch 127.43 | loss  3.53 | ppl    34.03\n",
            "| epoch   9 |   380/  506 batches | lr 10.00 | ms/batch 127.21 | loss  3.51 | ppl    33.35\n",
            "| epoch   9 |   390/  506 batches | lr 10.00 | ms/batch 126.83 | loss  3.52 | ppl    33.71\n",
            "| epoch   9 |   400/  506 batches | lr 10.00 | ms/batch 127.46 | loss  3.49 | ppl    32.92\n",
            "| epoch   9 |   410/  506 batches | lr 10.00 | ms/batch 127.36 | loss  3.50 | ppl    32.97\n",
            "| epoch   9 |   420/  506 batches | lr 10.00 | ms/batch 127.36 | loss  3.46 | ppl    31.83\n",
            "| epoch   9 |   430/  506 batches | lr 10.00 | ms/batch 127.56 | loss  3.43 | ppl    30.96\n",
            "| epoch   9 |   440/  506 batches | lr 10.00 | ms/batch 127.66 | loss  3.49 | ppl    32.72\n",
            "| epoch   9 |   450/  506 batches | lr 10.00 | ms/batch 127.80 | loss  3.52 | ppl    33.95\n",
            "| epoch   9 |   460/  506 batches | lr 10.00 | ms/batch 127.73 | loss  3.48 | ppl    32.51\n",
            "| epoch   9 |   470/  506 batches | lr 10.00 | ms/batch 127.08 | loss  3.48 | ppl    32.47\n",
            "| epoch   9 |   480/  506 batches | lr 10.00 | ms/batch 127.24 | loss  3.49 | ppl    32.72\n",
            "| epoch   9 |   490/  506 batches | lr 10.00 | ms/batch 127.71 | loss  3.48 | ppl    32.51\n",
            "| epoch   9 |   500/  506 batches | lr 10.00 | ms/batch 127.84 | loss  3.47 | ppl    32.10\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 76.27s | valid loss  3.56 | valid ppl    35.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |    10/  506 batches | lr 10.00 | ms/batch 139.03 | loss  3.86 | ppl    47.56\n",
            "| epoch  10 |    20/  506 batches | lr 10.00 | ms/batch 127.69 | loss  3.48 | ppl    32.52\n",
            "| epoch  10 |    30/  506 batches | lr 10.00 | ms/batch 128.01 | loss  3.47 | ppl    32.13\n",
            "| epoch  10 |    40/  506 batches | lr 10.00 | ms/batch 127.64 | loss  3.45 | ppl    31.64\n",
            "| epoch  10 |    50/  506 batches | lr 10.00 | ms/batch 127.87 | loss  3.50 | ppl    33.03\n",
            "| epoch  10 |    60/  506 batches | lr 10.00 | ms/batch 127.31 | loss  3.48 | ppl    32.47\n",
            "| epoch  10 |    70/  506 batches | lr 10.00 | ms/batch 127.24 | loss  3.49 | ppl    32.93\n",
            "| epoch  10 |    80/  506 batches | lr 10.00 | ms/batch 127.56 | loss  3.48 | ppl    32.37\n",
            "| epoch  10 |    90/  506 batches | lr 10.00 | ms/batch 127.74 | loss  3.51 | ppl    33.33\n",
            "| epoch  10 |   100/  506 batches | lr 10.00 | ms/batch 127.25 | loss  3.48 | ppl    32.45\n",
            "| epoch  10 |   110/  506 batches | lr 10.00 | ms/batch 127.63 | loss  3.43 | ppl    30.98\n",
            "| epoch  10 |   120/  506 batches | lr 10.00 | ms/batch 127.20 | loss  3.46 | ppl    31.93\n",
            "| epoch  10 |   130/  506 batches | lr 10.00 | ms/batch 127.34 | loss  3.48 | ppl    32.37\n",
            "| epoch  10 |   140/  506 batches | lr 10.00 | ms/batch 127.79 | loss  3.47 | ppl    31.98\n",
            "| epoch  10 |   150/  506 batches | lr 10.00 | ms/batch 127.53 | loss  3.47 | ppl    32.18\n",
            "| epoch  10 |   160/  506 batches | lr 10.00 | ms/batch 128.66 | loss  3.48 | ppl    32.61\n",
            "| epoch  10 |   170/  506 batches | lr 10.00 | ms/batch 128.12 | loss  3.45 | ppl    31.54\n",
            "| epoch  10 |   180/  506 batches | lr 10.00 | ms/batch 127.70 | loss  3.48 | ppl    32.30\n",
            "| epoch  10 |   190/  506 batches | lr 10.00 | ms/batch 127.17 | loss  3.49 | ppl    32.94\n",
            "| epoch  10 |   200/  506 batches | lr 10.00 | ms/batch 128.05 | loss  3.46 | ppl    31.71\n",
            "| epoch  10 |   210/  506 batches | lr 10.00 | ms/batch 127.30 | loss  3.46 | ppl    31.68\n",
            "| epoch  10 |   220/  506 batches | lr 10.00 | ms/batch 128.22 | loss  3.47 | ppl    32.22\n",
            "| epoch  10 |   230/  506 batches | lr 10.00 | ms/batch 127.57 | loss  3.46 | ppl    31.86\n",
            "| epoch  10 |   240/  506 batches | lr 10.00 | ms/batch 127.65 | loss  3.44 | ppl    31.07\n",
            "| epoch  10 |   250/  506 batches | lr 10.00 | ms/batch 127.57 | loss  3.43 | ppl    30.82\n",
            "| epoch  10 |   260/  506 batches | lr 10.00 | ms/batch 127.76 | loss  3.46 | ppl    31.77\n",
            "| epoch  10 |   270/  506 batches | lr 10.00 | ms/batch 128.36 | loss  3.44 | ppl    31.28\n",
            "| epoch  10 |   280/  506 batches | lr 10.00 | ms/batch 127.74 | loss  3.46 | ppl    31.85\n",
            "| epoch  10 |   290/  506 batches | lr 10.00 | ms/batch 127.71 | loss  3.47 | ppl    32.01\n",
            "| epoch  10 |   300/  506 batches | lr 10.00 | ms/batch 127.62 | loss  3.48 | ppl    32.55\n",
            "| epoch  10 |   310/  506 batches | lr 10.00 | ms/batch 127.72 | loss  3.46 | ppl    31.75\n",
            "| epoch  10 |   320/  506 batches | lr 10.00 | ms/batch 128.63 | loss  3.50 | ppl    32.95\n",
            "| epoch  10 |   330/  506 batches | lr 10.00 | ms/batch 127.54 | loss  3.45 | ppl    31.62\n",
            "| epoch  10 |   340/  506 batches | lr 10.00 | ms/batch 127.56 | loss  3.46 | ppl    31.92\n",
            "| epoch  10 |   350/  506 batches | lr 10.00 | ms/batch 127.80 | loss  3.47 | ppl    32.22\n",
            "| epoch  10 |   360/  506 batches | lr 10.00 | ms/batch 127.46 | loss  3.48 | ppl    32.42\n",
            "| epoch  10 |   370/  506 batches | lr 10.00 | ms/batch 127.73 | loss  3.46 | ppl    31.94\n",
            "| epoch  10 |   380/  506 batches | lr 10.00 | ms/batch 128.14 | loss  3.46 | ppl    31.88\n",
            "| epoch  10 |   390/  506 batches | lr 10.00 | ms/batch 127.93 | loss  3.46 | ppl    31.77\n",
            "| epoch  10 |   400/  506 batches | lr 10.00 | ms/batch 127.99 | loss  3.43 | ppl    30.91\n",
            "| epoch  10 |   410/  506 batches | lr 10.00 | ms/batch 127.38 | loss  3.44 | ppl    31.07\n",
            "| epoch  10 |   420/  506 batches | lr 10.00 | ms/batch 128.13 | loss  3.41 | ppl    30.35\n",
            "| epoch  10 |   430/  506 batches | lr 10.00 | ms/batch 127.43 | loss  3.39 | ppl    29.57\n",
            "| epoch  10 |   440/  506 batches | lr 10.00 | ms/batch 128.17 | loss  3.45 | ppl    31.41\n",
            "| epoch  10 |   450/  506 batches | lr 10.00 | ms/batch 127.67 | loss  3.47 | ppl    32.09\n",
            "| epoch  10 |   460/  506 batches | lr 10.00 | ms/batch 127.15 | loss  3.42 | ppl    30.43\n",
            "| epoch  10 |   470/  506 batches | lr 10.00 | ms/batch 127.95 | loss  3.43 | ppl    30.88\n",
            "| epoch  10 |   480/  506 batches | lr 10.00 | ms/batch 128.50 | loss  3.43 | ppl    30.75\n",
            "| epoch  10 |   490/  506 batches | lr 10.00 | ms/batch 127.81 | loss  3.42 | ppl    30.50\n",
            "| epoch  10 |   500/  506 batches | lr 10.00 | ms/batch 127.74 | loss  3.41 | ppl    30.31\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 76.61s | valid loss  3.52 | valid ppl    33.91\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  11 |    10/  506 batches | lr 10.00 | ms/batch 140.11 | loss  3.84 | ppl    46.55\n",
            "| epoch  11 |    20/  506 batches | lr 10.00 | ms/batch 126.88 | loss  3.43 | ppl    30.75\n",
            "| epoch  11 |    30/  506 batches | lr 10.00 | ms/batch 128.01 | loss  3.41 | ppl    30.31\n",
            "| epoch  11 |    40/  506 batches | lr 10.00 | ms/batch 127.96 | loss  3.40 | ppl    29.98\n",
            "| epoch  11 |    50/  506 batches | lr 10.00 | ms/batch 127.55 | loss  3.44 | ppl    31.06\n",
            "| epoch  11 |    60/  506 batches | lr 10.00 | ms/batch 128.08 | loss  3.42 | ppl    30.62\n",
            "| epoch  11 |    70/  506 batches | lr 10.00 | ms/batch 127.45 | loss  3.43 | ppl    31.00\n",
            "| epoch  11 |    80/  506 batches | lr 10.00 | ms/batch 128.10 | loss  3.41 | ppl    30.34\n",
            "| epoch  11 |    90/  506 batches | lr 10.00 | ms/batch 127.86 | loss  3.45 | ppl    31.62\n",
            "| epoch  11 |   100/  506 batches | lr 10.00 | ms/batch 127.75 | loss  3.43 | ppl    30.77\n",
            "| epoch  11 |   110/  506 batches | lr 10.00 | ms/batch 128.29 | loss  3.38 | ppl    29.45\n",
            "| epoch  11 |   120/  506 batches | lr 10.00 | ms/batch 127.66 | loss  3.41 | ppl    30.33\n",
            "| epoch  11 |   130/  506 batches | lr 10.00 | ms/batch 127.41 | loss  3.42 | ppl    30.59\n",
            "| epoch  11 |   140/  506 batches | lr 10.00 | ms/batch 127.42 | loss  3.41 | ppl    30.28\n",
            "| epoch  11 |   150/  506 batches | lr 10.00 | ms/batch 127.55 | loss  3.42 | ppl    30.61\n",
            "| epoch  11 |   160/  506 batches | lr 10.00 | ms/batch 128.15 | loss  3.44 | ppl    31.22\n",
            "| epoch  11 |   170/  506 batches | lr 10.00 | ms/batch 128.13 | loss  3.40 | ppl    29.91\n",
            "| epoch  11 |   180/  506 batches | lr 10.00 | ms/batch 127.50 | loss  3.42 | ppl    30.58\n",
            "| epoch  11 |   190/  506 batches | lr 10.00 | ms/batch 128.97 | loss  3.44 | ppl    31.13\n",
            "| epoch  11 |   200/  506 batches | lr 10.00 | ms/batch 127.64 | loss  3.41 | ppl    30.16\n",
            "| epoch  11 |   210/  506 batches | lr 10.00 | ms/batch 128.20 | loss  3.41 | ppl    30.14\n",
            "| epoch  11 |   220/  506 batches | lr 10.00 | ms/batch 127.70 | loss  3.41 | ppl    30.21\n",
            "| epoch  11 |   230/  506 batches | lr 10.00 | ms/batch 128.14 | loss  3.41 | ppl    30.14\n",
            "| epoch  11 |   240/  506 batches | lr 10.00 | ms/batch 128.29 | loss  3.38 | ppl    29.38\n",
            "| epoch  11 |   250/  506 batches | lr 10.00 | ms/batch 128.27 | loss  3.38 | ppl    29.32\n",
            "| epoch  11 |   260/  506 batches | lr 10.00 | ms/batch 128.22 | loss  3.41 | ppl    30.34\n",
            "| epoch  11 |   270/  506 batches | lr 10.00 | ms/batch 127.21 | loss  3.39 | ppl    29.71\n",
            "| epoch  11 |   280/  506 batches | lr 10.00 | ms/batch 128.29 | loss  3.41 | ppl    30.21\n",
            "| epoch  11 |   290/  506 batches | lr 10.00 | ms/batch 127.75 | loss  3.42 | ppl    30.60\n",
            "| epoch  11 |   300/  506 batches | lr 10.00 | ms/batch 127.86 | loss  3.42 | ppl    30.67\n",
            "| epoch  11 |   310/  506 batches | lr 10.00 | ms/batch 127.90 | loss  3.41 | ppl    30.17\n",
            "| epoch  11 |   320/  506 batches | lr 10.00 | ms/batch 128.19 | loss  3.44 | ppl    31.25\n",
            "| epoch  11 |   330/  506 batches | lr 10.00 | ms/batch 128.39 | loss  3.40 | ppl    30.09\n",
            "| epoch  11 |   340/  506 batches | lr 10.00 | ms/batch 128.34 | loss  3.41 | ppl    30.20\n",
            "| epoch  11 |   350/  506 batches | lr 10.00 | ms/batch 128.02 | loss  3.42 | ppl    30.53\n",
            "| epoch  11 |   360/  506 batches | lr 10.00 | ms/batch 127.68 | loss  3.43 | ppl    30.80\n",
            "| epoch  11 |   370/  506 batches | lr 10.00 | ms/batch 127.59 | loss  3.42 | ppl    30.43\n",
            "| epoch  11 |   380/  506 batches | lr 10.00 | ms/batch 128.09 | loss  3.41 | ppl    30.13\n",
            "| epoch  11 |   390/  506 batches | lr 10.00 | ms/batch 128.20 | loss  3.40 | ppl    29.90\n",
            "| epoch  11 |   400/  506 batches | lr 10.00 | ms/batch 127.62 | loss  3.38 | ppl    29.48\n",
            "| epoch  11 |   410/  506 batches | lr 10.00 | ms/batch 128.07 | loss  3.39 | ppl    29.54\n",
            "| epoch  11 |   420/  506 batches | lr 10.00 | ms/batch 128.73 | loss  3.36 | ppl    28.70\n",
            "| epoch  11 |   430/  506 batches | lr 10.00 | ms/batch 127.86 | loss  3.33 | ppl    27.89\n",
            "| epoch  11 |   440/  506 batches | lr 10.00 | ms/batch 127.94 | loss  3.38 | ppl    29.44\n",
            "| epoch  11 |   450/  506 batches | lr 10.00 | ms/batch 128.07 | loss  3.43 | ppl    30.81\n",
            "| epoch  11 |   460/  506 batches | lr 10.00 | ms/batch 127.83 | loss  3.37 | ppl    29.19\n",
            "| epoch  11 |   470/  506 batches | lr 10.00 | ms/batch 127.99 | loss  3.37 | ppl    29.13\n",
            "| epoch  11 |   480/  506 batches | lr 10.00 | ms/batch 127.61 | loss  3.37 | ppl    29.15\n",
            "| epoch  11 |   490/  506 batches | lr 10.00 | ms/batch 127.72 | loss  3.36 | ppl    28.80\n",
            "| epoch  11 |   500/  506 batches | lr 10.00 | ms/batch 128.22 | loss  3.36 | ppl    28.82\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time: 76.70s | valid loss  3.49 | valid ppl    32.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  12 |    10/  506 batches | lr 10.00 | ms/batch 140.02 | loss  3.75 | ppl    42.69\n",
            "| epoch  12 |    20/  506 batches | lr 10.00 | ms/batch 127.53 | loss  3.38 | ppl    29.24\n",
            "| epoch  12 |    30/  506 batches | lr 10.00 | ms/batch 128.80 | loss  3.37 | ppl    29.12\n",
            "| epoch  12 |    40/  506 batches | lr 10.00 | ms/batch 127.74 | loss  3.35 | ppl    28.50\n",
            "| epoch  12 |    50/  506 batches | lr 10.00 | ms/batch 127.21 | loss  3.39 | ppl    29.55\n",
            "| epoch  12 |    60/  506 batches | lr 10.00 | ms/batch 126.80 | loss  3.38 | ppl    29.50\n",
            "| epoch  12 |    70/  506 batches | lr 10.00 | ms/batch 128.26 | loss  3.39 | ppl    29.77\n",
            "| epoch  12 |    80/  506 batches | lr 10.00 | ms/batch 127.49 | loss  3.37 | ppl    29.09\n",
            "| epoch  12 |    90/  506 batches | lr 10.00 | ms/batch 127.15 | loss  3.40 | ppl    29.87\n",
            "| epoch  12 |   100/  506 batches | lr 10.00 | ms/batch 127.08 | loss  3.38 | ppl    29.25\n",
            "| epoch  12 |   110/  506 batches | lr 10.00 | ms/batch 127.60 | loss  3.32 | ppl    27.76\n",
            "| epoch  12 |   120/  506 batches | lr 10.00 | ms/batch 127.95 | loss  3.36 | ppl    28.79\n",
            "| epoch  12 |   130/  506 batches | lr 10.00 | ms/batch 127.56 | loss  3.38 | ppl    29.44\n",
            "| epoch  12 |   140/  506 batches | lr 10.00 | ms/batch 127.37 | loss  3.36 | ppl    28.79\n",
            "| epoch  12 |   150/  506 batches | lr 10.00 | ms/batch 127.31 | loss  3.37 | ppl    29.03\n",
            "| epoch  12 |   160/  506 batches | lr 10.00 | ms/batch 127.52 | loss  3.38 | ppl    29.49\n",
            "| epoch  12 |   170/  506 batches | lr 10.00 | ms/batch 127.94 | loss  3.36 | ppl    28.73\n",
            "| epoch  12 |   180/  506 batches | lr 10.00 | ms/batch 128.24 | loss  3.37 | ppl    29.05\n",
            "| epoch  12 |   190/  506 batches | lr 10.00 | ms/batch 127.30 | loss  3.40 | ppl    29.87\n",
            "| epoch  12 |   200/  506 batches | lr 10.00 | ms/batch 127.73 | loss  3.36 | ppl    28.76\n",
            "| epoch  12 |   210/  506 batches | lr 10.00 | ms/batch 127.94 | loss  3.35 | ppl    28.54\n",
            "| epoch  12 |   220/  506 batches | lr 10.00 | ms/batch 128.02 | loss  3.36 | ppl    28.78\n",
            "| epoch  12 |   230/  506 batches | lr 10.00 | ms/batch 127.87 | loss  3.35 | ppl    28.44\n",
            "| epoch  12 |   240/  506 batches | lr 10.00 | ms/batch 128.43 | loss  3.33 | ppl    28.01\n",
            "| epoch  12 |   250/  506 batches | lr 10.00 | ms/batch 128.36 | loss  3.33 | ppl    27.98\n",
            "| epoch  12 |   260/  506 batches | lr 10.00 | ms/batch 127.36 | loss  3.35 | ppl    28.56\n",
            "| epoch  12 |   270/  506 batches | lr 10.00 | ms/batch 127.28 | loss  3.34 | ppl    28.27\n",
            "| epoch  12 |   280/  506 batches | lr 10.00 | ms/batch 127.12 | loss  3.36 | ppl    28.79\n",
            "| epoch  12 |   290/  506 batches | lr 10.00 | ms/batch 127.54 | loss  3.37 | ppl    29.12\n",
            "| epoch  12 |   300/  506 batches | lr 10.00 | ms/batch 127.91 | loss  3.38 | ppl    29.41\n",
            "| epoch  12 |   310/  506 batches | lr 10.00 | ms/batch 127.51 | loss  3.36 | ppl    28.86\n",
            "| epoch  12 |   320/  506 batches | lr 10.00 | ms/batch 128.24 | loss  3.39 | ppl    29.57\n",
            "| epoch  12 |   330/  506 batches | lr 10.00 | ms/batch 127.39 | loss  3.36 | ppl    28.71\n",
            "| epoch  12 |   340/  506 batches | lr 10.00 | ms/batch 127.59 | loss  3.36 | ppl    28.78\n",
            "| epoch  12 |   350/  506 batches | lr 10.00 | ms/batch 127.84 | loss  3.37 | ppl    28.99\n",
            "| epoch  12 |   360/  506 batches | lr 10.00 | ms/batch 127.12 | loss  3.38 | ppl    29.27\n",
            "| epoch  12 |   370/  506 batches | lr 10.00 | ms/batch 127.12 | loss  3.37 | ppl    28.98\n",
            "| epoch  12 |   380/  506 batches | lr 10.00 | ms/batch 128.38 | loss  3.36 | ppl    28.72\n",
            "| epoch  12 |   390/  506 batches | lr 10.00 | ms/batch 127.11 | loss  3.36 | ppl    28.77\n",
            "| epoch  12 |   400/  506 batches | lr 10.00 | ms/batch 127.19 | loss  3.33 | ppl    28.04\n",
            "| epoch  12 |   410/  506 batches | lr 10.00 | ms/batch 128.41 | loss  3.34 | ppl    28.21\n",
            "| epoch  12 |   420/  506 batches | lr 10.00 | ms/batch 127.48 | loss  3.31 | ppl    27.31\n",
            "| epoch  12 |   430/  506 batches | lr 10.00 | ms/batch 127.35 | loss  3.28 | ppl    26.69\n",
            "| epoch  12 |   440/  506 batches | lr 10.00 | ms/batch 127.86 | loss  3.34 | ppl    28.18\n",
            "| epoch  12 |   450/  506 batches | lr 10.00 | ms/batch 127.63 | loss  3.37 | ppl    29.16\n",
            "| epoch  12 |   460/  506 batches | lr 10.00 | ms/batch 127.88 | loss  3.33 | ppl    27.81\n",
            "| epoch  12 |   470/  506 batches | lr 10.00 | ms/batch 127.96 | loss  3.33 | ppl    28.00\n",
            "| epoch  12 |   480/  506 batches | lr 10.00 | ms/batch 127.29 | loss  3.33 | ppl    27.86\n",
            "| epoch  12 |   490/  506 batches | lr 10.00 | ms/batch 128.03 | loss  3.32 | ppl    27.63\n",
            "| epoch  12 |   500/  506 batches | lr 10.00 | ms/batch 127.26 | loss  3.32 | ppl    27.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time: 76.55s | valid loss  3.47 | valid ppl    32.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  13 |    10/  506 batches | lr 10.00 | ms/batch 139.68 | loss  3.71 | ppl    40.83\n",
            "| epoch  13 |    20/  506 batches | lr 10.00 | ms/batch 127.15 | loss  3.34 | ppl    28.16\n",
            "| epoch  13 |    30/  506 batches | lr 10.00 | ms/batch 127.60 | loss  3.32 | ppl    27.76\n",
            "| epoch  13 |    40/  506 batches | lr 10.00 | ms/batch 127.23 | loss  3.31 | ppl    27.29\n",
            "| epoch  13 |    50/  506 batches | lr 10.00 | ms/batch 127.18 | loss  3.34 | ppl    28.24\n",
            "| epoch  13 |    60/  506 batches | lr 10.00 | ms/batch 127.65 | loss  3.33 | ppl    28.06\n",
            "| epoch  13 |    70/  506 batches | lr 10.00 | ms/batch 126.86 | loss  3.34 | ppl    28.33\n",
            "| epoch  13 |    80/  506 batches | lr 10.00 | ms/batch 127.54 | loss  3.33 | ppl    27.96\n",
            "| epoch  13 |    90/  506 batches | lr 10.00 | ms/batch 127.31 | loss  3.35 | ppl    28.55\n",
            "| epoch  13 |   100/  506 batches | lr 10.00 | ms/batch 127.55 | loss  3.32 | ppl    27.74\n",
            "| epoch  13 |   110/  506 batches | lr 10.00 | ms/batch 126.59 | loss  3.29 | ppl    26.81\n",
            "| epoch  13 |   120/  506 batches | lr 10.00 | ms/batch 128.78 | loss  3.32 | ppl    27.74\n",
            "| epoch  13 |   130/  506 batches | lr 10.00 | ms/batch 127.34 | loss  3.34 | ppl    28.14\n",
            "| epoch  13 |   140/  506 batches | lr 10.00 | ms/batch 127.36 | loss  3.32 | ppl    27.62\n",
            "| epoch  13 |   150/  506 batches | lr 10.00 | ms/batch 127.19 | loss  3.33 | ppl    27.93\n",
            "| epoch  13 |   160/  506 batches | lr 10.00 | ms/batch 127.72 | loss  3.34 | ppl    28.14\n",
            "| epoch  13 |   170/  506 batches | lr 10.00 | ms/batch 128.15 | loss  3.31 | ppl    27.42\n",
            "| epoch  13 |   180/  506 batches | lr 10.00 | ms/batch 126.61 | loss  3.33 | ppl    27.84\n",
            "| epoch  13 |   190/  506 batches | lr 10.00 | ms/batch 127.77 | loss  3.34 | ppl    28.09\n",
            "| epoch  13 |   200/  506 batches | lr 10.00 | ms/batch 127.36 | loss  3.31 | ppl    27.49\n",
            "| epoch  13 |   210/  506 batches | lr 10.00 | ms/batch 127.95 | loss  3.31 | ppl    27.43\n",
            "| epoch  13 |   220/  506 batches | lr 10.00 | ms/batch 127.75 | loss  3.31 | ppl    27.49\n",
            "| epoch  13 |   230/  506 batches | lr 10.00 | ms/batch 127.26 | loss  3.30 | ppl    27.06\n",
            "| epoch  13 |   240/  506 batches | lr 10.00 | ms/batch 127.99 | loss  3.29 | ppl    26.77\n",
            "| epoch  13 |   250/  506 batches | lr 10.00 | ms/batch 127.78 | loss  3.28 | ppl    26.65\n",
            "| epoch  13 |   260/  506 batches | lr 10.00 | ms/batch 128.68 | loss  3.32 | ppl    27.59\n",
            "| epoch  13 |   270/  506 batches | lr 10.00 | ms/batch 127.41 | loss  3.30 | ppl    27.22\n",
            "| epoch  13 |   280/  506 batches | lr 10.00 | ms/batch 127.61 | loss  3.32 | ppl    27.62\n",
            "| epoch  13 |   290/  506 batches | lr 10.00 | ms/batch 127.99 | loss  3.32 | ppl    27.78\n",
            "| epoch  13 |   300/  506 batches | lr 10.00 | ms/batch 128.21 | loss  3.33 | ppl    28.00\n",
            "| epoch  13 |   310/  506 batches | lr 10.00 | ms/batch 127.99 | loss  3.31 | ppl    27.39\n",
            "| epoch  13 |   320/  506 batches | lr 10.00 | ms/batch 128.12 | loss  3.35 | ppl    28.40\n",
            "| epoch  13 |   330/  506 batches | lr 10.00 | ms/batch 127.62 | loss  3.31 | ppl    27.37\n",
            "| epoch  13 |   340/  506 batches | lr 10.00 | ms/batch 127.74 | loss  3.32 | ppl    27.65\n",
            "| epoch  13 |   350/  506 batches | lr 10.00 | ms/batch 127.72 | loss  3.33 | ppl    27.85\n",
            "| epoch  13 |   360/  506 batches | lr 10.00 | ms/batch 127.88 | loss  3.33 | ppl    28.01\n",
            "| epoch  13 |   370/  506 batches | lr 10.00 | ms/batch 127.94 | loss  3.33 | ppl    27.99\n",
            "| epoch  13 |   380/  506 batches | lr 10.00 | ms/batch 127.62 | loss  3.32 | ppl    27.56\n",
            "| epoch  13 |   390/  506 batches | lr 10.00 | ms/batch 127.59 | loss  3.32 | ppl    27.58\n",
            "| epoch  13 |   400/  506 batches | lr 10.00 | ms/batch 127.58 | loss  3.30 | ppl    27.13\n",
            "| epoch  13 |   410/  506 batches | lr 10.00 | ms/batch 128.31 | loss  3.29 | ppl    26.77\n",
            "| epoch  13 |   420/  506 batches | lr 10.00 | ms/batch 127.94 | loss  3.26 | ppl    26.13\n",
            "| epoch  13 |   430/  506 batches | lr 10.00 | ms/batch 127.04 | loss  3.24 | ppl    25.47\n",
            "| epoch  13 |   440/  506 batches | lr 10.00 | ms/batch 127.49 | loss  3.30 | ppl    27.06\n",
            "| epoch  13 |   450/  506 batches | lr 10.00 | ms/batch 127.43 | loss  3.32 | ppl    27.74\n",
            "| epoch  13 |   460/  506 batches | lr 10.00 | ms/batch 127.50 | loss  3.28 | ppl    26.54\n",
            "| epoch  13 |   470/  506 batches | lr 10.00 | ms/batch 127.12 | loss  3.29 | ppl    26.73\n",
            "| epoch  13 |   480/  506 batches | lr 10.00 | ms/batch 127.52 | loss  3.29 | ppl    26.91\n",
            "| epoch  13 |   490/  506 batches | lr 10.00 | ms/batch 128.63 | loss  3.27 | ppl    26.41\n",
            "| epoch  13 |   500/  506 batches | lr 10.00 | ms/batch 128.14 | loss  3.28 | ppl    26.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time: 76.54s | valid loss  3.45 | valid ppl    31.49\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  14 |    10/  506 batches | lr 10.00 | ms/batch 138.86 | loss  3.65 | ppl    38.62\n",
            "| epoch  14 |    20/  506 batches | lr 10.00 | ms/batch 127.59 | loss  3.30 | ppl    27.10\n",
            "| epoch  14 |    30/  506 batches | lr 10.00 | ms/batch 127.99 | loss  3.28 | ppl    26.65\n",
            "| epoch  14 |    40/  506 batches | lr 10.00 | ms/batch 126.69 | loss  3.26 | ppl    26.13\n",
            "| epoch  14 |    50/  506 batches | lr 10.00 | ms/batch 128.00 | loss  3.30 | ppl    26.99\n",
            "| epoch  14 |    60/  506 batches | lr 10.00 | ms/batch 126.89 | loss  3.30 | ppl    27.06\n",
            "| epoch  14 |    70/  506 batches | lr 10.00 | ms/batch 126.95 | loss  3.31 | ppl    27.32\n",
            "| epoch  14 |    80/  506 batches | lr 10.00 | ms/batch 128.38 | loss  3.28 | ppl    26.59\n",
            "| epoch  14 |    90/  506 batches | lr 10.00 | ms/batch 127.75 | loss  3.30 | ppl    27.24\n",
            "| epoch  14 |   100/  506 batches | lr 10.00 | ms/batch 127.65 | loss  3.29 | ppl    26.79\n",
            "| epoch  14 |   110/  506 batches | lr 10.00 | ms/batch 127.53 | loss  3.25 | ppl    25.66\n",
            "| epoch  14 |   120/  506 batches | lr 10.00 | ms/batch 128.04 | loss  3.27 | ppl    26.43\n",
            "| epoch  14 |   130/  506 batches | lr 10.00 | ms/batch 128.35 | loss  3.29 | ppl    26.89\n",
            "| epoch  14 |   140/  506 batches | lr 10.00 | ms/batch 127.61 | loss  3.28 | ppl    26.59\n",
            "| epoch  14 |   150/  506 batches | lr 10.00 | ms/batch 128.49 | loss  3.28 | ppl    26.56\n",
            "| epoch  14 |   160/  506 batches | lr 10.00 | ms/batch 127.90 | loss  3.30 | ppl    27.04\n",
            "| epoch  14 |   170/  506 batches | lr 10.00 | ms/batch 127.76 | loss  3.28 | ppl    26.49\n",
            "| epoch  14 |   180/  506 batches | lr 10.00 | ms/batch 127.68 | loss  3.28 | ppl    26.57\n",
            "| epoch  14 |   190/  506 batches | lr 10.00 | ms/batch 128.71 | loss  3.30 | ppl    27.12\n",
            "| epoch  14 |   200/  506 batches | lr 10.00 | ms/batch 128.03 | loss  3.28 | ppl    26.44\n",
            "| epoch  14 |   210/  506 batches | lr 10.00 | ms/batch 128.23 | loss  3.27 | ppl    26.40\n",
            "| epoch  14 |   220/  506 batches | lr 10.00 | ms/batch 127.97 | loss  3.28 | ppl    26.55\n",
            "| epoch  14 |   230/  506 batches | lr 10.00 | ms/batch 128.20 | loss  3.26 | ppl    26.16\n",
            "| epoch  14 |   240/  506 batches | lr 10.00 | ms/batch 127.84 | loss  3.25 | ppl    25.74\n",
            "| epoch  14 |   250/  506 batches | lr 10.00 | ms/batch 127.98 | loss  3.25 | ppl    25.77\n",
            "| epoch  14 |   260/  506 batches | lr 10.00 | ms/batch 127.78 | loss  3.27 | ppl    26.35\n",
            "| epoch  14 |   270/  506 batches | lr 10.00 | ms/batch 128.32 | loss  3.26 | ppl    26.15\n",
            "| epoch  14 |   280/  506 batches | lr 10.00 | ms/batch 127.56 | loss  3.27 | ppl    26.44\n",
            "| epoch  14 |   290/  506 batches | lr 10.00 | ms/batch 127.78 | loss  3.28 | ppl    26.70\n",
            "| epoch  14 |   300/  506 batches | lr 10.00 | ms/batch 128.36 | loss  3.30 | ppl    27.08\n",
            "| epoch  14 |   310/  506 batches | lr 10.00 | ms/batch 128.22 | loss  3.27 | ppl    26.34\n",
            "| epoch  14 |   320/  506 batches | lr 10.00 | ms/batch 128.15 | loss  3.30 | ppl    27.24\n",
            "| epoch  14 |   330/  506 batches | lr 10.00 | ms/batch 127.81 | loss  3.27 | ppl    26.36\n",
            "| epoch  14 |   340/  506 batches | lr 10.00 | ms/batch 126.89 | loss  3.28 | ppl    26.66\n",
            "| epoch  14 |   350/  506 batches | lr 10.00 | ms/batch 127.48 | loss  3.28 | ppl    26.63\n",
            "| epoch  14 |   360/  506 batches | lr 10.00 | ms/batch 126.76 | loss  3.29 | ppl    26.87\n",
            "| epoch  14 |   370/  506 batches | lr 10.00 | ms/batch 127.78 | loss  3.28 | ppl    26.65\n",
            "| epoch  14 |   380/  506 batches | lr 10.00 | ms/batch 127.83 | loss  3.27 | ppl    26.42\n",
            "| epoch  14 |   390/  506 batches | lr 10.00 | ms/batch 127.56 | loss  3.28 | ppl    26.52\n",
            "| epoch  14 |   400/  506 batches | lr 10.00 | ms/batch 127.40 | loss  3.25 | ppl    25.86\n",
            "| epoch  14 |   410/  506 batches | lr 10.00 | ms/batch 127.03 | loss  3.26 | ppl    25.95\n",
            "| epoch  14 |   420/  506 batches | lr 10.00 | ms/batch 128.31 | loss  3.22 | ppl    24.95\n",
            "| epoch  14 |   430/  506 batches | lr 10.00 | ms/batch 128.12 | loss  3.21 | ppl    24.72\n",
            "| epoch  14 |   440/  506 batches | lr 10.00 | ms/batch 128.79 | loss  3.25 | ppl    25.90\n",
            "| epoch  14 |   450/  506 batches | lr 10.00 | ms/batch 127.58 | loss  3.28 | ppl    26.69\n",
            "| epoch  14 |   460/  506 batches | lr 10.00 | ms/batch 128.30 | loss  3.24 | ppl    25.55\n",
            "| epoch  14 |   470/  506 batches | lr 10.00 | ms/batch 128.44 | loss  3.25 | ppl    25.74\n",
            "| epoch  14 |   480/  506 batches | lr 10.00 | ms/batch 128.43 | loss  3.25 | ppl    25.91\n",
            "| epoch  14 |   490/  506 batches | lr 10.00 | ms/batch 128.10 | loss  3.23 | ppl    25.35\n",
            "| epoch  14 |   500/  506 batches | lr 10.00 | ms/batch 128.31 | loss  3.23 | ppl    25.35\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time: 76.65s | valid loss  3.44 | valid ppl    31.12\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  15 |    10/  506 batches | lr 10.00 | ms/batch 139.51 | loss  3.61 | ppl    36.86\n",
            "| epoch  15 |    20/  506 batches | lr 10.00 | ms/batch 128.32 | loss  3.25 | ppl    25.80\n",
            "| epoch  15 |    30/  506 batches | lr 10.00 | ms/batch 128.79 | loss  3.25 | ppl    25.67\n",
            "| epoch  15 |    40/  506 batches | lr 10.00 | ms/batch 127.68 | loss  3.23 | ppl    25.20\n",
            "| epoch  15 |    50/  506 batches | lr 10.00 | ms/batch 127.89 | loss  3.26 | ppl    26.00\n",
            "| epoch  15 |    60/  506 batches | lr 10.00 | ms/batch 127.80 | loss  3.26 | ppl    25.95\n",
            "| epoch  15 |    70/  506 batches | lr 10.00 | ms/batch 128.80 | loss  3.27 | ppl    26.42\n",
            "| epoch  15 |    80/  506 batches | lr 10.00 | ms/batch 127.93 | loss  3.24 | ppl    25.66\n",
            "| epoch  15 |    90/  506 batches | lr 10.00 | ms/batch 127.81 | loss  3.27 | ppl    26.27\n",
            "| epoch  15 |   100/  506 batches | lr 10.00 | ms/batch 127.88 | loss  3.25 | ppl    25.77\n",
            "| epoch  15 |   110/  506 batches | lr 10.00 | ms/batch 127.19 | loss  3.21 | ppl    24.74\n",
            "| epoch  15 |   120/  506 batches | lr 10.00 | ms/batch 128.22 | loss  3.24 | ppl    25.50\n",
            "| epoch  15 |   130/  506 batches | lr 10.00 | ms/batch 127.95 | loss  3.26 | ppl    26.10\n",
            "| epoch  15 |   140/  506 batches | lr 10.00 | ms/batch 127.73 | loss  3.24 | ppl    25.48\n",
            "| epoch  15 |   150/  506 batches | lr 10.00 | ms/batch 127.65 | loss  3.24 | ppl    25.64\n",
            "| epoch  15 |   160/  506 batches | lr 10.00 | ms/batch 128.61 | loss  3.26 | ppl    25.93\n",
            "| epoch  15 |   170/  506 batches | lr 10.00 | ms/batch 128.18 | loss  3.23 | ppl    25.26\n",
            "| epoch  15 |   180/  506 batches | lr 10.00 | ms/batch 127.90 | loss  3.25 | ppl    25.73\n",
            "| epoch  15 |   190/  506 batches | lr 10.00 | ms/batch 127.67 | loss  3.26 | ppl    26.04\n",
            "| epoch  15 |   200/  506 batches | lr 10.00 | ms/batch 128.93 | loss  3.23 | ppl    25.25\n",
            "| epoch  15 |   210/  506 batches | lr 10.00 | ms/batch 127.65 | loss  3.24 | ppl    25.52\n",
            "| epoch  15 |   220/  506 batches | lr 10.00 | ms/batch 127.06 | loss  3.25 | ppl    25.70\n",
            "| epoch  15 |   230/  506 batches | lr 10.00 | ms/batch 128.03 | loss  3.23 | ppl    25.25\n",
            "| epoch  15 |   240/  506 batches | lr 10.00 | ms/batch 127.85 | loss  3.22 | ppl    25.03\n",
            "| epoch  15 |   250/  506 batches | lr 10.00 | ms/batch 127.21 | loss  3.21 | ppl    24.71\n",
            "| epoch  15 |   260/  506 batches | lr 10.00 | ms/batch 127.76 | loss  3.24 | ppl    25.42\n",
            "| epoch  15 |   270/  506 batches | lr 10.00 | ms/batch 128.33 | loss  3.22 | ppl    25.12\n",
            "| epoch  15 |   280/  506 batches | lr 10.00 | ms/batch 128.03 | loss  3.24 | ppl    25.58\n",
            "| epoch  15 |   290/  506 batches | lr 10.00 | ms/batch 127.03 | loss  3.26 | ppl    25.94\n",
            "| epoch  15 |   300/  506 batches | lr 10.00 | ms/batch 128.20 | loss  3.26 | ppl    26.13\n",
            "| epoch  15 |   310/  506 batches | lr 10.00 | ms/batch 128.34 | loss  3.23 | ppl    25.33\n",
            "| epoch  15 |   320/  506 batches | lr 10.00 | ms/batch 127.35 | loss  3.27 | ppl    26.26\n",
            "| epoch  15 |   330/  506 batches | lr 10.00 | ms/batch 128.35 | loss  3.24 | ppl    25.46\n",
            "| epoch  15 |   340/  506 batches | lr 10.00 | ms/batch 128.24 | loss  3.24 | ppl    25.60\n",
            "| epoch  15 |   350/  506 batches | lr 10.00 | ms/batch 128.09 | loss  3.25 | ppl    25.76\n",
            "| epoch  15 |   360/  506 batches | lr 10.00 | ms/batch 127.11 | loss  3.25 | ppl    25.91\n",
            "| epoch  15 |   370/  506 batches | lr 10.00 | ms/batch 128.18 | loss  3.25 | ppl    25.90\n",
            "| epoch  15 |   380/  506 batches | lr 10.00 | ms/batch 127.70 | loss  3.24 | ppl    25.63\n",
            "| epoch  15 |   390/  506 batches | lr 10.00 | ms/batch 128.36 | loss  3.26 | ppl    25.99\n",
            "| epoch  15 |   400/  506 batches | lr 10.00 | ms/batch 127.46 | loss  3.22 | ppl    24.97\n",
            "| epoch  15 |   410/  506 batches | lr 10.00 | ms/batch 128.42 | loss  3.22 | ppl    25.01\n",
            "| epoch  15 |   420/  506 batches | lr 10.00 | ms/batch 127.81 | loss  3.19 | ppl    24.33\n",
            "| epoch  15 |   430/  506 batches | lr 10.00 | ms/batch 127.33 | loss  3.17 | ppl    23.89\n",
            "| epoch  15 |   440/  506 batches | lr 10.00 | ms/batch 127.58 | loss  3.22 | ppl    25.04\n",
            "| epoch  15 |   450/  506 batches | lr 10.00 | ms/batch 127.98 | loss  3.25 | ppl    25.73\n",
            "| epoch  15 |   460/  506 batches | lr 10.00 | ms/batch 128.61 | loss  3.20 | ppl    24.63\n",
            "| epoch  15 |   470/  506 batches | lr 10.00 | ms/batch 127.79 | loss  3.22 | ppl    25.04\n",
            "| epoch  15 |   480/  506 batches | lr 10.00 | ms/batch 127.57 | loss  3.21 | ppl    24.86\n",
            "| epoch  15 |   490/  506 batches | lr 10.00 | ms/batch 127.81 | loss  3.21 | ppl    24.70\n",
            "| epoch  15 |   500/  506 batches | lr 10.00 | ms/batch 127.68 | loss  3.20 | ppl    24.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time: 76.69s | valid loss  3.42 | valid ppl    30.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  16 |    10/  506 batches | lr 10.00 | ms/batch 139.25 | loss  3.57 | ppl    35.60\n",
            "| epoch  16 |    20/  506 batches | lr 10.00 | ms/batch 127.84 | loss  3.21 | ppl    24.88\n",
            "| epoch  16 |    30/  506 batches | lr 10.00 | ms/batch 127.90 | loss  3.21 | ppl    24.82\n",
            "| epoch  16 |    40/  506 batches | lr 10.00 | ms/batch 128.07 | loss  3.19 | ppl    24.36\n",
            "| epoch  16 |    50/  506 batches | lr 10.00 | ms/batch 127.95 | loss  3.22 | ppl    25.11\n",
            "| epoch  16 |    60/  506 batches | lr 10.00 | ms/batch 127.93 | loss  3.22 | ppl    24.94\n",
            "| epoch  16 |    70/  506 batches | lr 10.00 | ms/batch 128.11 | loss  3.23 | ppl    25.25\n",
            "| epoch  16 |    80/  506 batches | lr 10.00 | ms/batch 126.46 | loss  3.21 | ppl    24.85\n",
            "| epoch  16 |    90/  506 batches | lr 10.00 | ms/batch 128.27 | loss  3.23 | ppl    25.30\n",
            "| epoch  16 |   100/  506 batches | lr 10.00 | ms/batch 126.57 | loss  3.21 | ppl    24.87\n",
            "| epoch  16 |   110/  506 batches | lr 10.00 | ms/batch 127.01 | loss  3.17 | ppl    23.85\n",
            "| epoch  16 |   120/  506 batches | lr 10.00 | ms/batch 127.11 | loss  3.20 | ppl    24.59\n",
            "| epoch  16 |   130/  506 batches | lr 10.00 | ms/batch 127.66 | loss  3.22 | ppl    24.96\n",
            "| epoch  16 |   140/  506 batches | lr 10.00 | ms/batch 126.84 | loss  3.20 | ppl    24.65\n",
            "| epoch  16 |   150/  506 batches | lr 10.00 | ms/batch 126.96 | loss  3.21 | ppl    24.72\n",
            "| epoch  16 |   160/  506 batches | lr 10.00 | ms/batch 127.91 | loss  3.22 | ppl    24.90\n",
            "| epoch  16 |   170/  506 batches | lr 10.00 | ms/batch 126.85 | loss  3.20 | ppl    24.43\n",
            "| epoch  16 |   180/  506 batches | lr 10.00 | ms/batch 127.42 | loss  3.21 | ppl    24.81\n",
            "| epoch  16 |   190/  506 batches | lr 10.00 | ms/batch 127.15 | loss  3.22 | ppl    24.98\n",
            "| epoch  16 |   200/  506 batches | lr 10.00 | ms/batch 126.91 | loss  3.20 | ppl    24.49\n",
            "| epoch  16 |   210/  506 batches | lr 10.00 | ms/batch 127.88 | loss  3.20 | ppl    24.56\n",
            "| epoch  16 |   220/  506 batches | lr 10.00 | ms/batch 127.17 | loss  3.21 | ppl    24.75\n",
            "| epoch  16 |   230/  506 batches | lr 10.00 | ms/batch 127.59 | loss  3.19 | ppl    24.35\n",
            "| epoch  16 |   240/  506 batches | lr 10.00 | ms/batch 127.19 | loss  3.18 | ppl    23.94\n",
            "| epoch  16 |   250/  506 batches | lr 10.00 | ms/batch 127.35 | loss  3.17 | ppl    23.87\n",
            "| epoch  16 |   260/  506 batches | lr 10.00 | ms/batch 126.84 | loss  3.20 | ppl    24.46\n",
            "| epoch  16 |   270/  506 batches | lr 10.00 | ms/batch 127.08 | loss  3.20 | ppl    24.43\n",
            "| epoch  16 |   280/  506 batches | lr 10.00 | ms/batch 127.57 | loss  3.20 | ppl    24.61\n",
            "| epoch  16 |   290/  506 batches | lr 10.00 | ms/batch 126.46 | loss  3.21 | ppl    24.88\n",
            "| epoch  16 |   300/  506 batches | lr 10.00 | ms/batch 126.70 | loss  3.23 | ppl    25.21\n",
            "| epoch  16 |   310/  506 batches | lr 10.00 | ms/batch 127.00 | loss  3.20 | ppl    24.48\n",
            "| epoch  16 |   320/  506 batches | lr 10.00 | ms/batch 126.65 | loss  3.23 | ppl    25.21\n",
            "| epoch  16 |   330/  506 batches | lr 10.00 | ms/batch 126.73 | loss  3.20 | ppl    24.61\n",
            "| epoch  16 |   340/  506 batches | lr 10.00 | ms/batch 126.11 | loss  3.21 | ppl    24.81\n",
            "| epoch  16 |   350/  506 batches | lr 10.00 | ms/batch 126.24 | loss  3.21 | ppl    24.86\n",
            "| epoch  16 |   360/  506 batches | lr 10.00 | ms/batch 126.84 | loss  3.23 | ppl    25.16\n",
            "| epoch  16 |   370/  506 batches | lr 10.00 | ms/batch 126.21 | loss  3.21 | ppl    24.89\n",
            "| epoch  16 |   380/  506 batches | lr 10.00 | ms/batch 126.58 | loss  3.21 | ppl    24.82\n",
            "| epoch  16 |   390/  506 batches | lr 10.00 | ms/batch 126.47 | loss  3.20 | ppl    24.51\n",
            "| epoch  16 |   400/  506 batches | lr 10.00 | ms/batch 127.42 | loss  3.19 | ppl    24.18\n",
            "| epoch  16 |   410/  506 batches | lr 10.00 | ms/batch 126.75 | loss  3.18 | ppl    24.05\n",
            "| epoch  16 |   420/  506 batches | lr 10.00 | ms/batch 126.17 | loss  3.16 | ppl    23.48\n",
            "| epoch  16 |   430/  506 batches | lr 10.00 | ms/batch 127.64 | loss  3.13 | ppl    22.98\n",
            "| epoch  16 |   440/  506 batches | lr 10.00 | ms/batch 127.13 | loss  3.18 | ppl    24.05\n",
            "| epoch  16 |   450/  506 batches | lr 10.00 | ms/batch 127.89 | loss  3.21 | ppl    24.83\n",
            "| epoch  16 |   460/  506 batches | lr 10.00 | ms/batch 128.07 | loss  3.17 | ppl    23.86\n",
            "| epoch  16 |   470/  506 batches | lr 10.00 | ms/batch 127.82 | loss  3.18 | ppl    24.10\n",
            "| epoch  16 |   480/  506 batches | lr 10.00 | ms/batch 128.15 | loss  3.18 | ppl    23.97\n",
            "| epoch  16 |   490/  506 batches | lr 10.00 | ms/batch 127.84 | loss  3.16 | ppl    23.65\n",
            "| epoch  16 |   500/  506 batches | lr 10.00 | ms/batch 128.11 | loss  3.16 | ppl    23.69\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time: 76.36s | valid loss  3.41 | valid ppl    30.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  17 |    10/  506 batches | lr 10.00 | ms/batch 140.54 | loss  3.54 | ppl    34.43\n",
            "| epoch  17 |    20/  506 batches | lr 10.00 | ms/batch 127.80 | loss  3.18 | ppl    24.04\n",
            "| epoch  17 |    30/  506 batches | lr 10.00 | ms/batch 129.78 | loss  3.17 | ppl    23.91\n",
            "| epoch  17 |    40/  506 batches | lr 10.00 | ms/batch 127.65 | loss  3.15 | ppl    23.30\n",
            "| epoch  17 |    50/  506 batches | lr 10.00 | ms/batch 127.82 | loss  3.20 | ppl    24.43\n",
            "| epoch  17 |    60/  506 batches | lr 10.00 | ms/batch 127.53 | loss  3.19 | ppl    24.36\n",
            "| epoch  17 |    70/  506 batches | lr 10.00 | ms/batch 128.14 | loss  3.20 | ppl    24.41\n",
            "| epoch  17 |    80/  506 batches | lr 10.00 | ms/batch 126.70 | loss  3.18 | ppl    24.04\n",
            "| epoch  17 |    90/  506 batches | lr 10.00 | ms/batch 127.50 | loss  3.19 | ppl    24.31\n",
            "| epoch  17 |   100/  506 batches | lr 10.00 | ms/batch 127.39 | loss  3.17 | ppl    23.91\n",
            "| epoch  17 |   110/  506 batches | lr 10.00 | ms/batch 127.05 | loss  3.13 | ppl    22.97\n",
            "| epoch  17 |   120/  506 batches | lr 10.00 | ms/batch 127.55 | loss  3.17 | ppl    23.75\n",
            "| epoch  17 |   130/  506 batches | lr 10.00 | ms/batch 127.83 | loss  3.18 | ppl    24.15\n",
            "| epoch  17 |   140/  506 batches | lr 10.00 | ms/batch 127.10 | loss  3.16 | ppl    23.66\n",
            "| epoch  17 |   150/  506 batches | lr 10.00 | ms/batch 127.03 | loss  3.17 | ppl    23.79\n",
            "| epoch  17 |   160/  506 batches | lr 10.00 | ms/batch 127.40 | loss  3.18 | ppl    24.08\n",
            "| epoch  17 |   170/  506 batches | lr 10.00 | ms/batch 127.10 | loss  3.16 | ppl    23.61\n",
            "| epoch  17 |   180/  506 batches | lr 10.00 | ms/batch 127.39 | loss  3.17 | ppl    23.76\n",
            "| epoch  17 |   190/  506 batches | lr 10.00 | ms/batch 126.24 | loss  3.19 | ppl    24.25\n",
            "| epoch  17 |   200/  506 batches | lr 10.00 | ms/batch 126.91 | loss  3.17 | ppl    23.73\n",
            "| epoch  17 |   210/  506 batches | lr 10.00 | ms/batch 127.57 | loss  3.17 | ppl    23.79\n",
            "| epoch  17 |   220/  506 batches | lr 10.00 | ms/batch 127.22 | loss  3.18 | ppl    23.93\n",
            "| epoch  17 |   230/  506 batches | lr 10.00 | ms/batch 127.13 | loss  3.16 | ppl    23.56\n",
            "| epoch  17 |   240/  506 batches | lr 10.00 | ms/batch 126.84 | loss  3.14 | ppl    23.18\n",
            "| epoch  17 |   250/  506 batches | lr 10.00 | ms/batch 126.41 | loss  3.13 | ppl    22.81\n",
            "| epoch  17 |   260/  506 batches | lr 10.00 | ms/batch 127.29 | loss  3.16 | ppl    23.62\n",
            "| epoch  17 |   270/  506 batches | lr 10.00 | ms/batch 126.80 | loss  3.15 | ppl    23.44\n",
            "| epoch  17 |   280/  506 batches | lr 10.00 | ms/batch 126.90 | loss  3.17 | ppl    23.80\n",
            "| epoch  17 |   290/  506 batches | lr 10.00 | ms/batch 126.97 | loss  3.18 | ppl    24.11\n",
            "| epoch  17 |   300/  506 batches | lr 10.00 | ms/batch 126.66 | loss  3.19 | ppl    24.27\n",
            "| epoch  17 |   310/  506 batches | lr 10.00 | ms/batch 127.33 | loss  3.16 | ppl    23.62\n",
            "| epoch  17 |   320/  506 batches | lr 10.00 | ms/batch 127.28 | loss  3.19 | ppl    24.22\n",
            "| epoch  17 |   330/  506 batches | lr 10.00 | ms/batch 126.41 | loss  3.16 | ppl    23.57\n",
            "| epoch  17 |   340/  506 batches | lr 10.00 | ms/batch 127.36 | loss  3.17 | ppl    23.85\n",
            "| epoch  17 |   350/  506 batches | lr 10.00 | ms/batch 127.35 | loss  3.18 | ppl    24.14\n",
            "| epoch  17 |   360/  506 batches | lr 10.00 | ms/batch 127.30 | loss  3.19 | ppl    24.26\n",
            "| epoch  17 |   370/  506 batches | lr 10.00 | ms/batch 127.37 | loss  3.17 | ppl    23.85\n",
            "| epoch  17 |   380/  506 batches | lr 10.00 | ms/batch 128.36 | loss  3.17 | ppl    23.91\n",
            "| epoch  17 |   390/  506 batches | lr 10.00 | ms/batch 127.30 | loss  3.16 | ppl    23.48\n",
            "| epoch  17 |   400/  506 batches | lr 10.00 | ms/batch 128.24 | loss  3.15 | ppl    23.36\n",
            "| epoch  17 |   410/  506 batches | lr 10.00 | ms/batch 127.48 | loss  3.15 | ppl    23.26\n",
            "| epoch  17 |   420/  506 batches | lr 10.00 | ms/batch 127.45 | loss  3.12 | ppl    22.62\n",
            "| epoch  17 |   430/  506 batches | lr 10.00 | ms/batch 128.09 | loss  3.11 | ppl    22.33\n",
            "| epoch  17 |   440/  506 batches | lr 10.00 | ms/batch 128.13 | loss  3.15 | ppl    23.42\n",
            "| epoch  17 |   450/  506 batches | lr 10.00 | ms/batch 128.21 | loss  3.17 | ppl    23.89\n",
            "| epoch  17 |   460/  506 batches | lr 10.00 | ms/batch 128.18 | loss  3.13 | ppl    22.93\n",
            "| epoch  17 |   470/  506 batches | lr 10.00 | ms/batch 128.69 | loss  3.14 | ppl    23.18\n",
            "| epoch  17 |   480/  506 batches | lr 10.00 | ms/batch 129.23 | loss  3.14 | ppl    23.18\n",
            "| epoch  17 |   490/  506 batches | lr 10.00 | ms/batch 128.22 | loss  3.13 | ppl    22.85\n",
            "| epoch  17 |   500/  506 batches | lr 10.00 | ms/batch 128.70 | loss  3.13 | ppl    22.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time: 76.52s | valid loss  3.40 | valid ppl    29.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  18 |    10/  506 batches | lr 10.00 | ms/batch 140.20 | loss  3.51 | ppl    33.56\n",
            "| epoch  18 |    20/  506 batches | lr 10.00 | ms/batch 127.51 | loss  3.15 | ppl    23.30\n",
            "| epoch  18 |    30/  506 batches | lr 10.00 | ms/batch 127.31 | loss  3.14 | ppl    23.04\n",
            "| epoch  18 |    40/  506 batches | lr 10.00 | ms/batch 127.09 | loss  3.13 | ppl    22.76\n",
            "| epoch  18 |    50/  506 batches | lr 10.00 | ms/batch 126.97 | loss  3.15 | ppl    23.38\n",
            "| epoch  18 |    60/  506 batches | lr 10.00 | ms/batch 127.13 | loss  3.16 | ppl    23.53\n",
            "| epoch  18 |    70/  506 batches | lr 10.00 | ms/batch 127.85 | loss  3.16 | ppl    23.67\n",
            "| epoch  18 |    80/  506 batches | lr 10.00 | ms/batch 127.29 | loss  3.14 | ppl    23.19\n",
            "| epoch  18 |    90/  506 batches | lr 10.00 | ms/batch 126.53 | loss  3.15 | ppl    23.44\n",
            "| epoch  18 |   100/  506 batches | lr 10.00 | ms/batch 127.17 | loss  3.14 | ppl    23.06\n",
            "| epoch  18 |   110/  506 batches | lr 10.00 | ms/batch 126.98 | loss  3.10 | ppl    22.20\n",
            "| epoch  18 |   120/  506 batches | lr 10.00 | ms/batch 127.36 | loss  3.13 | ppl    22.96\n",
            "| epoch  18 |   130/  506 batches | lr 10.00 | ms/batch 127.30 | loss  3.15 | ppl    23.28\n",
            "| epoch  18 |   140/  506 batches | lr 10.00 | ms/batch 126.71 | loss  3.13 | ppl    22.79\n",
            "| epoch  18 |   150/  506 batches | lr 10.00 | ms/batch 126.20 | loss  3.14 | ppl    23.15\n",
            "| epoch  18 |   160/  506 batches | lr 10.00 | ms/batch 127.27 | loss  3.14 | ppl    23.18\n",
            "| epoch  18 |   170/  506 batches | lr 10.00 | ms/batch 126.96 | loss  3.12 | ppl    22.74\n",
            "| epoch  18 |   180/  506 batches | lr 10.00 | ms/batch 126.87 | loss  3.14 | ppl    23.05\n",
            "| epoch  18 |   190/  506 batches | lr 10.00 | ms/batch 126.68 | loss  3.15 | ppl    23.26\n",
            "| epoch  18 |   200/  506 batches | lr 10.00 | ms/batch 126.25 | loss  3.13 | ppl    22.79\n",
            "| epoch  18 |   210/  506 batches | lr 10.00 | ms/batch 126.48 | loss  3.13 | ppl    22.87\n",
            "| epoch  18 |   220/  506 batches | lr 10.00 | ms/batch 126.25 | loss  3.14 | ppl    23.12\n",
            "| epoch  18 |   230/  506 batches | lr 10.00 | ms/batch 126.67 | loss  3.12 | ppl    22.66\n",
            "| epoch  18 |   240/  506 batches | lr 10.00 | ms/batch 126.43 | loss  3.11 | ppl    22.39\n",
            "| epoch  18 |   250/  506 batches | lr 10.00 | ms/batch 127.11 | loss  3.10 | ppl    22.29\n",
            "| epoch  18 |   260/  506 batches | lr 10.00 | ms/batch 127.47 | loss  3.13 | ppl    22.95\n",
            "| epoch  18 |   270/  506 batches | lr 10.00 | ms/batch 127.34 | loss  3.13 | ppl    22.79\n",
            "| epoch  18 |   280/  506 batches | lr 10.00 | ms/batch 127.71 | loss  3.14 | ppl    23.16\n",
            "| epoch  18 |   290/  506 batches | lr 10.00 | ms/batch 126.92 | loss  3.15 | ppl    23.29\n",
            "| epoch  18 |   300/  506 batches | lr 10.00 | ms/batch 127.84 | loss  3.15 | ppl    23.44\n",
            "| epoch  18 |   310/  506 batches | lr 10.00 | ms/batch 128.22 | loss  3.12 | ppl    22.68\n",
            "| epoch  18 |   320/  506 batches | lr 10.00 | ms/batch 127.28 | loss  3.16 | ppl    23.62\n",
            "| epoch  18 |   330/  506 batches | lr 10.00 | ms/batch 127.40 | loss  3.14 | ppl    23.04\n",
            "| epoch  18 |   340/  506 batches | lr 10.00 | ms/batch 127.31 | loss  3.14 | ppl    23.10\n",
            "| epoch  18 |   350/  506 batches | lr 10.00 | ms/batch 127.39 | loss  3.15 | ppl    23.29\n",
            "| epoch  18 |   360/  506 batches | lr 10.00 | ms/batch 129.00 | loss  3.15 | ppl    23.25\n",
            "| epoch  18 |   370/  506 batches | lr 10.00 | ms/batch 128.10 | loss  3.14 | ppl    23.12\n",
            "| epoch  18 |   380/  506 batches | lr 10.00 | ms/batch 127.95 | loss  3.14 | ppl    23.21\n",
            "| epoch  18 |   390/  506 batches | lr 10.00 | ms/batch 128.11 | loss  3.13 | ppl    22.92\n",
            "| epoch  18 |   400/  506 batches | lr 10.00 | ms/batch 128.48 | loss  3.12 | ppl    22.55\n",
            "| epoch  18 |   410/  506 batches | lr 10.00 | ms/batch 128.41 | loss  3.11 | ppl    22.53\n",
            "| epoch  18 |   420/  506 batches | lr 10.00 | ms/batch 128.90 | loss  3.09 | ppl    22.00\n",
            "| epoch  18 |   430/  506 batches | lr 10.00 | ms/batch 128.36 | loss  3.07 | ppl    21.54\n",
            "| epoch  18 |   440/  506 batches | lr 10.00 | ms/batch 128.33 | loss  3.12 | ppl    22.66\n",
            "| epoch  18 |   450/  506 batches | lr 10.00 | ms/batch 128.69 | loss  3.14 | ppl    23.09\n",
            "| epoch  18 |   460/  506 batches | lr 10.00 | ms/batch 128.24 | loss  3.10 | ppl    22.21\n",
            "| epoch  18 |   470/  506 batches | lr 10.00 | ms/batch 128.95 | loss  3.11 | ppl    22.46\n",
            "| epoch  18 |   480/  506 batches | lr 10.00 | ms/batch 128.03 | loss  3.10 | ppl    22.17\n",
            "| epoch  18 |   490/  506 batches | lr 10.00 | ms/batch 128.81 | loss  3.10 | ppl    22.24\n",
            "| epoch  18 |   500/  506 batches | lr 10.00 | ms/batch 127.42 | loss  3.10 | ppl    22.23\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time: 76.47s | valid loss  3.39 | valid ppl    29.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  19 |    10/  506 batches | lr 10.00 | ms/batch 139.65 | loss  3.46 | ppl    31.88\n",
            "| epoch  19 |    20/  506 batches | lr 10.00 | ms/batch 126.63 | loss  3.12 | ppl    22.62\n",
            "| epoch  19 |    30/  506 batches | lr 10.00 | ms/batch 127.86 | loss  3.11 | ppl    22.34\n",
            "| epoch  19 |    40/  506 batches | lr 10.00 | ms/batch 126.80 | loss  3.09 | ppl    22.02\n",
            "| epoch  19 |    50/  506 batches | lr 10.00 | ms/batch 127.18 | loss  3.12 | ppl    22.61\n",
            "| epoch  19 |    60/  506 batches | lr 10.00 | ms/batch 127.12 | loss  3.12 | ppl    22.60\n",
            "| epoch  19 |    70/  506 batches | lr 10.00 | ms/batch 127.42 | loss  3.13 | ppl    22.78\n",
            "| epoch  19 |    80/  506 batches | lr 10.00 | ms/batch 127.46 | loss  3.11 | ppl    22.46\n",
            "| epoch  19 |    90/  506 batches | lr 10.00 | ms/batch 127.76 | loss  3.13 | ppl    22.88\n",
            "| epoch  19 |   100/  506 batches | lr 10.00 | ms/batch 127.51 | loss  3.11 | ppl    22.33\n",
            "| epoch  19 |   110/  506 batches | lr 10.00 | ms/batch 127.67 | loss  3.07 | ppl    21.50\n",
            "| epoch  19 |   120/  506 batches | lr 10.00 | ms/batch 127.52 | loss  3.10 | ppl    22.31\n",
            "| epoch  19 |   130/  506 batches | lr 10.00 | ms/batch 126.30 | loss  3.11 | ppl    22.48\n",
            "| epoch  19 |   140/  506 batches | lr 10.00 | ms/batch 127.18 | loss  3.09 | ppl    22.01\n",
            "| epoch  19 |   150/  506 batches | lr 10.00 | ms/batch 126.92 | loss  3.11 | ppl    22.35\n",
            "| epoch  19 |   160/  506 batches | lr 10.00 | ms/batch 127.11 | loss  3.11 | ppl    22.47\n",
            "| epoch  19 |   170/  506 batches | lr 10.00 | ms/batch 127.78 | loss  3.09 | ppl    22.07\n",
            "| epoch  19 |   180/  506 batches | lr 10.00 | ms/batch 126.54 | loss  3.10 | ppl    22.19\n",
            "| epoch  19 |   190/  506 batches | lr 10.00 | ms/batch 127.68 | loss  3.11 | ppl    22.53\n",
            "| epoch  19 |   200/  506 batches | lr 10.00 | ms/batch 127.40 | loss  3.09 | ppl    21.97\n",
            "| epoch  19 |   210/  506 batches | lr 10.00 | ms/batch 127.34 | loss  3.10 | ppl    22.26\n",
            "| epoch  19 |   220/  506 batches | lr 10.00 | ms/batch 127.17 | loss  3.10 | ppl    22.23\n",
            "| epoch  19 |   230/  506 batches | lr 10.00 | ms/batch 126.65 | loss  3.09 | ppl    21.89\n",
            "| epoch  19 |   240/  506 batches | lr 10.00 | ms/batch 127.30 | loss  3.08 | ppl    21.74\n",
            "| epoch  19 |   250/  506 batches | lr 10.00 | ms/batch 126.96 | loss  3.07 | ppl    21.52\n",
            "| epoch  19 |   260/  506 batches | lr 10.00 | ms/batch 126.53 | loss  3.08 | ppl    21.85\n",
            "| epoch  19 |   270/  506 batches | lr 10.00 | ms/batch 127.51 | loss  3.09 | ppl    21.96\n",
            "| epoch  19 |   280/  506 batches | lr 10.00 | ms/batch 126.75 | loss  3.11 | ppl    22.32\n",
            "| epoch  19 |   290/  506 batches | lr 10.00 | ms/batch 126.76 | loss  3.11 | ppl    22.47\n",
            "| epoch  19 |   300/  506 batches | lr 10.00 | ms/batch 127.10 | loss  3.12 | ppl    22.70\n",
            "| epoch  19 |   310/  506 batches | lr 10.00 | ms/batch 127.35 | loss  3.10 | ppl    22.26\n",
            "| epoch  19 |   320/  506 batches | lr 10.00 | ms/batch 127.93 | loss  3.13 | ppl    22.78\n",
            "| epoch  19 |   330/  506 batches | lr 10.00 | ms/batch 127.11 | loss  3.10 | ppl    22.29\n",
            "| epoch  19 |   340/  506 batches | lr 10.00 | ms/batch 127.71 | loss  3.10 | ppl    22.28\n",
            "| epoch  19 |   350/  506 batches | lr 10.00 | ms/batch 127.02 | loss  3.12 | ppl    22.60\n",
            "| epoch  19 |   360/  506 batches | lr 10.00 | ms/batch 126.40 | loss  3.13 | ppl    22.87\n",
            "| epoch  19 |   370/  506 batches | lr 10.00 | ms/batch 127.44 | loss  3.11 | ppl    22.41\n",
            "| epoch  19 |   380/  506 batches | lr 10.00 | ms/batch 126.94 | loss  3.11 | ppl    22.39\n",
            "| epoch  19 |   390/  506 batches | lr 10.00 | ms/batch 127.02 | loss  3.10 | ppl    22.26\n",
            "| epoch  19 |   400/  506 batches | lr 10.00 | ms/batch 127.65 | loss  3.08 | ppl    21.84\n",
            "| epoch  19 |   410/  506 batches | lr 10.00 | ms/batch 127.23 | loss  3.09 | ppl    21.95\n",
            "| epoch  19 |   420/  506 batches | lr 10.00 | ms/batch 127.61 | loss  3.06 | ppl    21.26\n",
            "| epoch  19 |   430/  506 batches | lr 10.00 | ms/batch 127.35 | loss  3.05 | ppl    21.02\n",
            "| epoch  19 |   440/  506 batches | lr 10.00 | ms/batch 127.53 | loss  3.09 | ppl    21.94\n",
            "| epoch  19 |   450/  506 batches | lr 10.00 | ms/batch 126.61 | loss  3.11 | ppl    22.49\n",
            "| epoch  19 |   460/  506 batches | lr 10.00 | ms/batch 127.17 | loss  3.07 | ppl    21.50\n",
            "| epoch  19 |   470/  506 batches | lr 10.00 | ms/batch 128.02 | loss  3.09 | ppl    21.91\n",
            "| epoch  19 |   480/  506 batches | lr 10.00 | ms/batch 127.58 | loss  3.08 | ppl    21.76\n",
            "| epoch  19 |   490/  506 batches | lr 10.00 | ms/batch 127.94 | loss  3.07 | ppl    21.47\n",
            "| epoch  19 |   500/  506 batches | lr 10.00 | ms/batch 127.37 | loss  3.07 | ppl    21.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time: 76.37s | valid loss  3.38 | valid ppl    29.51\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  20 |    10/  506 batches | lr 10.00 | ms/batch 140.24 | loss  3.44 | ppl    31.12\n",
            "| epoch  20 |    20/  506 batches | lr 10.00 | ms/batch 128.10 | loss  3.09 | ppl    21.95\n",
            "| epoch  20 |    30/  506 batches | lr 10.00 | ms/batch 128.95 | loss  3.07 | ppl    21.61\n",
            "| epoch  20 |    40/  506 batches | lr 10.00 | ms/batch 129.06 | loss  3.06 | ppl    21.26\n",
            "| epoch  20 |    50/  506 batches | lr 10.00 | ms/batch 128.49 | loss  3.09 | ppl    21.98\n",
            "| epoch  20 |    60/  506 batches | lr 10.00 | ms/batch 128.10 | loss  3.09 | ppl    21.95\n",
            "| epoch  20 |    70/  506 batches | lr 10.00 | ms/batch 128.85 | loss  3.10 | ppl    22.30\n",
            "| epoch  20 |    80/  506 batches | lr 10.00 | ms/batch 128.72 | loss  3.08 | ppl    21.79\n",
            "| epoch  20 |    90/  506 batches | lr 10.00 | ms/batch 128.04 | loss  3.10 | ppl    22.12\n",
            "| epoch  20 |   100/  506 batches | lr 10.00 | ms/batch 128.44 | loss  3.08 | ppl    21.72\n",
            "| epoch  20 |   110/  506 batches | lr 10.00 | ms/batch 127.80 | loss  3.04 | ppl    20.89\n",
            "| epoch  20 |   120/  506 batches | lr 10.00 | ms/batch 128.32 | loss  3.07 | ppl    21.56\n",
            "| epoch  20 |   130/  506 batches | lr 10.00 | ms/batch 128.53 | loss  3.09 | ppl    21.93\n",
            "| epoch  20 |   140/  506 batches | lr 10.00 | ms/batch 128.45 | loss  3.07 | ppl    21.51\n",
            "| epoch  20 |   150/  506 batches | lr 10.00 | ms/batch 127.89 | loss  3.07 | ppl    21.62\n",
            "| epoch  20 |   160/  506 batches | lr 10.00 | ms/batch 128.07 | loss  3.08 | ppl    21.80\n",
            "| epoch  20 |   170/  506 batches | lr 10.00 | ms/batch 127.75 | loss  3.07 | ppl    21.50\n",
            "| epoch  20 |   180/  506 batches | lr 10.00 | ms/batch 127.81 | loss  3.07 | ppl    21.50\n",
            "| epoch  20 |   190/  506 batches | lr 10.00 | ms/batch 128.07 | loss  3.09 | ppl    22.07\n",
            "| epoch  20 |   200/  506 batches | lr 10.00 | ms/batch 127.62 | loss  3.07 | ppl    21.46\n",
            "| epoch  20 |   210/  506 batches | lr 10.00 | ms/batch 127.67 | loss  3.07 | ppl    21.47\n",
            "| epoch  20 |   220/  506 batches | lr 10.00 | ms/batch 127.88 | loss  3.08 | ppl    21.77\n",
            "| epoch  20 |   230/  506 batches | lr 10.00 | ms/batch 127.54 | loss  3.06 | ppl    21.35\n",
            "| epoch  20 |   240/  506 batches | lr 10.00 | ms/batch 127.57 | loss  3.06 | ppl    21.37\n",
            "| epoch  20 |   250/  506 batches | lr 10.00 | ms/batch 127.37 | loss  3.04 | ppl    20.88\n",
            "| epoch  20 |   260/  506 batches | lr 10.00 | ms/batch 127.63 | loss  3.07 | ppl    21.43\n",
            "| epoch  20 |   270/  506 batches | lr 10.00 | ms/batch 127.90 | loss  3.06 | ppl    21.37\n",
            "| epoch  20 |   280/  506 batches | lr 10.00 | ms/batch 128.23 | loss  3.07 | ppl    21.63\n",
            "| epoch  20 |   290/  506 batches | lr 10.00 | ms/batch 127.16 | loss  3.10 | ppl    22.26\n",
            "| epoch  20 |   300/  506 batches | lr 10.00 | ms/batch 126.82 | loss  3.09 | ppl    21.88\n",
            "| epoch  20 |   310/  506 batches | lr 10.00 | ms/batch 127.19 | loss  3.07 | ppl    21.50\n",
            "| epoch  20 |   320/  506 batches | lr 10.00 | ms/batch 127.35 | loss  3.09 | ppl    22.03\n",
            "| epoch  20 |   330/  506 batches | lr 10.00 | ms/batch 128.05 | loss  3.08 | ppl    21.81\n",
            "| epoch  20 |   340/  506 batches | lr 10.00 | ms/batch 127.12 | loss  3.08 | ppl    21.78\n",
            "| epoch  20 |   350/  506 batches | lr 10.00 | ms/batch 126.88 | loss  3.08 | ppl    21.79\n",
            "| epoch  20 |   360/  506 batches | lr 10.00 | ms/batch 127.53 | loss  3.10 | ppl    22.12\n",
            "| epoch  20 |   370/  506 batches | lr 10.00 | ms/batch 126.87 | loss  3.08 | ppl    21.83\n",
            "| epoch  20 |   380/  506 batches | lr 10.00 | ms/batch 126.91 | loss  3.09 | ppl    21.88\n",
            "| epoch  20 |   390/  506 batches | lr 10.00 | ms/batch 127.16 | loss  3.08 | ppl    21.76\n",
            "| epoch  20 |   400/  506 batches | lr 10.00 | ms/batch 127.17 | loss  3.06 | ppl    21.35\n",
            "| epoch  20 |   410/  506 batches | lr 10.00 | ms/batch 127.09 | loss  3.07 | ppl    21.46\n",
            "| epoch  20 |   420/  506 batches | lr 10.00 | ms/batch 128.33 | loss  3.03 | ppl    20.68\n",
            "| epoch  20 |   430/  506 batches | lr 10.00 | ms/batch 127.89 | loss  3.01 | ppl    20.35\n",
            "| epoch  20 |   440/  506 batches | lr 10.00 | ms/batch 127.98 | loss  3.05 | ppl    21.20\n",
            "| epoch  20 |   450/  506 batches | lr 10.00 | ms/batch 128.34 | loss  3.08 | ppl    21.79\n",
            "| epoch  20 |   460/  506 batches | lr 10.00 | ms/batch 127.59 | loss  3.04 | ppl    21.00\n",
            "| epoch  20 |   470/  506 batches | lr 10.00 | ms/batch 128.27 | loss  3.06 | ppl    21.38\n",
            "| epoch  20 |   480/  506 batches | lr 10.00 | ms/batch 127.68 | loss  3.05 | ppl    21.11\n",
            "| epoch  20 |   490/  506 batches | lr 10.00 | ms/batch 129.03 | loss  3.04 | ppl    21.00\n",
            "| epoch  20 |   500/  506 batches | lr 10.00 | ms/batch 128.78 | loss  3.04 | ppl    20.86\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time: 76.69s | valid loss  3.38 | valid ppl    29.41\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  21 |    10/  506 batches | lr 10.00 | ms/batch 140.42 | loss  3.40 | ppl    29.98\n",
            "| epoch  21 |    20/  506 batches | lr 10.00 | ms/batch 128.56 | loss  3.06 | ppl    21.41\n",
            "| epoch  21 |    30/  506 batches | lr 10.00 | ms/batch 128.94 | loss  3.04 | ppl    20.98\n",
            "| epoch  21 |    40/  506 batches | lr 10.00 | ms/batch 128.99 | loss  3.03 | ppl    20.60\n",
            "| epoch  21 |    50/  506 batches | lr 10.00 | ms/batch 129.00 | loss  3.06 | ppl    21.29\n",
            "| epoch  21 |    60/  506 batches | lr 10.00 | ms/batch 128.47 | loss  3.06 | ppl    21.42\n",
            "| epoch  21 |    70/  506 batches | lr 10.00 | ms/batch 129.02 | loss  3.08 | ppl    21.83\n",
            "| epoch  21 |    80/  506 batches | lr 10.00 | ms/batch 128.65 | loss  3.06 | ppl    21.35\n",
            "| epoch  21 |    90/  506 batches | lr 10.00 | ms/batch 127.96 | loss  3.07 | ppl    21.54\n",
            "| epoch  21 |   100/  506 batches | lr 10.00 | ms/batch 128.33 | loss  3.04 | ppl    21.00\n",
            "| epoch  21 |   110/  506 batches | lr 10.00 | ms/batch 128.56 | loss  3.02 | ppl    20.39\n",
            "| epoch  21 |   120/  506 batches | lr 10.00 | ms/batch 128.66 | loss  3.05 | ppl    21.13\n",
            "| epoch  21 |   130/  506 batches | lr 10.00 | ms/batch 127.37 | loss  3.06 | ppl    21.26\n",
            "| epoch  21 |   140/  506 batches | lr 10.00 | ms/batch 128.21 | loss  3.03 | ppl    20.78\n",
            "| epoch  21 |   150/  506 batches | lr 10.00 | ms/batch 128.08 | loss  3.05 | ppl    21.06\n",
            "| epoch  21 |   160/  506 batches | lr 10.00 | ms/batch 129.00 | loss  3.06 | ppl    21.26\n",
            "| epoch  21 |   170/  506 batches | lr 10.00 | ms/batch 128.38 | loss  3.04 | ppl    20.95\n",
            "| epoch  21 |   180/  506 batches | lr 10.00 | ms/batch 127.86 | loss  3.05 | ppl    21.06\n",
            "| epoch  21 |   190/  506 batches | lr 10.00 | ms/batch 129.21 | loss  3.06 | ppl    21.33\n",
            "| epoch  21 |   200/  506 batches | lr 10.00 | ms/batch 128.46 | loss  3.04 | ppl    20.82\n",
            "| epoch  21 |   210/  506 batches | lr 10.00 | ms/batch 128.77 | loss  3.05 | ppl    21.13\n",
            "| epoch  21 |   220/  506 batches | lr 10.00 | ms/batch 128.06 | loss  3.05 | ppl    21.07\n",
            "| epoch  21 |   230/  506 batches | lr 10.00 | ms/batch 128.15 | loss  3.04 | ppl    20.88\n",
            "| epoch  21 |   240/  506 batches | lr 10.00 | ms/batch 127.87 | loss  3.02 | ppl    20.53\n",
            "| epoch  21 |   250/  506 batches | lr 10.00 | ms/batch 127.88 | loss  3.01 | ppl    20.22\n",
            "| epoch  21 |   260/  506 batches | lr 10.00 | ms/batch 128.70 | loss  3.04 | ppl    20.84\n",
            "| epoch  21 |   270/  506 batches | lr 10.00 | ms/batch 128.01 | loss  3.04 | ppl    20.95\n",
            "| epoch  21 |   280/  506 batches | lr 10.00 | ms/batch 127.86 | loss  3.05 | ppl    21.02\n",
            "| epoch  21 |   290/  506 batches | lr 10.00 | ms/batch 128.14 | loss  3.07 | ppl    21.55\n",
            "| epoch  21 |   300/  506 batches | lr 10.00 | ms/batch 128.28 | loss  3.06 | ppl    21.38\n",
            "| epoch  21 |   310/  506 batches | lr 10.00 | ms/batch 128.02 | loss  3.04 | ppl    20.82\n",
            "| epoch  21 |   320/  506 batches | lr 10.00 | ms/batch 127.88 | loss  3.06 | ppl    21.37\n",
            "| epoch  21 |   330/  506 batches | lr 10.00 | ms/batch 127.79 | loss  3.04 | ppl    20.92\n",
            "| epoch  21 |   340/  506 batches | lr 10.00 | ms/batch 128.12 | loss  3.05 | ppl    21.16\n",
            "| epoch  21 |   350/  506 batches | lr 10.00 | ms/batch 128.50 | loss  3.05 | ppl    21.22\n",
            "| epoch  21 |   360/  506 batches | lr 10.00 | ms/batch 127.44 | loss  3.06 | ppl    21.36\n",
            "| epoch  21 |   370/  506 batches | lr 10.00 | ms/batch 127.39 | loss  3.05 | ppl    21.21\n",
            "| epoch  21 |   380/  506 batches | lr 10.00 | ms/batch 127.44 | loss  3.05 | ppl    21.12\n",
            "| epoch  21 |   390/  506 batches | lr 10.00 | ms/batch 127.37 | loss  3.04 | ppl    20.98\n",
            "| epoch  21 |   400/  506 batches | lr 10.00 | ms/batch 127.99 | loss  3.02 | ppl    20.50\n",
            "| epoch  21 |   410/  506 batches | lr 10.00 | ms/batch 127.26 | loss  3.04 | ppl    20.88\n",
            "| epoch  21 |   420/  506 batches | lr 10.00 | ms/batch 127.75 | loss  3.00 | ppl    20.17\n",
            "| epoch  21 |   430/  506 batches | lr 10.00 | ms/batch 127.81 | loss  2.98 | ppl    19.75\n",
            "| epoch  21 |   440/  506 batches | lr 10.00 | ms/batch 127.50 | loss  3.03 | ppl    20.72\n",
            "| epoch  21 |   450/  506 batches | lr 10.00 | ms/batch 127.49 | loss  3.06 | ppl    21.23\n",
            "| epoch  21 |   460/  506 batches | lr 10.00 | ms/batch 127.89 | loss  3.02 | ppl    20.46\n",
            "| epoch  21 |   470/  506 batches | lr 10.00 | ms/batch 127.61 | loss  3.03 | ppl    20.75\n",
            "| epoch  21 |   480/  506 batches | lr 10.00 | ms/batch 128.66 | loss  3.02 | ppl    20.58\n",
            "| epoch  21 |   490/  506 batches | lr 10.00 | ms/batch 128.72 | loss  3.02 | ppl    20.42\n",
            "| epoch  21 |   500/  506 batches | lr 10.00 | ms/batch 128.00 | loss  3.01 | ppl    20.22\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  21 | time: 76.84s | valid loss  3.38 | valid ppl    29.33\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  22 |    10/  506 batches | lr 10.00 | ms/batch 140.28 | loss  3.37 | ppl    28.95\n",
            "| epoch  22 |    20/  506 batches | lr 10.00 | ms/batch 128.96 | loss  3.03 | ppl    20.76\n",
            "| epoch  22 |    30/  506 batches | lr 10.00 | ms/batch 129.01 | loss  3.02 | ppl    20.55\n",
            "| epoch  22 |    40/  506 batches | lr 10.00 | ms/batch 128.43 | loss  3.01 | ppl    20.23\n",
            "| epoch  22 |    50/  506 batches | lr 10.00 | ms/batch 128.30 | loss  3.03 | ppl    20.80\n",
            "| epoch  22 |    60/  506 batches | lr 10.00 | ms/batch 128.14 | loss  3.03 | ppl    20.77\n",
            "| epoch  22 |    70/  506 batches | lr 10.00 | ms/batch 128.60 | loss  3.05 | ppl    21.18\n",
            "| epoch  22 |    80/  506 batches | lr 10.00 | ms/batch 128.71 | loss  3.03 | ppl    20.76\n",
            "| epoch  22 |    90/  506 batches | lr 10.00 | ms/batch 128.21 | loss  3.04 | ppl    20.90\n",
            "| epoch  22 |   100/  506 batches | lr 10.00 | ms/batch 129.46 | loss  3.03 | ppl    20.60\n",
            "| epoch  22 |   110/  506 batches | lr 10.00 | ms/batch 128.76 | loss  2.99 | ppl    19.83\n",
            "| epoch  22 |   120/  506 batches | lr 10.00 | ms/batch 129.35 | loss  3.02 | ppl    20.47\n",
            "| epoch  22 |   130/  506 batches | lr 10.00 | ms/batch 128.63 | loss  3.03 | ppl    20.74\n",
            "| epoch  22 |   140/  506 batches | lr 10.00 | ms/batch 128.51 | loss  3.01 | ppl    20.22\n",
            "| epoch  22 |   150/  506 batches | lr 10.00 | ms/batch 129.52 | loss  3.02 | ppl    20.46\n",
            "| epoch  22 |   160/  506 batches | lr 10.00 | ms/batch 129.21 | loss  3.03 | ppl    20.64\n",
            "| epoch  22 |   170/  506 batches | lr 10.00 | ms/batch 128.45 | loss  3.01 | ppl    20.31\n",
            "| epoch  22 |   180/  506 batches | lr 10.00 | ms/batch 128.46 | loss  3.02 | ppl    20.39\n",
            "| epoch  22 |   190/  506 batches | lr 10.00 | ms/batch 128.42 | loss  3.02 | ppl    20.58\n",
            "| epoch  22 |   200/  506 batches | lr 10.00 | ms/batch 128.42 | loss  3.01 | ppl    20.25\n",
            "| epoch  22 |   210/  506 batches | lr 10.00 | ms/batch 129.37 | loss  3.02 | ppl    20.52\n",
            "| epoch  22 |   220/  506 batches | lr 10.00 | ms/batch 128.95 | loss  3.03 | ppl    20.69\n",
            "| epoch  22 |   230/  506 batches | lr 10.00 | ms/batch 128.09 | loss  3.00 | ppl    20.07\n",
            "| epoch  22 |   240/  506 batches | lr 10.00 | ms/batch 129.25 | loss  2.99 | ppl    19.89\n",
            "| epoch  22 |   250/  506 batches | lr 10.00 | ms/batch 128.71 | loss  2.99 | ppl    19.83\n",
            "| epoch  22 |   260/  506 batches | lr 10.00 | ms/batch 128.83 | loss  3.02 | ppl    20.49\n",
            "| epoch  22 |   270/  506 batches | lr 10.00 | ms/batch 128.86 | loss  3.01 | ppl    20.24\n",
            "| epoch  22 |   280/  506 batches | lr 10.00 | ms/batch 129.09 | loss  3.01 | ppl    20.36\n",
            "| epoch  22 |   290/  506 batches | lr 10.00 | ms/batch 128.82 | loss  3.04 | ppl    20.90\n",
            "| epoch  22 |   300/  506 batches | lr 10.00 | ms/batch 128.97 | loss  3.03 | ppl    20.74\n",
            "| epoch  22 |   310/  506 batches | lr 10.00 | ms/batch 128.90 | loss  3.02 | ppl    20.44\n",
            "| epoch  22 |   320/  506 batches | lr 10.00 | ms/batch 129.29 | loss  3.04 | ppl    20.91\n",
            "| epoch  22 |   330/  506 batches | lr 10.00 | ms/batch 128.82 | loss  3.02 | ppl    20.50\n",
            "| epoch  22 |   340/  506 batches | lr 10.00 | ms/batch 129.53 | loss  3.02 | ppl    20.50\n",
            "| epoch  22 |   350/  506 batches | lr 10.00 | ms/batch 128.98 | loss  3.04 | ppl    20.85\n",
            "| epoch  22 |   360/  506 batches | lr 10.00 | ms/batch 129.36 | loss  3.04 | ppl    20.87\n",
            "| epoch  22 |   370/  506 batches | lr 10.00 | ms/batch 129.10 | loss  3.03 | ppl    20.73\n",
            "| epoch  22 |   380/  506 batches | lr 10.00 | ms/batch 128.66 | loss  3.03 | ppl    20.75\n",
            "| epoch  22 |   390/  506 batches | lr 10.00 | ms/batch 128.45 | loss  3.02 | ppl    20.59\n",
            "| epoch  22 |   400/  506 batches | lr 10.00 | ms/batch 128.66 | loss  3.00 | ppl    20.16\n",
            "| epoch  22 |   410/  506 batches | lr 10.00 | ms/batch 128.82 | loss  3.01 | ppl    20.29\n",
            "| epoch  22 |   420/  506 batches | lr 10.00 | ms/batch 128.65 | loss  2.98 | ppl    19.67\n",
            "| epoch  22 |   430/  506 batches | lr 10.00 | ms/batch 129.17 | loss  2.97 | ppl    19.44\n",
            "| epoch  22 |   440/  506 batches | lr 10.00 | ms/batch 128.48 | loss  3.00 | ppl    20.09\n",
            "| epoch  22 |   450/  506 batches | lr 10.00 | ms/batch 128.27 | loss  3.02 | ppl    20.58\n",
            "| epoch  22 |   460/  506 batches | lr 10.00 | ms/batch 128.32 | loss  2.99 | ppl    19.92\n",
            "| epoch  22 |   470/  506 batches | lr 10.00 | ms/batch 128.84 | loss  3.00 | ppl    20.07\n",
            "| epoch  22 |   480/  506 batches | lr 10.00 | ms/batch 128.36 | loss  3.00 | ppl    20.05\n",
            "| epoch  22 |   490/  506 batches | lr 10.00 | ms/batch 128.62 | loss  3.00 | ppl    19.99\n",
            "| epoch  22 |   500/  506 batches | lr 10.00 | ms/batch 128.97 | loss  2.98 | ppl    19.70\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  22 | time: 77.19s | valid loss  3.37 | valid ppl    29.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  23 |    10/  506 batches | lr 10.00 | ms/batch 141.24 | loss  3.33 | ppl    27.98\n",
            "| epoch  23 |    20/  506 batches | lr 10.00 | ms/batch 127.98 | loss  3.01 | ppl    20.23\n",
            "| epoch  23 |    30/  506 batches | lr 10.00 | ms/batch 128.97 | loss  3.00 | ppl    20.01\n",
            "| epoch  23 |    40/  506 batches | lr 10.00 | ms/batch 129.12 | loss  2.98 | ppl    19.75\n",
            "| epoch  23 |    50/  506 batches | lr 10.00 | ms/batch 128.40 | loss  3.01 | ppl    20.38\n",
            "| epoch  23 |    60/  506 batches | lr 10.00 | ms/batch 128.22 | loss  3.02 | ppl    20.40\n",
            "| epoch  23 |    70/  506 batches | lr 10.00 | ms/batch 128.53 | loss  3.02 | ppl    20.57\n",
            "| epoch  23 |    80/  506 batches | lr 10.00 | ms/batch 128.52 | loss  3.00 | ppl    20.15\n",
            "| epoch  23 |    90/  506 batches | lr 10.00 | ms/batch 128.75 | loss  3.02 | ppl    20.50\n",
            "| epoch  23 |   100/  506 batches | lr 10.00 | ms/batch 128.27 | loss  3.01 | ppl    20.19\n",
            "| epoch  23 |   110/  506 batches | lr 10.00 | ms/batch 128.17 | loss  2.96 | ppl    19.21\n",
            "| epoch  23 |   120/  506 batches | lr 10.00 | ms/batch 128.65 | loss  2.99 | ppl    19.96\n",
            "| epoch  23 |   130/  506 batches | lr 10.00 | ms/batch 128.60 | loss  3.01 | ppl    20.33\n",
            "| epoch  23 |   140/  506 batches | lr 10.00 | ms/batch 128.04 | loss  2.98 | ppl    19.64\n",
            "| epoch  23 |   150/  506 batches | lr 10.00 | ms/batch 128.64 | loss  2.99 | ppl    19.92\n",
            "| epoch  23 |   160/  506 batches | lr 10.00 | ms/batch 129.05 | loss  3.01 | ppl    20.19\n",
            "| epoch  23 |   170/  506 batches | lr 10.00 | ms/batch 127.79 | loss  3.00 | ppl    19.99\n",
            "| epoch  23 |   180/  506 batches | lr 10.00 | ms/batch 129.02 | loss  2.99 | ppl    19.91\n",
            "| epoch  23 |   190/  506 batches | lr 10.00 | ms/batch 128.44 | loss  3.01 | ppl    20.29\n",
            "| epoch  23 |   200/  506 batches | lr 10.00 | ms/batch 128.31 | loss  2.99 | ppl    19.81\n",
            "| epoch  23 |   210/  506 batches | lr 10.00 | ms/batch 128.93 | loss  2.99 | ppl    19.98\n",
            "| epoch  23 |   220/  506 batches | lr 10.00 | ms/batch 128.50 | loss  3.00 | ppl    20.07\n",
            "| epoch  23 |   230/  506 batches | lr 10.00 | ms/batch 128.70 | loss  2.98 | ppl    19.74\n",
            "| epoch  23 |   240/  506 batches | lr 10.00 | ms/batch 128.20 | loss  2.97 | ppl    19.45\n",
            "| epoch  23 |   250/  506 batches | lr 10.00 | ms/batch 128.22 | loss  2.96 | ppl    19.37\n",
            "| epoch  23 |   260/  506 batches | lr 10.00 | ms/batch 128.96 | loss  2.99 | ppl    19.79\n",
            "| epoch  23 |   270/  506 batches | lr 10.00 | ms/batch 127.95 | loss  2.99 | ppl    19.85\n",
            "| epoch  23 |   280/  506 batches | lr 10.00 | ms/batch 128.60 | loss  3.00 | ppl    20.04\n",
            "| epoch  23 |   290/  506 batches | lr 10.00 | ms/batch 128.36 | loss  3.01 | ppl    20.33\n",
            "| epoch  23 |   300/  506 batches | lr 10.00 | ms/batch 128.77 | loss  3.01 | ppl    20.21\n",
            "| epoch  23 |   310/  506 batches | lr 10.00 | ms/batch 127.90 | loss  2.98 | ppl    19.73\n",
            "| epoch  23 |   320/  506 batches | lr 10.00 | ms/batch 128.42 | loss  3.02 | ppl    20.47\n",
            "| epoch  23 |   330/  506 batches | lr 10.00 | ms/batch 127.91 | loss  2.99 | ppl    19.95\n",
            "| epoch  23 |   340/  506 batches | lr 10.00 | ms/batch 127.96 | loss  3.00 | ppl    20.13\n",
            "| epoch  23 |   350/  506 batches | lr 10.00 | ms/batch 128.86 | loss  3.02 | ppl    20.41\n",
            "| epoch  23 |   360/  506 batches | lr 10.00 | ms/batch 127.83 | loss  3.01 | ppl    20.25\n",
            "| epoch  23 |   370/  506 batches | lr 10.00 | ms/batch 128.54 | loss  3.01 | ppl    20.25\n",
            "| epoch  23 |   380/  506 batches | lr 10.00 | ms/batch 128.07 | loss  3.01 | ppl    20.25\n",
            "| epoch  23 |   390/  506 batches | lr 10.00 | ms/batch 127.87 | loss  2.99 | ppl    19.95\n",
            "| epoch  23 |   400/  506 batches | lr 10.00 | ms/batch 128.52 | loss  2.98 | ppl    19.65\n",
            "| epoch  23 |   410/  506 batches | lr 10.00 | ms/batch 128.33 | loss  2.98 | ppl    19.69\n",
            "| epoch  23 |   420/  506 batches | lr 10.00 | ms/batch 128.65 | loss  2.95 | ppl    19.19\n",
            "| epoch  23 |   430/  506 batches | lr 10.00 | ms/batch 127.46 | loss  2.94 | ppl    18.91\n",
            "| epoch  23 |   440/  506 batches | lr 10.00 | ms/batch 128.54 | loss  2.98 | ppl    19.75\n",
            "| epoch  23 |   450/  506 batches | lr 10.00 | ms/batch 128.23 | loss  3.00 | ppl    19.99\n",
            "| epoch  23 |   460/  506 batches | lr 10.00 | ms/batch 128.45 | loss  2.96 | ppl    19.33\n",
            "| epoch  23 |   470/  506 batches | lr 10.00 | ms/batch 128.18 | loss  2.98 | ppl    19.75\n",
            "| epoch  23 |   480/  506 batches | lr 10.00 | ms/batch 128.94 | loss  2.98 | ppl    19.65\n",
            "| epoch  23 |   490/  506 batches | lr 10.00 | ms/batch 129.27 | loss  2.97 | ppl    19.43\n",
            "| epoch  23 |   500/  506 batches | lr 10.00 | ms/batch 129.21 | loss  2.96 | ppl    19.33\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  23 | time: 77.01s | valid loss  3.38 | valid ppl    29.26\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  24 |    10/  506 batches | lr 2.50 | ms/batch 142.13 | loss  3.31 | ppl    27.34\n",
            "| epoch  24 |    20/  506 batches | lr 2.50 | ms/batch 128.44 | loss  2.98 | ppl    19.73\n",
            "| epoch  24 |    30/  506 batches | lr 2.50 | ms/batch 128.85 | loss  2.97 | ppl    19.52\n",
            "| epoch  24 |    40/  506 batches | lr 2.50 | ms/batch 129.42 | loss  2.96 | ppl    19.27\n",
            "| epoch  24 |    50/  506 batches | lr 2.50 | ms/batch 129.77 | loss  2.98 | ppl    19.63\n",
            "| epoch  24 |    60/  506 batches | lr 2.50 | ms/batch 128.86 | loss  2.99 | ppl    19.92\n",
            "| epoch  24 |    70/  506 batches | lr 2.50 | ms/batch 128.84 | loss  3.00 | ppl    20.09\n",
            "| epoch  24 |    80/  506 batches | lr 2.50 | ms/batch 128.67 | loss  2.98 | ppl    19.64\n",
            "| epoch  24 |    90/  506 batches | lr 2.50 | ms/batch 128.84 | loss  2.99 | ppl    19.88\n",
            "| epoch  24 |   100/  506 batches | lr 2.50 | ms/batch 128.84 | loss  2.98 | ppl    19.66\n",
            "| epoch  24 |   110/  506 batches | lr 2.50 | ms/batch 128.95 | loss  2.95 | ppl    19.07\n",
            "| epoch  24 |   120/  506 batches | lr 2.50 | ms/batch 128.88 | loss  2.97 | ppl    19.57\n",
            "| epoch  24 |   130/  506 batches | lr 2.50 | ms/batch 128.49 | loss  2.98 | ppl    19.72\n",
            "| epoch  24 |   140/  506 batches | lr 2.50 | ms/batch 128.70 | loss  2.97 | ppl    19.42\n",
            "| epoch  24 |   150/  506 batches | lr 2.50 | ms/batch 129.35 | loss  2.97 | ppl    19.53\n",
            "| epoch  24 |   160/  506 batches | lr 2.50 | ms/batch 129.61 | loss  2.98 | ppl    19.76\n",
            "| epoch  24 |   170/  506 batches | lr 2.50 | ms/batch 129.49 | loss  2.97 | ppl    19.50\n",
            "| epoch  24 |   180/  506 batches | lr 2.50 | ms/batch 128.99 | loss  2.96 | ppl    19.37\n",
            "| epoch  24 |   190/  506 batches | lr 2.50 | ms/batch 128.81 | loss  2.98 | ppl    19.65\n",
            "| epoch  24 |   200/  506 batches | lr 2.50 | ms/batch 129.02 | loss  2.96 | ppl    19.27\n",
            "| epoch  24 |   210/  506 batches | lr 2.50 | ms/batch 128.55 | loss  2.97 | ppl    19.55\n",
            "| epoch  24 |   220/  506 batches | lr 2.50 | ms/batch 128.67 | loss  2.96 | ppl    19.39\n",
            "| epoch  24 |   230/  506 batches | lr 2.50 | ms/batch 128.82 | loss  2.96 | ppl    19.27\n",
            "| epoch  24 |   240/  506 batches | lr 2.50 | ms/batch 128.99 | loss  2.95 | ppl    19.08\n",
            "| epoch  24 |   250/  506 batches | lr 2.50 | ms/batch 129.05 | loss  2.94 | ppl    18.87\n",
            "| epoch  24 |   260/  506 batches | lr 2.50 | ms/batch 128.74 | loss  2.96 | ppl    19.29\n",
            "| epoch  24 |   270/  506 batches | lr 2.50 | ms/batch 128.27 | loss  2.96 | ppl    19.37\n",
            "| epoch  24 |   280/  506 batches | lr 2.50 | ms/batch 127.95 | loss  2.97 | ppl    19.44\n",
            "| epoch  24 |   290/  506 batches | lr 2.50 | ms/batch 128.76 | loss  2.99 | ppl    19.94\n",
            "| epoch  24 |   300/  506 batches | lr 2.50 | ms/batch 128.31 | loss  2.99 | ppl    19.80\n",
            "| epoch  24 |   310/  506 batches | lr 2.50 | ms/batch 128.51 | loss  2.97 | ppl    19.43\n",
            "| epoch  24 |   320/  506 batches | lr 2.50 | ms/batch 129.13 | loss  2.99 | ppl    19.84\n",
            "| epoch  24 |   330/  506 batches | lr 2.50 | ms/batch 128.16 | loss  2.97 | ppl    19.53\n",
            "| epoch  24 |   340/  506 batches | lr 2.50 | ms/batch 128.92 | loss  2.97 | ppl    19.51\n",
            "| epoch  24 |   350/  506 batches | lr 2.50 | ms/batch 128.54 | loss  2.98 | ppl    19.71\n",
            "| epoch  24 |   360/  506 batches | lr 2.50 | ms/batch 128.42 | loss  2.99 | ppl    19.86\n",
            "| epoch  24 |   370/  506 batches | lr 2.50 | ms/batch 128.52 | loss  2.98 | ppl    19.63\n",
            "| epoch  24 |   380/  506 batches | lr 2.50 | ms/batch 128.17 | loss  2.99 | ppl    19.83\n",
            "| epoch  24 |   390/  506 batches | lr 2.50 | ms/batch 128.46 | loss  2.98 | ppl    19.67\n",
            "| epoch  24 |   400/  506 batches | lr 2.50 | ms/batch 128.18 | loss  2.96 | ppl    19.36\n",
            "| epoch  24 |   410/  506 batches | lr 2.50 | ms/batch 127.67 | loss  2.96 | ppl    19.25\n",
            "| epoch  24 |   420/  506 batches | lr 2.50 | ms/batch 128.08 | loss  2.94 | ppl    18.86\n",
            "| epoch  24 |   430/  506 batches | lr 2.50 | ms/batch 128.42 | loss  2.91 | ppl    18.35\n",
            "| epoch  24 |   440/  506 batches | lr 2.50 | ms/batch 127.63 | loss  2.96 | ppl    19.33\n",
            "| epoch  24 |   450/  506 batches | lr 2.50 | ms/batch 128.00 | loss  2.98 | ppl    19.62\n",
            "| epoch  24 |   460/  506 batches | lr 2.50 | ms/batch 128.53 | loss  2.94 | ppl    18.92\n",
            "| epoch  24 |   470/  506 batches | lr 2.50 | ms/batch 127.78 | loss  2.95 | ppl    19.17\n",
            "| epoch  24 |   480/  506 batches | lr 2.50 | ms/batch 127.95 | loss  2.95 | ppl    19.11\n",
            "| epoch  24 |   490/  506 batches | lr 2.50 | ms/batch 127.95 | loss  2.94 | ppl    18.89\n",
            "| epoch  24 |   500/  506 batches | lr 2.50 | ms/batch 127.66 | loss  2.94 | ppl    18.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  24 | time: 77.07s | valid loss  3.37 | valid ppl    29.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  25 |    10/  506 batches | lr 2.50 | ms/batch 140.79 | loss  3.29 | ppl    26.87\n",
            "| epoch  25 |    20/  506 batches | lr 2.50 | ms/batch 127.94 | loss  2.97 | ppl    19.41\n",
            "| epoch  25 |    30/  506 batches | lr 2.50 | ms/batch 129.14 | loss  2.95 | ppl    19.14\n",
            "| epoch  25 |    40/  506 batches | lr 2.50 | ms/batch 128.25 | loss  2.93 | ppl    18.68\n",
            "| epoch  25 |    50/  506 batches | lr 2.50 | ms/batch 128.38 | loss  2.96 | ppl    19.24\n",
            "| epoch  25 |    60/  506 batches | lr 2.50 | ms/batch 128.31 | loss  2.97 | ppl    19.40\n",
            "| epoch  25 |    70/  506 batches | lr 2.50 | ms/batch 128.97 | loss  2.98 | ppl    19.61\n",
            "| epoch  25 |    80/  506 batches | lr 2.50 | ms/batch 128.79 | loss  2.95 | ppl    19.03\n",
            "| epoch  25 |    90/  506 batches | lr 2.50 | ms/batch 128.21 | loss  2.97 | ppl    19.51\n",
            "| epoch  25 |   100/  506 batches | lr 2.50 | ms/batch 128.95 | loss  2.96 | ppl    19.25\n",
            "| epoch  25 |   110/  506 batches | lr 2.50 | ms/batch 129.10 | loss  2.92 | ppl    18.55\n",
            "| epoch  25 |   120/  506 batches | lr 2.50 | ms/batch 128.77 | loss  2.95 | ppl    19.07\n",
            "| epoch  25 |   130/  506 batches | lr 2.50 | ms/batch 128.09 | loss  2.96 | ppl    19.38\n",
            "| epoch  25 |   140/  506 batches | lr 2.50 | ms/batch 129.35 | loss  2.95 | ppl    19.04\n",
            "| epoch  25 |   150/  506 batches | lr 2.50 | ms/batch 128.24 | loss  2.95 | ppl    19.18\n",
            "| epoch  25 |   160/  506 batches | lr 2.50 | ms/batch 128.88 | loss  2.95 | ppl    19.20\n",
            "| epoch  25 |   170/  506 batches | lr 2.50 | ms/batch 128.86 | loss  2.94 | ppl    18.97\n",
            "| epoch  25 |   180/  506 batches | lr 2.50 | ms/batch 128.87 | loss  2.94 | ppl    18.91\n",
            "| epoch  25 |   190/  506 batches | lr 2.50 | ms/batch 129.96 | loss  2.95 | ppl    19.18\n",
            "| epoch  25 |   200/  506 batches | lr 2.50 | ms/batch 128.17 | loss  2.94 | ppl    18.83\n",
            "| epoch  25 |   210/  506 batches | lr 2.50 | ms/batch 129.18 | loss  2.95 | ppl    19.12\n",
            "| epoch  25 |   220/  506 batches | lr 2.50 | ms/batch 129.15 | loss  2.95 | ppl    19.12\n",
            "| epoch  25 |   230/  506 batches | lr 2.50 | ms/batch 129.20 | loss  2.94 | ppl    18.84\n",
            "| epoch  25 |   240/  506 batches | lr 2.50 | ms/batch 129.10 | loss  2.93 | ppl    18.70\n",
            "| epoch  25 |   250/  506 batches | lr 2.50 | ms/batch 129.04 | loss  2.91 | ppl    18.38\n",
            "| epoch  25 |   260/  506 batches | lr 2.50 | ms/batch 129.48 | loss  2.93 | ppl    18.81\n",
            "| epoch  25 |   270/  506 batches | lr 2.50 | ms/batch 129.72 | loss  2.94 | ppl    18.95\n",
            "| epoch  25 |   280/  506 batches | lr 2.50 | ms/batch 129.09 | loss  2.95 | ppl    19.05\n",
            "| epoch  25 |   290/  506 batches | lr 2.50 | ms/batch 129.77 | loss  2.96 | ppl    19.37\n",
            "| epoch  25 |   300/  506 batches | lr 2.50 | ms/batch 129.24 | loss  2.96 | ppl    19.27\n",
            "| epoch  25 |   310/  506 batches | lr 2.50 | ms/batch 129.14 | loss  2.94 | ppl    18.87\n",
            "| epoch  25 |   320/  506 batches | lr 2.50 | ms/batch 128.89 | loss  2.96 | ppl    19.34\n",
            "| epoch  25 |   330/  506 batches | lr 2.50 | ms/batch 128.29 | loss  2.95 | ppl    19.16\n",
            "| epoch  25 |   340/  506 batches | lr 2.50 | ms/batch 128.79 | loss  2.95 | ppl    19.17\n",
            "| epoch  25 |   350/  506 batches | lr 2.50 | ms/batch 129.03 | loss  2.97 | ppl    19.40\n",
            "| epoch  25 |   360/  506 batches | lr 2.50 | ms/batch 128.83 | loss  2.96 | ppl    19.25\n",
            "| epoch  25 |   370/  506 batches | lr 2.50 | ms/batch 129.38 | loss  2.96 | ppl    19.27\n",
            "| epoch  25 |   380/  506 batches | lr 2.50 | ms/batch 128.75 | loss  2.97 | ppl    19.45\n",
            "| epoch  25 |   390/  506 batches | lr 2.50 | ms/batch 127.63 | loss  2.95 | ppl    19.16\n",
            "| epoch  25 |   400/  506 batches | lr 2.50 | ms/batch 128.64 | loss  2.93 | ppl    18.75\n",
            "| epoch  25 |   410/  506 batches | lr 2.50 | ms/batch 128.50 | loss  2.93 | ppl    18.70\n",
            "| epoch  25 |   420/  506 batches | lr 2.50 | ms/batch 128.67 | loss  2.91 | ppl    18.29\n",
            "| epoch  25 |   430/  506 batches | lr 2.50 | ms/batch 128.12 | loss  2.90 | ppl    18.18\n",
            "| epoch  25 |   440/  506 batches | lr 2.50 | ms/batch 128.08 | loss  2.94 | ppl    18.84\n",
            "| epoch  25 |   450/  506 batches | lr 2.50 | ms/batch 128.06 | loss  2.96 | ppl    19.27\n",
            "| epoch  25 |   460/  506 batches | lr 2.50 | ms/batch 128.03 | loss  2.92 | ppl    18.63\n",
            "| epoch  25 |   470/  506 batches | lr 2.50 | ms/batch 127.99 | loss  2.94 | ppl    18.85\n",
            "| epoch  25 |   480/  506 batches | lr 2.50 | ms/batch 127.75 | loss  2.93 | ppl    18.70\n",
            "| epoch  25 |   490/  506 batches | lr 2.50 | ms/batch 127.28 | loss  2.92 | ppl    18.51\n",
            "| epoch  25 |   500/  506 batches | lr 2.50 | ms/batch 127.75 | loss  2.92 | ppl    18.54\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  25 | time: 77.09s | valid loss  3.38 | valid ppl    29.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  26 |    10/  506 batches | lr 0.62 | ms/batch 142.22 | loss  3.27 | ppl    26.26\n",
            "| epoch  26 |    20/  506 batches | lr 0.62 | ms/batch 127.72 | loss  2.95 | ppl    19.08\n",
            "| epoch  26 |    30/  506 batches | lr 0.62 | ms/batch 128.41 | loss  2.93 | ppl    18.67\n",
            "| epoch  26 |    40/  506 batches | lr 0.62 | ms/batch 127.33 | loss  2.91 | ppl    18.37\n",
            "| epoch  26 |    50/  506 batches | lr 0.62 | ms/batch 128.23 | loss  2.94 | ppl    18.95\n",
            "| epoch  26 |    60/  506 batches | lr 0.62 | ms/batch 128.76 | loss  2.95 | ppl    19.09\n",
            "| epoch  26 |    70/  506 batches | lr 0.62 | ms/batch 128.04 | loss  2.95 | ppl    19.17\n",
            "| epoch  26 |    80/  506 batches | lr 0.62 | ms/batch 128.26 | loss  2.94 | ppl    18.87\n",
            "| epoch  26 |    90/  506 batches | lr 0.62 | ms/batch 127.83 | loss  2.94 | ppl    19.01\n",
            "| epoch  26 |   100/  506 batches | lr 0.62 | ms/batch 128.24 | loss  2.94 | ppl    18.89\n",
            "| epoch  26 |   110/  506 batches | lr 0.62 | ms/batch 127.77 | loss  2.89 | ppl    18.08\n",
            "| epoch  26 |   120/  506 batches | lr 0.62 | ms/batch 128.66 | loss  2.92 | ppl    18.63\n",
            "| epoch  26 |   130/  506 batches | lr 0.62 | ms/batch 128.03 | loss  2.93 | ppl    18.75\n",
            "| epoch  26 |   140/  506 batches | lr 0.62 | ms/batch 129.21 | loss  2.92 | ppl    18.52\n",
            "| epoch  26 |   150/  506 batches | lr 0.62 | ms/batch 128.27 | loss  2.93 | ppl    18.73\n",
            "| epoch  26 |   160/  506 batches | lr 0.62 | ms/batch 127.85 | loss  2.93 | ppl    18.70\n",
            "| epoch  26 |   170/  506 batches | lr 0.62 | ms/batch 128.71 | loss  2.92 | ppl    18.58\n",
            "| epoch  26 |   180/  506 batches | lr 0.62 | ms/batch 128.38 | loss  2.92 | ppl    18.46\n",
            "| epoch  26 |   190/  506 batches | lr 0.62 | ms/batch 129.05 | loss  2.93 | ppl    18.77\n",
            "| epoch  26 |   200/  506 batches | lr 0.62 | ms/batch 128.36 | loss  2.91 | ppl    18.40\n",
            "| epoch  26 |   210/  506 batches | lr 0.62 | ms/batch 128.70 | loss  2.93 | ppl    18.74\n",
            "| epoch  26 |   220/  506 batches | lr 0.62 | ms/batch 128.79 | loss  2.92 | ppl    18.61\n",
            "| epoch  26 |   230/  506 batches | lr 0.62 | ms/batch 129.02 | loss  2.91 | ppl    18.34\n",
            "| epoch  26 |   240/  506 batches | lr 0.62 | ms/batch 128.85 | loss  2.92 | ppl    18.48\n",
            "| epoch  26 |   250/  506 batches | lr 0.62 | ms/batch 128.92 | loss  2.89 | ppl    18.06\n",
            "| epoch  26 |   260/  506 batches | lr 0.62 | ms/batch 129.43 | loss  2.92 | ppl    18.57\n",
            "| epoch  26 |   270/  506 batches | lr 0.62 | ms/batch 129.05 | loss  2.92 | ppl    18.54\n",
            "| epoch  26 |   280/  506 batches | lr 0.62 | ms/batch 128.74 | loss  2.93 | ppl    18.76\n",
            "| epoch  26 |   290/  506 batches | lr 0.62 | ms/batch 129.35 | loss  2.95 | ppl    19.03\n",
            "| epoch  26 |   300/  506 batches | lr 0.62 | ms/batch 129.01 | loss  2.93 | ppl    18.82\n",
            "| epoch  26 |   310/  506 batches | lr 0.62 | ms/batch 129.15 | loss  2.91 | ppl    18.40\n",
            "| epoch  26 |   320/  506 batches | lr 0.62 | ms/batch 129.63 | loss  2.94 | ppl    18.93\n",
            "| epoch  26 |   330/  506 batches | lr 0.62 | ms/batch 128.88 | loss  2.93 | ppl    18.67\n",
            "| epoch  26 |   340/  506 batches | lr 0.62 | ms/batch 128.94 | loss  2.94 | ppl    18.83\n",
            "| epoch  26 |   350/  506 batches | lr 0.62 | ms/batch 128.58 | loss  2.95 | ppl    19.15\n",
            "| epoch  26 |   360/  506 batches | lr 0.62 | ms/batch 129.31 | loss  2.94 | ppl    18.91\n",
            "| epoch  26 |   370/  506 batches | lr 0.62 | ms/batch 129.60 | loss  2.94 | ppl    18.88\n",
            "| epoch  26 |   380/  506 batches | lr 0.62 | ms/batch 129.28 | loss  2.93 | ppl    18.81\n",
            "| epoch  26 |   390/  506 batches | lr 0.62 | ms/batch 129.62 | loss  2.94 | ppl    18.86\n",
            "| epoch  26 |   400/  506 batches | lr 0.62 | ms/batch 129.30 | loss  2.91 | ppl    18.29\n",
            "| epoch  26 |   410/  506 batches | lr 0.62 | ms/batch 129.39 | loss  2.91 | ppl    18.41\n",
            "| epoch  26 |   420/  506 batches | lr 0.62 | ms/batch 128.42 | loss  2.90 | ppl    18.11\n",
            "| epoch  26 |   430/  506 batches | lr 0.62 | ms/batch 128.48 | loss  2.88 | ppl    17.75\n",
            "| epoch  26 |   440/  506 batches | lr 0.62 | ms/batch 129.39 | loss  2.92 | ppl    18.58\n",
            "| epoch  26 |   450/  506 batches | lr 0.62 | ms/batch 129.21 | loss  2.93 | ppl    18.82\n",
            "| epoch  26 |   460/  506 batches | lr 0.62 | ms/batch 128.51 | loss  2.90 | ppl    18.14\n",
            "| epoch  26 |   470/  506 batches | lr 0.62 | ms/batch 128.74 | loss  2.92 | ppl    18.50\n",
            "| epoch  26 |   480/  506 batches | lr 0.62 | ms/batch 128.56 | loss  2.90 | ppl    18.21\n",
            "| epoch  26 |   490/  506 batches | lr 0.62 | ms/batch 128.66 | loss  2.90 | ppl    18.11\n",
            "| epoch  26 |   500/  506 batches | lr 0.62 | ms/batch 128.69 | loss  2.89 | ppl    18.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  26 | time: 77.16s | valid loss  3.38 | valid ppl    29.29\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  27 |    10/  506 batches | lr 0.16 | ms/batch 141.59 | loss  3.24 | ppl    25.55\n",
            "| epoch  27 |    20/  506 batches | lr 0.16 | ms/batch 128.24 | loss  2.92 | ppl    18.53\n",
            "| epoch  27 |    30/  506 batches | lr 0.16 | ms/batch 128.40 | loss  2.90 | ppl    18.25\n",
            "| epoch  27 |    40/  506 batches | lr 0.16 | ms/batch 127.84 | loss  2.89 | ppl    18.06\n",
            "| epoch  27 |    50/  506 batches | lr 0.16 | ms/batch 128.33 | loss  2.92 | ppl    18.52\n",
            "| epoch  27 |    60/  506 batches | lr 0.16 | ms/batch 127.15 | loss  2.93 | ppl    18.71\n",
            "| epoch  27 |    70/  506 batches | lr 0.16 | ms/batch 127.85 | loss  2.93 | ppl    18.79\n",
            "| epoch  27 |    80/  506 batches | lr 0.16 | ms/batch 128.70 | loss  2.91 | ppl    18.37\n",
            "| epoch  27 |    90/  506 batches | lr 0.16 | ms/batch 127.98 | loss  2.93 | ppl    18.80\n",
            "| epoch  27 |   100/  506 batches | lr 0.16 | ms/batch 127.73 | loss  2.91 | ppl    18.35\n",
            "| epoch  27 |   110/  506 batches | lr 0.16 | ms/batch 127.89 | loss  2.87 | ppl    17.66\n",
            "| epoch  27 |   120/  506 batches | lr 0.16 | ms/batch 128.19 | loss  2.91 | ppl    18.37\n",
            "| epoch  27 |   130/  506 batches | lr 0.16 | ms/batch 128.15 | loss  2.92 | ppl    18.49\n",
            "| epoch  27 |   140/  506 batches | lr 0.16 | ms/batch 127.56 | loss  2.90 | ppl    18.15\n",
            "| epoch  27 |   150/  506 batches | lr 0.16 | ms/batch 128.06 | loss  2.90 | ppl    18.23\n",
            "| epoch  27 |   160/  506 batches | lr 0.16 | ms/batch 127.64 | loss  2.91 | ppl    18.39\n",
            "| epoch  27 |   170/  506 batches | lr 0.16 | ms/batch 127.52 | loss  2.90 | ppl    18.14\n",
            "| epoch  27 |   180/  506 batches | lr 0.16 | ms/batch 128.27 | loss  2.90 | ppl    18.16\n",
            "| epoch  27 |   190/  506 batches | lr 0.16 | ms/batch 128.12 | loss  2.91 | ppl    18.34\n",
            "| epoch  27 |   200/  506 batches | lr 0.16 | ms/batch 127.59 | loss  2.89 | ppl    18.01\n",
            "| epoch  27 |   210/  506 batches | lr 0.16 | ms/batch 129.01 | loss  2.91 | ppl    18.34\n",
            "| epoch  27 |   220/  506 batches | lr 0.16 | ms/batch 127.70 | loss  2.91 | ppl    18.33\n",
            "| epoch  27 |   230/  506 batches | lr 0.16 | ms/batch 128.18 | loss  2.89 | ppl    17.95\n",
            "| epoch  27 |   240/  506 batches | lr 0.16 | ms/batch 128.63 | loss  2.89 | ppl    17.96\n",
            "| epoch  27 |   250/  506 batches | lr 0.16 | ms/batch 128.61 | loss  2.87 | ppl    17.68\n",
            "| epoch  27 |   260/  506 batches | lr 0.16 | ms/batch 127.76 | loss  2.90 | ppl    18.17\n",
            "| epoch  27 |   270/  506 batches | lr 0.16 | ms/batch 128.98 | loss  2.91 | ppl    18.36\n",
            "| epoch  27 |   280/  506 batches | lr 0.16 | ms/batch 128.25 | loss  2.91 | ppl    18.39\n",
            "| epoch  27 |   290/  506 batches | lr 0.16 | ms/batch 128.48 | loss  2.93 | ppl    18.66\n",
            "| epoch  27 |   300/  506 batches | lr 0.16 | ms/batch 127.72 | loss  2.92 | ppl    18.52\n",
            "| epoch  27 |   310/  506 batches | lr 0.16 | ms/batch 128.13 | loss  2.89 | ppl    18.03\n",
            "| epoch  27 |   320/  506 batches | lr 0.16 | ms/batch 128.71 | loss  2.93 | ppl    18.65\n",
            "| epoch  27 |   330/  506 batches | lr 0.16 | ms/batch 127.76 | loss  2.90 | ppl    18.10\n",
            "| epoch  27 |   340/  506 batches | lr 0.16 | ms/batch 128.17 | loss  2.92 | ppl    18.46\n",
            "| epoch  27 |   350/  506 batches | lr 0.16 | ms/batch 128.16 | loss  2.91 | ppl    18.45\n",
            "| epoch  27 |   360/  506 batches | lr 0.16 | ms/batch 128.45 | loss  2.92 | ppl    18.60\n",
            "| epoch  27 |   370/  506 batches | lr 0.16 | ms/batch 128.69 | loss  2.92 | ppl    18.48\n",
            "| epoch  27 |   380/  506 batches | lr 0.16 | ms/batch 128.38 | loss  2.92 | ppl    18.47\n",
            "| epoch  27 |   390/  506 batches | lr 0.16 | ms/batch 128.60 | loss  2.90 | ppl    18.21\n",
            "| epoch  27 |   400/  506 batches | lr 0.16 | ms/batch 128.11 | loss  2.89 | ppl    17.94\n",
            "| epoch  27 |   410/  506 batches | lr 0.16 | ms/batch 127.99 | loss  2.89 | ppl    18.00\n",
            "| epoch  27 |   420/  506 batches | lr 0.16 | ms/batch 127.92 | loss  2.86 | ppl    17.52\n",
            "| epoch  27 |   430/  506 batches | lr 0.16 | ms/batch 127.79 | loss  2.86 | ppl    17.43\n",
            "| epoch  27 |   440/  506 batches | lr 0.16 | ms/batch 128.19 | loss  2.89 | ppl    17.98\n",
            "| epoch  27 |   450/  506 batches | lr 0.16 | ms/batch 128.14 | loss  2.91 | ppl    18.42\n",
            "| epoch  27 |   460/  506 batches | lr 0.16 | ms/batch 128.01 | loss  2.88 | ppl    17.79\n",
            "| epoch  27 |   470/  506 batches | lr 0.16 | ms/batch 128.56 | loss  2.89 | ppl    18.08\n",
            "| epoch  27 |   480/  506 batches | lr 0.16 | ms/batch 128.64 | loss  2.89 | ppl    17.91\n",
            "| epoch  27 |   490/  506 batches | lr 0.16 | ms/batch 128.68 | loss  2.88 | ppl    17.90\n",
            "| epoch  27 |   500/  506 batches | lr 0.16 | ms/batch 127.81 | loss  2.87 | ppl    17.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  27 | time: 76.86s | valid loss  3.38 | valid ppl    29.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  28 |    10/  506 batches | lr 0.04 | ms/batch 142.21 | loss  3.22 | ppl    24.91\n",
            "| epoch  28 |    20/  506 batches | lr 0.04 | ms/batch 128.84 | loss  2.90 | ppl    18.13\n",
            "| epoch  28 |    30/  506 batches | lr 0.04 | ms/batch 128.90 | loss  2.88 | ppl    17.83\n",
            "| epoch  28 |    40/  506 batches | lr 0.04 | ms/batch 128.81 | loss  2.87 | ppl    17.72\n",
            "| epoch  28 |    50/  506 batches | lr 0.04 | ms/batch 128.30 | loss  2.89 | ppl    18.06\n",
            "| epoch  28 |    60/  506 batches | lr 0.04 | ms/batch 128.34 | loss  2.91 | ppl    18.32\n",
            "| epoch  28 |    70/  506 batches | lr 0.04 | ms/batch 128.41 | loss  2.91 | ppl    18.42\n",
            "| epoch  28 |    80/  506 batches | lr 0.04 | ms/batch 128.87 | loss  2.90 | ppl    18.11\n",
            "| epoch  28 |    90/  506 batches | lr 0.04 | ms/batch 128.43 | loss  2.91 | ppl    18.36\n",
            "| epoch  28 |   100/  506 batches | lr 0.04 | ms/batch 128.81 | loss  2.90 | ppl    18.16\n",
            "| epoch  28 |   110/  506 batches | lr 0.04 | ms/batch 128.84 | loss  2.85 | ppl    17.24\n",
            "| epoch  28 |   120/  506 batches | lr 0.04 | ms/batch 128.34 | loss  2.88 | ppl    17.89\n",
            "| epoch  28 |   130/  506 batches | lr 0.04 | ms/batch 128.20 | loss  2.89 | ppl    17.99\n",
            "| epoch  28 |   140/  506 batches | lr 0.04 | ms/batch 128.59 | loss  2.88 | ppl    17.81\n",
            "| epoch  28 |   150/  506 batches | lr 0.04 | ms/batch 129.03 | loss  2.88 | ppl    17.86\n",
            "| epoch  28 |   160/  506 batches | lr 0.04 | ms/batch 128.82 | loss  2.89 | ppl    18.07\n",
            "| epoch  28 |   170/  506 batches | lr 0.04 | ms/batch 128.63 | loss  2.88 | ppl    17.76\n",
            "-----------------------------------------------------------------------------------------\n",
            "Exiting from training early\n",
            "=========================================================================================\n",
            "| End of training | test loss  3.40 | test ppl    29.98\n",
            "=========================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ikajkc9I3Sl2"
      },
      "source": [
        "def beam_search_decode(device, net, words, vocab_to_int, int_to_vocab, top_k, temperature):\n",
        "  net.eval()\n",
        "  softmax = nn.Softmax(dim = 1)\n",
        "  words = words.split(' ')\n",
        "  words.append('<sos>')\n",
        "  hidden = net.init_hidden(1)\n",
        "  for v in hidden:\n",
        "    v = v.to(device)\n",
        "  for w in words:\n",
        "    ix = torch.tensor([[vocab_to_int[w]]]).to(device)\n",
        "    output, hidden = net(ix, hidden)\n",
        "  output = output / temperature\n",
        "  prob, top_ix = torch.topk(softmax(output[0]), k=top_k)\n",
        "  prob = torch.log(prob)\n",
        "  #print(top_ix)\n",
        "  list_ids = [[id] for id in top_ix[0].tolist()]\n",
        "  outputs = [output for _ in range(top_k)]\n",
        "  hiddens = [hidden for _ in range(top_k)] \n",
        "  #print(\"avant beam search, top indices : \",top_ix.tolist(), \"de proba\", prob.tolist())\n",
        "  #print(list_ids)\n",
        "  for i in range(40):\n",
        "    probas = torch.zeros(top_k, top_k).float().to(device)\n",
        "    indxes = torch.zeros(top_k, top_k).to(device)\n",
        "    for k in range(top_k):\n",
        "      ix = torch.tensor([[top_ix[0][k]]]).to(device)\n",
        "      output, hiddens[k] = net(ix, hiddens[k])\n",
        "      output = output / temperature\n",
        "      pro, indxes[k] = torch.topk(softmax(output[0]), k=top_k)\n",
        "      pro = torch.log(pro)\n",
        "      #print(\"probas du choix \", k+1,\" : \", pro.tolist())\n",
        "      probas[k] = torch.add(pro[0], prob[0][k])\n",
        "    #print(indxes.tolist())\n",
        "    #print(list_ids)\n",
        "    prob, indices = torch.topk(probas.flatten(), top_k)\n",
        "    prob = torch.unsqueeze(prob, 0)\n",
        "    for k in range(top_k):\n",
        "      top_ix[0][k] = indxes.flatten()[indices[k]]\n",
        "    indices = indices // top_k\n",
        "    temp1 = []\n",
        "    temp2 = []\n",
        "    for k in range(top_k):\n",
        "      temp1.append(hiddens[indices.tolist()[k]])\n",
        "      temp2.append(list_ids[indices.tolist()[k]] + [top_ix[0].tolist()[k]])\n",
        "    hiddens = temp1\n",
        "    list_ids = temp2\n",
        "    #print(\"top indices : \",top_ix.tolist(), \"de proba\", prob.tolist())\n",
        "  best_branch = list_ids[torch.argmax(prob)]\n",
        "  words = []\n",
        "  for id in best_branch:\n",
        "    words.append(int_to_vocab[id])\n",
        "  print(' '.join(words))"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLCMetacxM6v"
      },
      "source": [
        "def predict(device, net, words, vocab_to_int, int_to_vocab, temperature):\n",
        "  net.eval()\n",
        "  softmax = nn.Softmax(dim=1)\n",
        "  words = words.split(' ')\n",
        "  hidden = net.init_hidden(1)\n",
        "  for v in hidden:\n",
        "    v = v.to(device)\n",
        "  for w in words:\n",
        "    ix = torch.tensor([[vocab_to_int[w]]]).to(device)\n",
        "    output, hidden = net(ix, hidden)\n",
        "  output = output / temperature\n",
        "  idx_max = torch.argmax(softmax(output[0]))\n",
        "  words = []\n",
        "  words.append(int_to_vocab[idx_max])\n",
        "  for i in range(0, 40):\n",
        "      ix = torch.tensor([[idx_max]]).to(device)\n",
        "      output, hidden = net(ix, hidden)\n",
        "      output = output / temperature\n",
        "      idx_max = torch.argmax(softmax(output[0]))\n",
        "      words.append(int_to_vocab[idx_max])\n",
        "  print(' '.join(words))"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s22J_29pRd5r"
      },
      "source": [
        "def top_k_sampling(device, net, words, vocab_to_int, int_to_vocab, top_k, temperature):\r\n",
        "  net.eval()\r\n",
        "  softmax = nn.Softmax(dim=-1)\r\n",
        "  words = words.split(' ')\r\n",
        "  hidden = net.init_hidden(1)\r\n",
        "  for v in hidden:\r\n",
        "    v = v.to(device)\r\n",
        "  for w in words:\r\n",
        "    ix = torch.tensor([[vocab_to_int[w]]]).to(device)\r\n",
        "    output, hidden = net(ix, hidden)\r\n",
        "  output = output / temperature\r\n",
        "  indices_to_remove = output[0] < torch.topk(output[0], top_k)[0][..., -1, None]\r\n",
        "  output[0][indices_to_remove] = -float('Inf')\r\n",
        "  prob = softmax(output[0])\r\n",
        "  idx_max = torch.multinomial(prob, 1)\r\n",
        "  words = []\r\n",
        "  words.append(int_to_vocab[idx_max])\r\n",
        "  for i in range(0, 40):\r\n",
        "      ix = torch.tensor([[idx_max]]).to(device)\r\n",
        "      output, hidden = net(ix, hidden)\r\n",
        "      output = output[0][0] / temperature\r\n",
        "      indices_to_remove = output < torch.topk(output, top_k)[0][..., -1, None]\r\n",
        "      output[indices_to_remove] = -float('Inf')\r\n",
        "      prob = softmax(output)\r\n",
        "      idx_max = torch.multinomial(prob, 1)\r\n",
        "      words.append(int_to_vocab[idx_max])\r\n",
        "  print(' '.join(words))"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9V_Q2YLG0qCB",
        "outputId": "71ddab9b-a812-4cf0-a46a-7f7159c3abe5"
      },
      "source": [
        "words = '<sos> Mettant au plus heureux le sceptre dans la main , <eos1> <sos> Va faire l’ un sujet , et l’ autre souverain . <eos2>'\n",
        "predict(device, model, words, corpus.dictionary.word2idx, corpus.dictionary.idx2word,1)\n",
        "beam_search_decode(device, model, words, corpus.dictionary.word2idx, corpus.dictionary.idx2word,10,0.2)\n",
        "beam_search_decode(device, model, words, corpus.dictionary.word2idx, corpus.dictionary.idx2word,10,0.5)\n",
        "beam_search_decode(device, model, words, corpus.dictionary.word2idx, corpus.dictionary.idx2word,10,1)\n",
        "top_k_sampling(device, model, words, corpus.dictionary.word2idx, corpus.dictionary.idx2word, 10, 1)\n",
        "top_k_sampling(device, model, words, corpus.dictionary.word2idx, corpus.dictionary.idx2word, 50, 1)\n",
        "top_k_sampling(device, model, words, corpus.dictionary.word2idx, corpus.dictionary.idx2word, 50, 0.5)\n",
        "top_k_sampling(device, model, words, corpus.dictionary.word2idx, corpus.dictionary.idx2word, 50, 0.2)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<sos> Mais , Madame , voyez que je suis votre mère . <eos1> <sos> Vous en êtes , Madame , et pour vous en parler , <eos2> <sos> Et vous pouvez , seigneur , de ne le point souffrir . <eos1>\n",
            "Mais , Madame , voyez que je suis à vous - même . <eos1> <sos> Je vous l’ ai déjà dit , je vous en fais justice . <eos2> <sos> Je vous l’ ai déjà dit , je vous en fais\n",
            "Il faut que je le voie , et que je te pardonne . <eos1> <sos> Mais , Madame , après tout , que faut - il que je fasse ? <eos2> <sos> Que ne puis - je , Seigneur , que\n",
            "Ce n’ est qu’ un faux effet qu’ il faut que je lui fasse . <eos1> <sos> Mais , Madame , après tout , je n’ ai rien à vous dire . <eos2> <sos> Je ne vous dirai point que je\n",
            "<sos> Vous avez en ces lieux quelque chose d’ un père . <eos1> <sos> J’ en réponds , j’ y consens , mais je l’ y vois paraître . <eos2> <sos> Que me vient de l’ amour ! Et qu’ en\n",
            "<sos> Qu’ elle parte , elle fuit , et que Rome il vit . <eos1> <sos> Je voudrais , à sa vue , en sa protection , <eos2> <sos> Des fureurs d’ un héros qu’ il lui faut arracher ; <eos1>\n",
            "<sos> Mais , seigneur , s’ il est vrai , je ne sais qu’ un rebelle . <eos1> <sos> Je suis trop malheureuse . Et si je sens qu’ on aime , <eos2> <sos> Si je n’ en avais pas ,\n",
            "<sos> Mais , seigneur , que j’ ai vu ce qu’ il faut que je fasse , <eos1> <sos> Et que je vous en donne un si digne supplice , <eos2> <sos> Que je vous dois de moi , je vous\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "it6lUAwIXOcO"
      },
      "source": [
        "Je veux , et l’ en a vu . . Mais il faut à l’ entendre : . . ; <eos> \n",
        "\n",
        "Je vous dois voir à vous . Et que j’ ai su l’ attendre <eos> \n",
        "\n",
        "Ce que j’ avais pu voir , et l’ autre à l’ honneur ; <eos> \n",
        "\n",
        "Il n’ a rien dit qu’ en ce mot je n’ en dois point de rien\n",
        "\n",
        "**Ajout du SoS**\n",
        "\n",
        ". . . Ah ! Seigneur , c’ est moi - même . <eos> \n",
        "\n",
        "<sos> Ah ! Madame , il est vrai , je n’ en veux point douter , <eos>\n",
        "\n",
        "<sos> Et je n’ ai pas besoin de m’ en faire haïr ."
      ]
    }
  ]
}