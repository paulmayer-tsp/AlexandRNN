{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "AlexandRNN_pytorch.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5LqrlD34g3m"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1ACLXSw4g3s"
      },
      "source": [
        "### Tools for data processing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6D83zYN44g3s"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import math\n",
        "from collections import Counter\n",
        "import pprint\n",
        "pp = pprint.PrettyPrinter(indent=1)"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGJBDQsI4g3s"
      },
      "source": [
        "We create a ```Dictionary``` class, that we are going to use to create a vocabulary for our text data. The goal here is to have a convenient tool, with easy access to any information we could need:\n",
        "- A python dictionary ```word2idx``` allowing easy transformation of tokenized text into indexes\n",
        "- A list ```idx2word```, allowing us to find the word corresponding to an index (for interpretation and generation)\n",
        "- A python dictionary ```counter``` used to build the vocabulary, that can provide us with frequency information if needed. \n",
        "- The ```total``` count of words in the dictionary.\n",
        "\n",
        "Important: The data that we are going to use are already pre-processed so we don't need to create special tokens and control the size of the vocabulary ourselves. However, when the text data is raw, methods to preprocess it conveniently should be added here. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urQQ5WEI4g3s"
      },
      "source": [
        "class Dictionary(object):\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = []\n",
        "        self.counter = {}\n",
        "        self.total = 0\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.idx2word.append(word)\n",
        "            self.word2idx[word] = len(self.idx2word) - 1\n",
        "            self.counter.setdefault(word, 0)\n",
        "        self.counter[word] += 1\n",
        "        self.total += 1\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RG6rsEQB4g3t"
      },
      "source": [
        "class Corpus(object):\n",
        "    def __init__(self, path):\n",
        "        # We create an object Dictionary associated to Corpus\n",
        "        self.dictionary = Dictionary()\n",
        "        # We go through all files, adding all words to the dictionary\n",
        "        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n",
        "        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n",
        "        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n",
        "        \n",
        "    def tokenize(self, path):\n",
        "        \"\"\"Tokenizes a text file, knowing the dictionary, in order to tranform it into a list of indexes\"\"\"\n",
        "        assert os.path.exists(path)\n",
        "        # Add words to the dictionary\n",
        "        with open(path, 'r') as f:\n",
        "            tokens = 0\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                for word in words:\n",
        "                    self.dictionary.add_word(word)\n",
        "                tokens += len(words)\n",
        "        \n",
        "        # Once done, go through the file a second time and fill a Torch Tensor with the associated indexes \n",
        "        with open(path, 'r') as f:\n",
        "            ids = torch.LongTensor(tokens)\n",
        "            token = 0\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                for word in words:\n",
        "                    ids[token] = self.dictionary.word2idx[word]\n",
        "                    token += 1\n",
        "        return ids"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytRgixof4g3t"
      },
      "source": [
        "###############################################################################\n",
        "# Load data\n",
        "###############################################################################\n",
        "\n",
        "data = './corpus/'\n",
        "corpus = Corpus(data)"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHf1bA7C4g3t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcbaa8c7-214c-465a-8819-f3ccfe5df210"
      },
      "source": [
        "print(corpus.dictionary.total)\n",
        "print(len(corpus.dictionary.idx2word))\n",
        "print(len(corpus.dictionary.word2idx))\n",
        "\n",
        "print(corpus.train.shape)\n",
        "print(corpus.train[0:7])\n",
        "print([corpus.dictionary.idx2word[corpus.train[i]] for i in range(7)])\n",
        "\n",
        "print(corpus.valid.shape)\n",
        "print(corpus.valid[0:7])\n",
        "print([corpus.dictionary.idx2word[corpus.valid[i]] for i in range(7)])"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1076937\n",
            "23244\n",
            "23244\n",
            "torch.Size([773840])\n",
            "tensor([0, 1, 2, 3, 4, 5, 6])\n",
            "['Oui', ',', 'je', 'viens', 'dans', 'son', 'Temple']\n",
            "torch.Size([135984])\n",
            "tensor([  33,    2, 1176,  104,   21, 1218,   41])\n",
            "['Que', 'je', 'sens', 'à', 'la', 'fois', 'de']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIWjBwFj4g3t"
      },
      "source": [
        "# We now have data under a very long list of indexes: the text is as one sequence.\n",
        "# The idea now is to create batches from this. Note that this is absolutely not the best\n",
        "# way to proceed with large quantities of data (where we'll try not to store huge tensors\n",
        "# in memory but read them from file as we go) !\n",
        "# Here, we are looking for simplicity and efficiency with regards to computation time.\n",
        "# That is why we will ignore sentence separations and treat the data as one long stream that\n",
        "# we will cut arbitrarily as we need.\n",
        "# With the alphabet being our data, we currently have the sequence:\n",
        "# [a b c d e f g h i j k l m n o p q r s t u v w x y z]\n",
        "# We want to reorganize it as independant batches that will be processed independantly by the model !\n",
        "# For instance, with the alphabet as the sequence and batch size 4, we'd get the 4 following sequences:\n",
        "# ┌ a g m s ┐\n",
        "# │ b h n t │\n",
        "# │ c i o u │\n",
        "# │ d j p v │\n",
        "# │ e k q w │\n",
        "# └ f l r x ┘\n",
        "# with the last two elements being lost.\n",
        "# Again, these columns are treated as independent by the model, which means that the\n",
        "# dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient processing.\n",
        "\n",
        "def batchify(data, batch_size, cuda = False):\n",
        "    # Cut the elements that are unnecessary\n",
        "    nbatch = data.size(0) // batch_size\n",
        "    data = data.narrow(0, 0, nbatch * batch_size)\n",
        "    # Reorganize the data\n",
        "    data = data.view(batch_size, -1).t().contiguous()\n",
        "    # If we can use a GPU, let's tranfer the tensor to it\n",
        "    if cuda:\n",
        "        data = data.cuda()\n",
        "    return data\n",
        "\n",
        "# get_batch subdivides the source data into chunks of the appropriate length.\n",
        "# If source is equal to the example output of the batchify function, with\n",
        "# a sequence length (seq_len) of 3, we'd get the following two variables:\n",
        "# ┌ a g m s ┐ ┌ b h n t ┐\n",
        "# | b h n t | | c i o u │\n",
        "# └ c i o u ┘ └ d j p v ┘\n",
        "# The first variable contains the letters input to the network, while the second\n",
        "# contains the one we want the network to predict (b for a, h for g, v for u, etc..)\n",
        "# Note that despite the name of the function, we are cutting the data in the\n",
        "# temporal dimension, since we already divided data into batches in the previous\n",
        "# function. \n",
        "\n",
        "def get_batch(source, i, seq_len, evaluation=False):\n",
        "    # Deal with the possibility that there's not enough data left for a full sequence\n",
        "    seq_len = min(seq_len, len(source) - 1 - i)\n",
        "    # Take the input data\n",
        "    data = source[i:i+seq_len]\n",
        "    # Shift by one for the target data\n",
        "    target = source[i+1:i+1+seq_len]\n",
        "    return data, target"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwfqKiMJ4g3t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a7ea7e5-99aa-40ef-ab89-a1c44b7a1714"
      },
      "source": [
        "batch_size = 100\n",
        "eval_batch_size = 4\n",
        "train_data = batchify(corpus.train, batch_size)\n",
        "val_data = batchify(corpus.valid, eval_batch_size)\n",
        "test_data = batchify(corpus.test, eval_batch_size)\n",
        "\n",
        "print(train_data.shape)\n",
        "print(val_data.shape)"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([7738, 100])\n",
            "torch.Size([33996, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9OVIoVJ4g3u"
      },
      "source": [
        "### LSTM Cells in pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztKrssh84g3u"
      },
      "source": [
        "### Creating our own LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zD0K9CQ14g3u"
      },
      "source": [
        "# Models are usually implemented as custom nn.Module subclass\n",
        "# We need to redefine the __init__ method, which creates the object\n",
        "# We also need to redefine the forward method, which transform the input into outputs\n",
        "# We can also add any method that we need: here, in order to initiate weights in the model\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        # Create a dropout object to use on layers for regularization\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        # Create an encoder - which is an embedding layer\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        # Create the LSTM layers - find out how to stack them !\n",
        "        self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
        "        # Create what we call the decoder: a linear transformation to map the hidden state into scores for all words in the vocabulary\n",
        "        # (Note that the softmax application function will be applied out of the model)\n",
        "        self.decoder = nn.Linear(nhid, ntoken)\n",
        "        \n",
        "        # Initialize non-reccurent weights \n",
        "        self.init_weights()\n",
        "\n",
        "        self.ninp = ninp\n",
        "        self.nhid = nhid\n",
        "        self.nlayers = nlayers\n",
        "        \n",
        "    def init_weights(self):\n",
        "        # Initialize the encoder and decoder weights with the uniform distribution\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.fill_(0)\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "        \n",
        "    def init_hidden(self, batch_size):\n",
        "        # Initialize the hidden state and cell state to zero, with the right sizes\n",
        "        weight = next(self.parameters())\n",
        "        return (weight.new_zeros(self.nlayers, batch_size, self.nhid),\n",
        "                weight.new_zeros(self.nlayers, batch_size, self.nhid))    \n",
        "\n",
        "    def forward(self, input, hidden, return_h=False):\n",
        "        # Process the input\n",
        "        emb = self.drop(self.encoder(input))   \n",
        "        \n",
        "        # Apply the LSTMs\n",
        "        output, hidden = self.rnn(emb, hidden)\n",
        "        \n",
        "        # Decode into scores\n",
        "        output = self.drop(output)      \n",
        "        decoded = self.decoder(output)\n",
        "        return decoded, hidden"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucXMYksR4g3u"
      },
      "source": [
        "### Building the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HL-vvLGm4g3u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a382563-9d5d-46b2-ce1e-31a91609b605"
      },
      "source": [
        "# Set the random seed manually for reproducibility.\n",
        "torch.manual_seed(1)\n",
        "\n",
        "# If you have Cuda installed and a GPU available\n",
        "cuda = False\n",
        "if torch.cuda.is_available():\n",
        "    if not cuda:\n",
        "        print(\"WARNING: You have a CUDA device, so you should probably choose cuda = True\")\n",
        "        \n",
        "device = torch.device(\"cuda\" if cuda else \"cpu\")"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: You have a CUDA device, so you should probably choose cuda = True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S147j0Dx4g3u"
      },
      "source": [
        "embedding_size = 200\n",
        "hidden_size = 200\n",
        "layers = 2\n",
        "dropout = 0.5\n",
        "\n",
        "###############################################################################\n",
        "# Build the model\n",
        "###############################################################################\n",
        "\n",
        "vocab_size = len(corpus.dictionary)\n",
        "model = LSTMModel(vocab_size, embedding_size, hidden_size, layers, dropout).to(device)\n",
        "params = list(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RU2X8VHG4g3v"
      },
      "source": [
        "lr = 10.0\n",
        "optimizer = 'sgd'\n",
        "wdecay = 1.2e-6\n",
        "# For gradient clipping\n",
        "clip = 0.25\n",
        "\n",
        "if optimizer == 'sgd':\n",
        "    optim = torch.optim.SGD(params, lr=lr, weight_decay=wdecay)\n",
        "if optimizer == 'adam':\n",
        "    optim = torch.optim.Adam(params, lr=lr, weight_decay=wdecay)"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fGlrwME4g3v"
      },
      "source": [
        "# Let's think about gradient propagation:\n",
        "# We plan to keep the second ouput of the LSTM layer (the hidden/cell states) to initialize\n",
        "# the next call to LSTM. In this way, we can back-propagate the gradient for as long as we want.\n",
        "# However, this put a huge strain on the memory used by the model, since it implies retaining\n",
        "# a always-growing number of tensors of gradients in the cache.\n",
        "# We decide to not backpropagate through time beyond the current sequence ! \n",
        "# We use a specific function to cut the 'hidden/state cell' states from their previous dependencies\n",
        "# before using them to initialize the next call to the LSTM.\n",
        "# This is done with the .detach() function.\n",
        "\n",
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4CGuudp4g3v"
      },
      "source": [
        "# Other global parameters\n",
        "epochs = 10\n",
        "seq_len = 30\n",
        "log_interval = 10\n",
        "save = 'model.pt'"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xx6UKwTO4g3v"
      },
      "source": [
        "def evaluate(data_source):\n",
        "    # Turn on evaluation mode which disables dropout.\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    hidden = model.init_hidden(eval_batch_size)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, seq_len):\n",
        "            data, targets = get_batch(data_source, i, seq_len)\n",
        "            output, hidden = model(data, hidden)\n",
        "            hidden = repackage_hidden(hidden)\n",
        "            total_loss += len(data) * criterion(output.view(-1, vocab_size), targets.view(-1)).item()\n",
        "    return total_loss / (len(data_source) - 1)"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EhuQglX4g3v"
      },
      "source": [
        "def train():\n",
        "    # Turn on training mode which enables dropout.\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    hidden = model.init_hidden(batch_size)\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, seq_len)):\n",
        "        data, targets = get_batch(train_data, i, seq_len)\n",
        "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "        hidden = repackage_hidden(hidden)\n",
        "        optim.zero_grad()\n",
        "        \n",
        "        output, hidden = model(data, hidden)\n",
        "        loss = criterion(output.view(-1, vocab_size), targets.view(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        torch.nn.utils.clip_grad_norm_(params, clip)\n",
        "        optim.step()\n",
        "        \n",
        "        total_loss += loss.data\n",
        "\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
        "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                epoch, batch, len(train_data) // seq_len, lr,\n",
        "                elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bN0gBRgf4g3v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5aaa10a-cea7-4e1b-f3f3-541d80a39521"
      },
      "source": [
        "# Loop over epochs.\n",
        "best_val_loss = None\n",
        "\n",
        "# At any point you can hit Ctrl + C to break out of training early.\n",
        "try:\n",
        "    for epoch in range(1, epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        train()\n",
        "        val_loss = evaluate(val_data)\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                           val_loss, math.exp(val_loss)))\n",
        "        print('-' * 89)\n",
        "        # Save the model if the validation loss is the best we've seen so far.\n",
        "        if not best_val_loss or val_loss < best_val_loss:\n",
        "            with open(save, 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_val_loss = val_loss\n",
        "        else:\n",
        "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "            lr /= 4.0\n",
        "except KeyboardInterrupt:\n",
        "    print('-' * 89)\n",
        "    print('Exiting from training early')\n",
        "\n",
        "# Load the best saved model.\n",
        "with open(save, 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "    # after load the rnn params are not a continuous chunk of memory\n",
        "    # this makes them a continuous chunk, and will speed up forward pass\n",
        "    model.rnn.flatten_parameters()\n",
        "\n",
        "# Run on test data.\n",
        "test_loss = evaluate(test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, math.exp(test_loss)))\n",
        "print('=' * 89)"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| epoch   1 |    10/  257 batches | lr 10.00 | ms/batch 2181.04 | loss 10.01 | ppl 22224.26\n",
            "| epoch   1 |    20/  257 batches | lr 10.00 | ms/batch 1983.83 | loss  8.08 | ppl  3226.40\n",
            "| epoch   1 |    30/  257 batches | lr 10.00 | ms/batch 1990.92 | loss  7.49 | ppl  1785.09\n",
            "| epoch   1 |    40/  257 batches | lr 10.00 | ms/batch 2006.81 | loss  7.37 | ppl  1594.91\n",
            "| epoch   1 |    50/  257 batches | lr 10.00 | ms/batch 1990.30 | loss  7.17 | ppl  1302.55\n",
            "| epoch   1 |    60/  257 batches | lr 10.00 | ms/batch 1986.34 | loss  7.09 | ppl  1200.23\n",
            "| epoch   1 |    70/  257 batches | lr 10.00 | ms/batch 1961.53 | loss  7.06 | ppl  1159.49\n",
            "| epoch   1 |    80/  257 batches | lr 10.00 | ms/batch 1950.24 | loss  7.01 | ppl  1106.91\n",
            "| epoch   1 |    90/  257 batches | lr 10.00 | ms/batch 1925.70 | loss  6.92 | ppl  1011.12\n",
            "| epoch   1 |   100/  257 batches | lr 10.00 | ms/batch 1886.88 | loss  6.97 | ppl  1060.34\n",
            "| epoch   1 |   110/  257 batches | lr 10.00 | ms/batch 1842.40 | loss  6.90 | ppl   995.25\n",
            "| epoch   1 |   120/  257 batches | lr 10.00 | ms/batch 1855.98 | loss  6.90 | ppl   990.38\n",
            "| epoch   1 |   130/  257 batches | lr 10.00 | ms/batch 1810.45 | loss  6.81 | ppl   905.61\n",
            "| epoch   1 |   140/  257 batches | lr 10.00 | ms/batch 1825.50 | loss  6.83 | ppl   927.38\n",
            "| epoch   1 |   150/  257 batches | lr 10.00 | ms/batch 1794.68 | loss  6.70 | ppl   813.73\n",
            "| epoch   1 |   160/  257 batches | lr 10.00 | ms/batch 1818.79 | loss  6.71 | ppl   824.04\n",
            "| epoch   1 |   170/  257 batches | lr 10.00 | ms/batch 1800.15 | loss  6.79 | ppl   890.17\n",
            "| epoch   1 |   180/  257 batches | lr 10.00 | ms/batch 1770.53 | loss  6.71 | ppl   824.06\n",
            "| epoch   1 |   190/  257 batches | lr 10.00 | ms/batch 1778.24 | loss  6.69 | ppl   802.28\n",
            "| epoch   1 |   200/  257 batches | lr 10.00 | ms/batch 1764.69 | loss  6.65 | ppl   770.81\n",
            "| epoch   1 |   210/  257 batches | lr 10.00 | ms/batch 1773.26 | loss  6.66 | ppl   782.99\n",
            "| epoch   1 |   220/  257 batches | lr 10.00 | ms/batch 1745.84 | loss  6.67 | ppl   789.31\n",
            "| epoch   1 |   230/  257 batches | lr 10.00 | ms/batch 1730.15 | loss  6.59 | ppl   727.53\n",
            "| epoch   1 |   240/  257 batches | lr 10.00 | ms/batch 1725.92 | loss  6.64 | ppl   766.38\n",
            "| epoch   1 |   250/  257 batches | lr 10.00 | ms/batch 1724.55 | loss  6.61 | ppl   743.10\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 508.97s | valid loss  6.59 | valid ppl   725.80\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |    10/  257 batches | lr 10.00 | ms/batch 1891.12 | loss  7.24 | ppl  1399.81\n",
            "| epoch   2 |    20/  257 batches | lr 10.00 | ms/batch 1715.30 | loss  6.49 | ppl   661.61\n",
            "| epoch   2 |    30/  257 batches | lr 10.00 | ms/batch 1732.01 | loss  6.35 | ppl   574.62\n",
            "| epoch   2 |    40/  257 batches | lr 10.00 | ms/batch 1710.32 | loss  6.26 | ppl   524.79\n",
            "| epoch   2 |    50/  257 batches | lr 10.00 | ms/batch 1711.67 | loss  6.18 | ppl   483.63\n",
            "| epoch   2 |    60/  257 batches | lr 10.00 | ms/batch 1736.55 | loss  6.11 | ppl   448.35\n",
            "| epoch   2 |    70/  257 batches | lr 10.00 | ms/batch 1723.04 | loss  6.08 | ppl   438.80\n",
            "| epoch   2 |    80/  257 batches | lr 10.00 | ms/batch 1732.96 | loss  6.04 | ppl   419.18\n",
            "| epoch   2 |    90/  257 batches | lr 10.00 | ms/batch 1727.20 | loss  5.97 | ppl   392.88\n",
            "| epoch   2 |   100/  257 batches | lr 10.00 | ms/batch 1712.82 | loss  5.97 | ppl   390.01\n",
            "| epoch   2 |   110/  257 batches | lr 10.00 | ms/batch 1705.38 | loss  5.91 | ppl   368.93\n",
            "| epoch   2 |   120/  257 batches | lr 10.00 | ms/batch 1714.89 | loss  5.90 | ppl   366.77\n",
            "| epoch   2 |   130/  257 batches | lr 10.00 | ms/batch 1718.31 | loss  5.92 | ppl   371.09\n",
            "| epoch   2 |   140/  257 batches | lr 10.00 | ms/batch 1699.72 | loss  5.83 | ppl   338.86\n",
            "| epoch   2 |   150/  257 batches | lr 10.00 | ms/batch 1704.81 | loss  5.81 | ppl   333.26\n",
            "| epoch   2 |   160/  257 batches | lr 10.00 | ms/batch 1703.75 | loss  5.80 | ppl   331.89\n",
            "| epoch   2 |   170/  257 batches | lr 10.00 | ms/batch 1731.89 | loss  5.78 | ppl   325.16\n",
            "| epoch   2 |   180/  257 batches | lr 10.00 | ms/batch 1709.06 | loss  5.74 | ppl   311.03\n",
            "| epoch   2 |   190/  257 batches | lr 10.00 | ms/batch 1704.97 | loss  5.76 | ppl   318.31\n",
            "| epoch   2 |   200/  257 batches | lr 10.00 | ms/batch 1709.22 | loss  5.73 | ppl   306.60\n",
            "| epoch   2 |   210/  257 batches | lr 10.00 | ms/batch 1712.55 | loss  5.65 | ppl   284.02\n",
            "| epoch   2 |   220/  257 batches | lr 10.00 | ms/batch 1723.75 | loss  5.66 | ppl   285.85\n",
            "| epoch   2 |   230/  257 batches | lr 10.00 | ms/batch 1710.04 | loss  5.59 | ppl   268.77\n",
            "| epoch   2 |   240/  257 batches | lr 10.00 | ms/batch 1729.39 | loss  5.59 | ppl   267.62\n",
            "| epoch   2 |   250/  257 batches | lr 10.00 | ms/batch 1726.70 | loss  5.62 | ppl   277.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 473.59s | valid loss  5.73 | valid ppl   308.25\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |    10/  257 batches | lr 10.00 | ms/batch 1877.03 | loss  6.14 | ppl   462.35\n",
            "| epoch   3 |    20/  257 batches | lr 10.00 | ms/batch 1707.70 | loss  5.54 | ppl   255.40\n",
            "| epoch   3 |    30/  257 batches | lr 10.00 | ms/batch 1710.65 | loss  5.48 | ppl   240.58\n",
            "| epoch   3 |    40/  257 batches | lr 10.00 | ms/batch 1720.00 | loss  5.47 | ppl   238.53\n",
            "| epoch   3 |    50/  257 batches | lr 10.00 | ms/batch 1699.94 | loss  5.47 | ppl   238.56\n",
            "| epoch   3 |    60/  257 batches | lr 10.00 | ms/batch 1697.77 | loss  5.42 | ppl   226.86\n",
            "| epoch   3 |    70/  257 batches | lr 10.00 | ms/batch 1703.21 | loss  5.42 | ppl   225.50\n",
            "| epoch   3 |    80/  257 batches | lr 10.00 | ms/batch 1710.05 | loss  5.39 | ppl   219.84\n",
            "| epoch   3 |    90/  257 batches | lr 10.00 | ms/batch 1709.88 | loss  5.43 | ppl   228.04\n",
            "| epoch   3 |   100/  257 batches | lr 10.00 | ms/batch 1699.99 | loss  5.38 | ppl   217.29\n",
            "| epoch   3 |   110/  257 batches | lr 10.00 | ms/batch 1698.15 | loss  5.33 | ppl   206.64\n",
            "| epoch   3 |   120/  257 batches | lr 10.00 | ms/batch 1698.68 | loss  5.35 | ppl   211.66\n",
            "| epoch   3 |   130/  257 batches | lr 10.00 | ms/batch 1711.89 | loss  5.37 | ppl   215.78\n",
            "| epoch   3 |   140/  257 batches | lr 10.00 | ms/batch 1695.38 | loss  5.29 | ppl   197.88\n",
            "| epoch   3 |   150/  257 batches | lr 10.00 | ms/batch 1729.33 | loss  5.30 | ppl   199.88\n",
            "| epoch   3 |   160/  257 batches | lr 10.00 | ms/batch 1699.70 | loss  5.28 | ppl   197.09\n",
            "| epoch   3 |   170/  257 batches | lr 10.00 | ms/batch 1722.30 | loss  5.32 | ppl   204.10\n",
            "| epoch   3 |   180/  257 batches | lr 10.00 | ms/batch 1718.48 | loss  5.26 | ppl   192.33\n",
            "| epoch   3 |   190/  257 batches | lr 10.00 | ms/batch 1703.35 | loss  5.27 | ppl   194.92\n",
            "| epoch   3 |   200/  257 batches | lr 10.00 | ms/batch 1702.98 | loss  5.25 | ppl   190.95\n",
            "| epoch   3 |   210/  257 batches | lr 10.00 | ms/batch 1702.23 | loss  5.17 | ppl   175.16\n",
            "| epoch   3 |   220/  257 batches | lr 10.00 | ms/batch 1709.38 | loss  5.20 | ppl   181.69\n",
            "| epoch   3 |   230/  257 batches | lr 10.00 | ms/batch 1716.23 | loss  5.20 | ppl   181.79\n",
            "| epoch   3 |   240/  257 batches | lr 10.00 | ms/batch 1705.45 | loss  5.20 | ppl   180.75\n",
            "| epoch   3 |   250/  257 batches | lr 10.00 | ms/batch 1702.56 | loss  5.22 | ppl   185.12\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 471.00s | valid loss  5.25 | valid ppl   191.02\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |    10/  257 batches | lr 10.00 | ms/batch 1929.95 | loss  5.72 | ppl   304.70\n",
            "| epoch   4 |    20/  257 batches | lr 10.00 | ms/batch 1792.92 | loss  5.18 | ppl   177.08\n",
            "| epoch   4 |    30/  257 batches | lr 10.00 | ms/batch 1784.88 | loss  5.15 | ppl   171.96\n",
            "| epoch   4 |    40/  257 batches | lr 10.00 | ms/batch 1758.16 | loss  5.13 | ppl   168.95\n",
            "| epoch   4 |    50/  257 batches | lr 10.00 | ms/batch 1766.23 | loss  5.16 | ppl   173.41\n",
            "| epoch   4 |    60/  257 batches | lr 10.00 | ms/batch 1756.48 | loss  5.10 | ppl   164.04\n",
            "| epoch   4 |    70/  257 batches | lr 10.00 | ms/batch 1754.26 | loss  5.09 | ppl   161.94\n",
            "| epoch   4 |    80/  257 batches | lr 10.00 | ms/batch 1745.42 | loss  5.08 | ppl   161.55\n",
            "| epoch   4 |    90/  257 batches | lr 10.00 | ms/batch 1762.36 | loss  5.12 | ppl   167.27\n",
            "| epoch   4 |   100/  257 batches | lr 10.00 | ms/batch 1741.44 | loss  5.11 | ppl   166.07\n",
            "| epoch   4 |   110/  257 batches | lr 10.00 | ms/batch 1745.21 | loss  5.05 | ppl   156.13\n",
            "| epoch   4 |   120/  257 batches | lr 10.00 | ms/batch 1739.32 | loss  5.06 | ppl   156.92\n",
            "| epoch   4 |   130/  257 batches | lr 10.00 | ms/batch 1745.30 | loss  5.07 | ppl   159.73\n",
            "| epoch   4 |   140/  257 batches | lr 10.00 | ms/batch 1758.13 | loss  5.00 | ppl   148.50\n",
            "| epoch   4 |   150/  257 batches | lr 10.00 | ms/batch 1739.64 | loss  5.02 | ppl   151.00\n",
            "| epoch   4 |   160/  257 batches | lr 10.00 | ms/batch 1735.92 | loss  5.01 | ppl   150.06\n",
            "| epoch   4 |   170/  257 batches | lr 10.00 | ms/batch 1737.46 | loss  5.01 | ppl   150.62\n",
            "| epoch   4 |   180/  257 batches | lr 10.00 | ms/batch 1760.72 | loss  5.01 | ppl   150.00\n",
            "| epoch   4 |   190/  257 batches | lr 10.00 | ms/batch 1738.78 | loss  4.97 | ppl   144.28\n",
            "| epoch   4 |   200/  257 batches | lr 10.00 | ms/batch 1732.20 | loss  4.97 | ppl   143.55\n",
            "| epoch   4 |   210/  257 batches | lr 10.00 | ms/batch 1729.63 | loss  4.93 | ppl   138.97\n",
            "| epoch   4 |   220/  257 batches | lr 10.00 | ms/batch 1734.44 | loss  4.97 | ppl   143.33\n",
            "| epoch   4 |   230/  257 batches | lr 10.00 | ms/batch 1773.00 | loss  4.95 | ppl   140.86\n",
            "| epoch   4 |   240/  257 batches | lr 10.00 | ms/batch 1748.40 | loss  4.95 | ppl   141.24\n",
            "| epoch   4 |   250/  257 batches | lr 10.00 | ms/batch 1735.48 | loss  4.96 | ppl   142.10\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 482.70s | valid loss  5.08 | valid ppl   161.37\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |    10/  257 batches | lr 10.00 | ms/batch 1902.33 | loss  5.47 | ppl   237.78\n",
            "| epoch   5 |    20/  257 batches | lr 10.00 | ms/batch 1730.82 | loss  4.94 | ppl   139.16\n",
            "| epoch   5 |    30/  257 batches | lr 10.00 | ms/batch 1731.84 | loss  4.93 | ppl   138.79\n",
            "| epoch   5 |    40/  257 batches | lr 10.00 | ms/batch 1731.81 | loss  4.92 | ppl   137.44\n",
            "| epoch   5 |    50/  257 batches | lr 10.00 | ms/batch 1743.49 | loss  4.92 | ppl   137.53\n",
            "| epoch   5 |    60/  257 batches | lr 10.00 | ms/batch 1731.36 | loss  4.89 | ppl   132.36\n",
            "| epoch   5 |    70/  257 batches | lr 10.00 | ms/batch 1732.16 | loss  4.86 | ppl   129.49\n",
            "| epoch   5 |    80/  257 batches | lr 10.00 | ms/batch 1732.09 | loss  4.86 | ppl   129.25\n",
            "| epoch   5 |    90/  257 batches | lr 10.00 | ms/batch 1746.47 | loss  4.91 | ppl   135.24\n",
            "| epoch   5 |   100/  257 batches | lr 10.00 | ms/batch 1732.05 | loss  4.88 | ppl   131.00\n",
            "| epoch   5 |   110/  257 batches | lr 10.00 | ms/batch 1723.69 | loss  4.85 | ppl   127.55\n",
            "| epoch   5 |   120/  257 batches | lr 10.00 | ms/batch 1724.98 | loss  4.87 | ppl   130.81\n",
            "| epoch   5 |   130/  257 batches | lr 10.00 | ms/batch 1731.39 | loss  4.88 | ppl   131.30\n",
            "| epoch   5 |   140/  257 batches | lr 10.00 | ms/batch 1754.95 | loss  4.82 | ppl   124.57\n",
            "| epoch   5 |   150/  257 batches | lr 10.00 | ms/batch 1735.52 | loss  4.81 | ppl   122.68\n",
            "| epoch   5 |   160/  257 batches | lr 10.00 | ms/batch 1724.81 | loss  4.83 | ppl   125.01\n",
            "| epoch   5 |   170/  257 batches | lr 10.00 | ms/batch 1726.35 | loss  4.88 | ppl   131.23\n",
            "| epoch   5 |   180/  257 batches | lr 10.00 | ms/batch 1739.63 | loss  4.84 | ppl   126.70\n",
            "| epoch   5 |   190/  257 batches | lr 10.00 | ms/batch 1733.49 | loss  4.84 | ppl   125.91\n",
            "| epoch   5 |   200/  257 batches | lr 10.00 | ms/batch 1731.25 | loss  4.77 | ppl   118.35\n",
            "| epoch   5 |   210/  257 batches | lr 10.00 | ms/batch 1727.38 | loss  4.74 | ppl   114.22\n",
            "| epoch   5 |   220/  257 batches | lr 10.00 | ms/batch 1729.09 | loss  4.80 | ppl   121.89\n",
            "| epoch   5 |   230/  257 batches | lr 10.00 | ms/batch 1748.78 | loss  4.78 | ppl   118.56\n",
            "| epoch   5 |   240/  257 batches | lr 10.00 | ms/batch 1729.63 | loss  4.79 | ppl   120.90\n",
            "| epoch   5 |   250/  257 batches | lr 10.00 | ms/batch 1724.78 | loss  4.79 | ppl   120.18\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 478.19s | valid loss  4.87 | valid ppl   129.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |    10/  257 batches | lr 10.00 | ms/batch 1903.50 | loss  5.32 | ppl   204.99\n",
            "| epoch   6 |    20/  257 batches | lr 10.00 | ms/batch 1726.68 | loss  4.78 | ppl   118.81\n",
            "| epoch   6 |    30/  257 batches | lr 10.00 | ms/batch 1724.13 | loss  4.76 | ppl   116.35\n",
            "| epoch   6 |    40/  257 batches | lr 10.00 | ms/batch 1766.62 | loss  4.77 | ppl   118.28\n",
            "| epoch   6 |    50/  257 batches | lr 10.00 | ms/batch 1750.22 | loss  4.78 | ppl   119.43\n",
            "| epoch   6 |    60/  257 batches | lr 10.00 | ms/batch 1722.93 | loss  4.75 | ppl   116.01\n",
            "| epoch   6 |    70/  257 batches | lr 10.00 | ms/batch 1729.80 | loss  4.73 | ppl   112.95\n",
            "| epoch   6 |    80/  257 batches | lr 10.00 | ms/batch 1726.70 | loss  4.71 | ppl   110.95\n",
            "| epoch   6 |    90/  257 batches | lr 10.00 | ms/batch 1734.63 | loss  4.77 | ppl   117.89\n",
            "| epoch   6 |   100/  257 batches | lr 10.00 | ms/batch 1740.28 | loss  4.75 | ppl   115.57\n",
            "| epoch   6 |   110/  257 batches | lr 10.00 | ms/batch 1728.80 | loss  4.71 | ppl   110.86\n",
            "| epoch   6 |   120/  257 batches | lr 10.00 | ms/batch 1727.50 | loss  4.71 | ppl   111.60\n",
            "| epoch   6 |   130/  257 batches | lr 10.00 | ms/batch 1729.93 | loss  4.73 | ppl   113.32\n",
            "| epoch   6 |   140/  257 batches | lr 10.00 | ms/batch 1744.29 | loss  4.68 | ppl   107.83\n",
            "| epoch   6 |   150/  257 batches | lr 10.00 | ms/batch 1722.98 | loss  4.68 | ppl   107.64\n",
            "| epoch   6 |   160/  257 batches | lr 10.00 | ms/batch 1725.99 | loss  4.70 | ppl   110.35\n",
            "| epoch   6 |   170/  257 batches | lr 10.00 | ms/batch 1728.12 | loss  4.72 | ppl   112.66\n",
            "| epoch   6 |   180/  257 batches | lr 10.00 | ms/batch 1730.09 | loss  4.71 | ppl   110.83\n",
            "| epoch   6 |   190/  257 batches | lr 10.00 | ms/batch 1750.04 | loss  4.69 | ppl   109.35\n",
            "| epoch   6 |   200/  257 batches | lr 10.00 | ms/batch 1723.92 | loss  4.65 | ppl   105.07\n",
            "| epoch   6 |   210/  257 batches | lr 10.00 | ms/batch 1722.24 | loss  4.61 | ppl   100.41\n",
            "| epoch   6 |   220/  257 batches | lr 10.00 | ms/batch 1764.06 | loss  4.66 | ppl   106.14\n",
            "| epoch   6 |   230/  257 batches | lr 10.00 | ms/batch 1741.44 | loss  4.66 | ppl   105.24\n",
            "| epoch   6 |   240/  257 batches | lr 10.00 | ms/batch 1745.69 | loss  4.66 | ppl   105.29\n",
            "| epoch   6 |   250/  257 batches | lr 10.00 | ms/batch 1729.56 | loss  4.67 | ppl   106.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 478.98s | valid loss  4.76 | valid ppl   116.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |    10/  257 batches | lr 10.00 | ms/batch 1913.60 | loss  5.19 | ppl   179.42\n",
            "| epoch   7 |    20/  257 batches | lr 10.00 | ms/batch 1729.23 | loss  4.66 | ppl   105.21\n",
            "| epoch   7 |    30/  257 batches | lr 10.00 | ms/batch 1727.96 | loss  4.64 | ppl   103.50\n",
            "| epoch   7 |    40/  257 batches | lr 10.00 | ms/batch 1730.77 | loss  4.64 | ppl   103.77\n",
            "| epoch   7 |    50/  257 batches | lr 10.00 | ms/batch 1760.06 | loss  4.66 | ppl   105.71\n",
            "| epoch   7 |    60/  257 batches | lr 10.00 | ms/batch 1727.44 | loss  4.64 | ppl   103.36\n",
            "| epoch   7 |    70/  257 batches | lr 10.00 | ms/batch 1724.34 | loss  4.61 | ppl   100.91\n",
            "| epoch   7 |    80/  257 batches | lr 10.00 | ms/batch 1722.45 | loss  4.60 | ppl    99.08\n",
            "| epoch   7 |    90/  257 batches | lr 10.00 | ms/batch 1726.90 | loss  4.66 | ppl   105.94\n",
            "| epoch   7 |   100/  257 batches | lr 10.00 | ms/batch 1744.26 | loss  4.63 | ppl   102.53\n",
            "| epoch   7 |   110/  257 batches | lr 10.00 | ms/batch 1725.16 | loss  4.60 | ppl    99.31\n",
            "| epoch   7 |   120/  257 batches | lr 10.00 | ms/batch 1763.97 | loss  4.61 | ppl   100.42\n",
            "| epoch   7 |   130/  257 batches | lr 10.00 | ms/batch 1728.65 | loss  4.63 | ppl   102.71\n",
            "| epoch   7 |   140/  257 batches | lr 10.00 | ms/batch 1756.07 | loss  4.56 | ppl    95.82\n",
            "| epoch   7 |   150/  257 batches | lr 10.00 | ms/batch 1731.82 | loss  4.57 | ppl    96.14\n",
            "| epoch   7 |   160/  257 batches | lr 10.00 | ms/batch 1729.24 | loss  4.59 | ppl    98.38\n",
            "| epoch   7 |   170/  257 batches | lr 10.00 | ms/batch 1727.63 | loss  4.62 | ppl   101.43\n",
            "| epoch   7 |   180/  257 batches | lr 10.00 | ms/batch 1743.43 | loss  4.59 | ppl    98.75\n",
            "| epoch   7 |   190/  257 batches | lr 10.00 | ms/batch 1752.13 | loss  4.59 | ppl    98.07\n",
            "| epoch   7 |   200/  257 batches | lr 10.00 | ms/batch 1736.85 | loss  4.55 | ppl    94.51\n",
            "| epoch   7 |   210/  257 batches | lr 10.00 | ms/batch 1731.76 | loss  4.51 | ppl    90.91\n",
            "| epoch   7 |   220/  257 batches | lr 10.00 | ms/batch 1735.22 | loss  4.58 | ppl    97.03\n",
            "| epoch   7 |   230/  257 batches | lr 10.00 | ms/batch 1735.13 | loss  4.55 | ppl    94.61\n",
            "| epoch   7 |   240/  257 batches | lr 10.00 | ms/batch 1752.60 | loss  4.56 | ppl    95.28\n",
            "| epoch   7 |   250/  257 batches | lr 10.00 | ms/batch 1733.04 | loss  4.58 | ppl    97.29\n",
            "| epoch   8 |    10/  257 batches | lr 10.00 | ms/batch 1920.61 | loss  5.06 | ppl   157.41\n",
            "| epoch   8 |    20/  257 batches | lr 10.00 | ms/batch 1757.06 | loss  4.55 | ppl    94.71\n",
            "| epoch   8 |    30/  257 batches | lr 10.00 | ms/batch 1745.76 | loss  4.56 | ppl    95.26\n",
            "| epoch   8 |    40/  257 batches | lr 10.00 | ms/batch 1724.93 | loss  4.56 | ppl    95.15\n",
            "| epoch   8 |    50/  257 batches | lr 10.00 | ms/batch 1758.40 | loss  4.56 | ppl    95.86\n",
            "| epoch   8 |    60/  257 batches | lr 10.00 | ms/batch 1745.49 | loss  4.56 | ppl    95.13\n",
            "| epoch   8 |    70/  257 batches | lr 10.00 | ms/batch 1747.07 | loss  4.53 | ppl    92.31\n",
            "| epoch   8 |    80/  257 batches | lr 10.00 | ms/batch 1759.68 | loss  4.50 | ppl    90.38\n",
            "| epoch   8 |    90/  257 batches | lr 10.00 | ms/batch 1751.96 | loss  4.57 | ppl    96.85\n",
            "| epoch   8 |   100/  257 batches | lr 10.00 | ms/batch 1766.39 | loss  4.55 | ppl    94.81\n",
            "| epoch   8 |   110/  257 batches | lr 10.00 | ms/batch 1752.63 | loss  4.50 | ppl    89.82\n",
            "| epoch   8 |   120/  257 batches | lr 10.00 | ms/batch 1736.33 | loss  4.52 | ppl    91.74\n",
            "| epoch   8 |   130/  257 batches | lr 10.00 | ms/batch 1743.58 | loss  4.54 | ppl    93.95\n",
            "| epoch   8 |   140/  257 batches | lr 10.00 | ms/batch 1753.33 | loss  4.49 | ppl    89.01\n",
            "| epoch   8 |   150/  257 batches | lr 10.00 | ms/batch 1765.52 | loss  4.48 | ppl    88.47\n",
            "| epoch   8 |   160/  257 batches | lr 10.00 | ms/batch 1749.43 | loss  4.51 | ppl    91.11\n",
            "| epoch   8 |   170/  257 batches | lr 10.00 | ms/batch 1735.14 | loss  4.53 | ppl    93.15\n",
            "| epoch   8 |   180/  257 batches | lr 10.00 | ms/batch 1723.77 | loss  4.52 | ppl    91.44\n",
            "| epoch   8 |   190/  257 batches | lr 10.00 | ms/batch 1740.60 | loss  4.52 | ppl    91.50\n",
            "| epoch   8 |   200/  257 batches | lr 10.00 | ms/batch 1770.28 | loss  4.47 | ppl    87.47\n",
            "| epoch   8 |   210/  257 batches | lr 10.00 | ms/batch 1723.78 | loss  4.44 | ppl    84.45\n",
            "| epoch   8 |   220/  257 batches | lr 10.00 | ms/batch 1723.60 | loss  4.50 | ppl    89.78\n",
            "| epoch   8 |   230/  257 batches | lr 10.00 | ms/batch 1744.93 | loss  4.47 | ppl    87.34\n",
            "| epoch   8 |   240/  257 batches | lr 10.00 | ms/batch 1806.34 | loss  4.47 | ppl    87.18\n",
            "| epoch   8 |   250/  257 batches | lr 10.00 | ms/batch 1832.72 | loss  4.52 | ppl    91.46\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 485.35s | valid loss  4.63 | valid ppl   102.47\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |    10/  257 batches | lr 10.00 | ms/batch 2018.42 | loss  4.98 | ppl   145.32\n",
            "| epoch   9 |    20/  257 batches | lr 10.00 | ms/batch 1884.00 | loss  4.50 | ppl    90.12\n",
            "| epoch   9 |    30/  257 batches | lr 10.00 | ms/batch 1874.91 | loss  4.48 | ppl    87.97\n",
            "| epoch   9 |    40/  257 batches | lr 10.00 | ms/batch 1832.40 | loss  4.47 | ppl    87.40\n",
            "| epoch   9 |    50/  257 batches | lr 10.00 | ms/batch 1925.61 | loss  4.49 | ppl    89.36\n",
            "| epoch   9 |    60/  257 batches | lr 10.00 | ms/batch 1855.54 | loss  4.46 | ppl    86.45\n",
            "| epoch   9 |    70/  257 batches | lr 10.00 | ms/batch 1822.84 | loss  4.47 | ppl    87.53\n",
            "| epoch   9 |    80/  257 batches | lr 10.00 | ms/batch 1855.48 | loss  4.43 | ppl    84.19\n",
            "| epoch   9 |    90/  257 batches | lr 10.00 | ms/batch 1938.48 | loss  4.50 | ppl    90.39\n",
            "| epoch   9 |   100/  257 batches | lr 10.00 | ms/batch 1872.52 | loss  4.48 | ppl    88.45\n",
            "| epoch   9 |   110/  257 batches | lr 10.00 | ms/batch 1823.51 | loss  4.44 | ppl    84.79\n",
            "| epoch   9 |   120/  257 batches | lr 10.00 | ms/batch 1846.30 | loss  4.45 | ppl    85.84\n",
            "| epoch   9 |   130/  257 batches | lr 10.00 | ms/batch 1895.66 | loss  4.48 | ppl    88.17\n",
            "| epoch   9 |   140/  257 batches | lr 10.00 | ms/batch 1830.37 | loss  4.42 | ppl    83.30\n",
            "| epoch   9 |   150/  257 batches | lr 10.00 | ms/batch 1886.52 | loss  4.42 | ppl    83.42\n",
            "| epoch   9 |   160/  257 batches | lr 10.00 | ms/batch 1890.24 | loss  4.45 | ppl    85.45\n",
            "| epoch   9 |   170/  257 batches | lr 10.00 | ms/batch 1863.50 | loss  4.45 | ppl    85.55\n",
            "| epoch   9 |   180/  257 batches | lr 10.00 | ms/batch 1824.75 | loss  4.44 | ppl    84.73\n",
            "| epoch   9 |   190/  257 batches | lr 10.00 | ms/batch 1819.08 | loss  4.42 | ppl    83.15\n",
            "| epoch   9 |   200/  257 batches | lr 10.00 | ms/batch 1809.12 | loss  4.41 | ppl    82.49\n",
            "| epoch   9 |   210/  257 batches | lr 10.00 | ms/batch 1810.30 | loss  4.37 | ppl    79.02\n",
            "| epoch   9 |   220/  257 batches | lr 10.00 | ms/batch 1808.40 | loss  4.44 | ppl    84.81\n",
            "| epoch   9 |   230/  257 batches | lr 10.00 | ms/batch 1798.91 | loss  4.41 | ppl    82.25\n",
            "| epoch   9 |   240/  257 batches | lr 10.00 | ms/batch 1782.99 | loss  4.42 | ppl    82.86\n",
            "| epoch   9 |   250/  257 batches | lr 10.00 | ms/batch 1793.38 | loss  4.45 | ppl    85.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 509.12s | valid loss  4.56 | valid ppl    96.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |    10/  257 batches | lr 10.00 | ms/batch 1962.09 | loss  4.91 | ppl   135.66\n",
            "| epoch  10 |    20/  257 batches | lr 10.00 | ms/batch 1786.05 | loss  4.43 | ppl    83.95\n",
            "| epoch  10 |    30/  257 batches | lr 10.00 | ms/batch 1772.52 | loss  4.42 | ppl    82.93\n",
            "| epoch  10 |    40/  257 batches | lr 10.00 | ms/batch 1803.26 | loss  4.42 | ppl    83.15\n",
            "| epoch  10 |    50/  257 batches | lr 10.00 | ms/batch 1783.21 | loss  4.41 | ppl    82.23\n",
            "| epoch  10 |    60/  257 batches | lr 10.00 | ms/batch 1778.71 | loss  4.40 | ppl    81.23\n",
            "| epoch  10 |    70/  257 batches | lr 10.00 | ms/batch 1781.26 | loss  4.39 | ppl    80.85\n",
            "| epoch  10 |    80/  257 batches | lr 10.00 | ms/batch 1796.14 | loss  4.37 | ppl    79.41\n",
            "| epoch  10 |    90/  257 batches | lr 10.00 | ms/batch 1785.13 | loss  4.42 | ppl    83.42\n",
            "| epoch  10 |   100/  257 batches | lr 10.00 | ms/batch 1774.00 | loss  4.42 | ppl    82.78\n",
            "| epoch  10 |   110/  257 batches | lr 10.00 | ms/batch 1775.60 | loss  4.39 | ppl    80.35\n",
            "| epoch  10 |   120/  257 batches | lr 10.00 | ms/batch 1781.30 | loss  4.40 | ppl    81.51\n",
            "| epoch  10 |   130/  257 batches | lr 10.00 | ms/batch 1792.52 | loss  4.42 | ppl    83.49\n",
            "| epoch  10 |   140/  257 batches | lr 10.00 | ms/batch 1772.49 | loss  4.36 | ppl    78.01\n",
            "| epoch  10 |   150/  257 batches | lr 10.00 | ms/batch 1793.24 | loss  4.35 | ppl    77.50\n",
            "| epoch  10 |   160/  257 batches | lr 10.00 | ms/batch 1803.58 | loss  4.40 | ppl    81.18\n",
            "| epoch  10 |   170/  257 batches | lr 10.00 | ms/batch 1798.58 | loss  4.41 | ppl    82.37\n",
            "| epoch  10 |   180/  257 batches | lr 10.00 | ms/batch 1784.87 | loss  4.40 | ppl    81.81\n",
            "| epoch  10 |   190/  257 batches | lr 10.00 | ms/batch 1786.25 | loss  4.39 | ppl    80.74\n",
            "| epoch  10 |   200/  257 batches | lr 10.00 | ms/batch 1771.39 | loss  4.35 | ppl    77.80\n",
            "| epoch  10 |   210/  257 batches | lr 10.00 | ms/batch 1776.48 | loss  4.31 | ppl    74.49\n",
            "| epoch  10 |   220/  257 batches | lr 10.00 | ms/batch 1793.89 | loss  4.38 | ppl    80.07\n",
            "| epoch  10 |   230/  257 batches | lr 10.00 | ms/batch 1780.81 | loss  4.36 | ppl    78.34\n",
            "| epoch  10 |   240/  257 batches | lr 10.00 | ms/batch 1770.08 | loss  4.36 | ppl    78.62\n",
            "| epoch  10 |   250/  257 batches | lr 10.00 | ms/batch 1770.51 | loss  4.38 | ppl    79.87\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 492.52s | valid loss  4.52 | valid ppl    91.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "=========================================================================================\n",
            "| End of training | test loss  4.38 | test ppl    79.51\n",
            "=========================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLCMetacxM6v"
      },
      "source": [
        "import numpy as np\n",
        "def predict(device, net, words, n_vocab, vocab_to_int, int_to_vocab, top_k=5):\n",
        "    net.eval()\n",
        "\n",
        "    state_h, state_c = torch.zeros(2,1,model.nhid), torch.zeros(2,1,model.nhid)\n",
        "    state_h = state_h.to(device)\n",
        "    state_c = state_c.to(device)\n",
        "    for w in words:\n",
        "        ix = torch.tensor([[vocab_to_int[w]]]).to(device)\n",
        "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
        "    \n",
        "    _, top_ix = torch.topk(output[0], k=top_k)\n",
        "    choices = top_ix.tolist()\n",
        "    choice = np.random.choice(choices[0])\n",
        "\n",
        "    words.append(int_to_vocab[choice])\n",
        "    for _ in range(100):\n",
        "        ix = torch.tensor([[choice]]).to(device)\n",
        "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
        "\n",
        "        _, top_ix = torch.topk(output[0], k=top_k)\n",
        "        choices = top_ix.tolist()\n",
        "        choice = np.random.choice(choices[0])\n",
        "        words.append(int_to_vocab[choice])\n",
        "\n",
        "    print(' '.join(words))"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pi9OJjDDxrlT",
        "outputId": "26776672-3adf-40ad-b37e-5b168f657dd5"
      },
      "source": [
        "predict(device, model, ['Souvent', ','], corpus.dictionary.total, corpus.dictionary.word2idx, corpus.dictionary.idx2word)"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Souvent , <eos> Et vous me devez le seul , qui me faut dire , , <eos> Que vous n’ êtes plus , seigneur , à l’ avoir - vous plus , madame . , Madame . , , . . . Ah ! je ne veux rien , je veux - il , je me dis , je veux , et j’ ai dit <eos> À ce prix de sa gloire , ou l’ autre à l’ autre <eos> À l’ empire à ce jour il faut à la gloire . \" <eos> Vous êtes un grand moment qui ne vous a\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "it6lUAwIXOcO"
      },
      "source": [
        "Souvent , <eos> \n",
        "\n",
        "Et vous me devez le seul , qui me faut dire , , <eos> \n",
        "\n",
        "Que vous n’ êtes plus , seigneur , à l’ avoir - vous plus , madame . , Madame . , , . . . Ah ! je ne veux rien , je veux - il , je me dis , je veux , et j’ ai dit <eos> \n",
        "\n",
        "À ce prix de sa gloire , ou l’ autre à l’ autre <eos> \n",
        "\n",
        "À l’ empire à ce jour il faut à la gloire . \" <eos>"
      ]
    }
  ]
}